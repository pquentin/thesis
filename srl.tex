% vim: set spelllang=fr:
\chapter{Annotation en rôle sémantique basée sur la connaissance}
\label{ch:srl}


%\citep{simmons1973semantic} is the earliest work on Semantic Role Labeling.
%Already based on \citep{fillmore1968case}, it parsed a sentence into what we
% know call semantic roles. (check)

Contrairement aux autres ressources pour l'annotation en rôles sémantiques,
VerbNet couvre à la fois l'ensemble des verbes fréquents du vocabulaire tout en
étant conçu sur des préceptes robustes le rendant utile pour les tâches de
Traitement Automatique des Langues.

La tâche d'annotation en rôles sémantiques a reçu beaucoup d'attention ces
dernières années, que ce soit pour les approches supervisées ou non
supervisées. Cependant, les approches basées sur la connaissance ont été
négligées alors qu'elles apportent un compromis intéressant et sont
complémentaires par rapport aux autres approches. Ce chapitre présente un tel
système qui se veut simple et facile à reproduire. Nous montrons aussi comment
la prise en compte de la voix passive réduit le taux d'erreur de 15.7~\%, ce
qui met en avant les possibilités offertes par un tel système : une approche
simple qui facilite l'analyse des erreurs, qui n'a pas besoin d'un corpus
annoté manuellement et qui n'est pas spécifique à un domaine en particulier.

\section{Algorithme}

L'algorithme présenté est similaire à
\cite{swier2004unsupervised,swier2005exploiting}. Cependant, dix ans après, il
est important d'évaluer à nouveau cette approche pour savoir où elle se situe
par rapport à l'état de l'art et comprendre les améliorations possibles.

Le principe est d'utiliser directement les informations proposées par VerbNet.
Par exemple, VerbNet peut indiquer que dans une certaine classe, le cadre de
sous-catégorisation NP V NP correspond aux rôles Agent V Theme. Cette notation
indique que lorsque que le sujet du verbe est un syntagme nominal et que
l'objet du verbe est aussi un syntagme nominal, ils jouent respectivement les
rôles d'Agent et Theme. Dans certains cas, un même cadre de sous-catégorisation
peut correspondre à plusieurs annotations en rôles sémantiques différentes. En effet :

\begin{itemize}

    % TODO /!\ il faut déterminer la classe de toute façon
    \item si un prédicat est présent dans plusieurs classes VerbNet partageant
    les mêmes cadres de sous-catégorisation, il est possible que les annotations
    soient différentes suivant la classe considérée.

    \item certaines correspondances entre la syntaxe et la sémantique sont
    ambiguës et ne déterminent pas complètement les rôles sémantiques à utiliser.

\end{itemize}

Sans corpus annoté, ces ambiguïtés ne peuvent pas être résolues. Cependant, une
fois qu'une première série de correspondances a été effectuée, il est possible
d'utiliser les connaissances du domaine étudié pour annoter de nouveaux verbes
auparavant inaccessibles.

\subsection{Identification des arguments}

Cette première étape identifie les syntagmes amenés à recevoir un rôles
sémantique lors de l'annotation. La phrase est d'abord analysée syntaxiquement
puis certains syntagmes de la phrase sont considérés. Pour cette étape, nous
suivons \cite{lang2011unsupervised} qui propose huit règles proposant chacune
des syntagmes candidats. Ces règles génèrent trop de candidats : il faudra par
la suite soit assigner un rôle au candidat soit déclarer qu'il n'a pas de rôle
dans la phrase.

% TODO Examples
% TODO Règles
% TODO gold

\subsection{Frame matching}

Cette étape met en correspondance les syntagmes candidats précédemment
identifiés à des rôles sémantiques. Cette étape inverse deux étapes
traditionnellement utilisées dans les sytèmes d'annotation en rôles sémantiques
: \citep{gildea2002automatic,das2014frame} : l'identification des frames et
l'assignation de rôles aux arguments identifiés. En effet, nous commençons par
transformer notre liste d'arguments en construction de type VerbNet : si trois
syntagmes nominaux ont été identifiés comme arguments, dont un avant le verbe,
la représentation VerbNet de la phrase devient NP V NP NP.
% TODO confus et pas nécessairement intéressant ?
Les positions des arguments par rapport aux verbes déterminent la fonction
grammaticale de ces arguments : le premier argument est le sujet, le second un
objet direct, et le troisième un object indirect. Enfin, les syntagmes
prépositionnels sont traités à part. \citep{swier2005exploiting}.

Une fois que la phrase a été transformée en représentation Verbnet, nous
identifions toutes les classes VerbNet incluant le prédicat de la phrase afin
de comparer les frames de ces classes avec la frame identifiée dans la phrase.
Par exemple, le prédicat \textit{classify} est présent dans deux classes
VerbNet : \textit{characterize-29.2} et \textit{classify-29.10}. Les frames
possibles sont :

\begin{itemize}
    \item NP.Agent V NP.Theme (as) S\_ING.Attribute
    \item NP.Agent V NP.Theme to be ADJ.Attribute
    \item NP.Agent V NP.Theme as PP.Attribute
    \item NP.Agent V NP.Theme
    \item NP.Agent V NP.Theme as PP.Goal
    \item NP.Agent V NP.Theme in PP.Location
\end{itemize}

Prenons un example. La phrase \emph{The company also classifies short and wide
radius ruts according to their severity} est transformée en NP V NP according
PP. Dans ce cas, seuls les deux premiers arguments correspondent à des
arguments identifiés par VerbNet. Pour ces deux arguments, la correspondance
n'est pas ambiguë selon VerbNet, et le premier syntagme joue donc le rôle
d'Agent alors que le second joue le rôle de Theme. Il n'y a pas de
correspondance possible pour le troisième argument : Verbnet n'encode pas
\emph{according} comme une préposition possible alors que \emph{in} et
\emph{as} sont acceptées. Les auteurs de VerbNet travaillent actuellement sur
le problème en ajoutant des informations syntaxiques et lexicales issues de
très larges corpus \citep{bonial2013expanding}. Le résultat de ces travaux
n'est cependant pas encore disponible.

% TODO un exemple de mapping ambigu
% TODO meilleure terminologie pour ambigu/non ambigu

\subsection{Modèles de probabilité}

Maintenant qu'une partie du corpus a été annotée, nous pouvons utiliser cette
information pour annoter de nouveaux arguments ambigus. La méthode reste non
supervisée : bien que nous entraînons une forme simple d'algorithme supervisé,
nous le faisons sur des données non annotées simplement obtenues sur le corpus
existant. Dans le cas où un corpus serait inexistant, il est possible
d'abandonner cette étape ou d'alimenter les modèles de probabilité au fur et à
mesure des annotations.

Chacun de ces modèles de probabilité assigne une probabilité aux différents
rôles possibles selon les rôles identifiés de manière non ambiguë auparavant.
Une correspondance est ambigue quand deux rôles ou plus sont possibles, et est
non ambigue quand un seul rôle est possible. Différents modèles de probabilités
issus de \citep{swier2005exploiting} sont considérés.

% TODO belles formules comme dans gildea2002automatic

\paragraph{predicate-slot}

Le modèle \emph{predicate-slot} utilise l'information du prédicat et la
fonction grammaticale détectée. Par exemple, l'object direct du verbe
'négliger' sera toujours 'Theme' étant donné notre corpus
(section~\ref{srl:evaluation}). La précision pour ce modèle est forte, mais il
n'assigne des rôles que pour 40~\% des arguments : dans les autres cas, nous ne
disposons pas d'assez d'informations pour cette paire (prédicat, fonction
grammaticale) précise.

\paragraph{slot}

Étant donné que \emph{predicate-slot} ne donne pas de résultats dans tous les
cas, un modèle plus simple a aussi été utilisé : \emph{slot}. Ce modèle se
concentre uniquement sur la fonction grammaticale (en incluant la préposition
considérée). Par exemple, un syntagme prépositionnel \emph{of NP} correspond
dans l'ordre à Attribute, Theme et Topic dans le corpus utilisé dans cette
expérience. Dans un contexte précis, c'est le premier rôle possible qui sera
choisi. Ainsi, si l'étape précédente nous indique que Topic et Recipient sont
possibles, Topic sera choisi.

Ces deux modèles probabilistes simples sont complémentaires : un est précis
sans couvrir une large partie du corpus, alors que l'autre permet d'assigner un
rôle à chaque verbe. Cependant, la faible précision du second modèle nous
empêche de l'utiliser en l'état. % TODO que ce soit clair + future work

\section{Gestion de la voix passive}

Une analyse d'erreur a révélé que la voix passive était une source d'erreur
important dans notre corpus FrameNet. En effet, VerbNet n'encode pas la voix
passive qui est un phénomène syntaxique : c'est à l'analyse syntaxique que les
sujets et objets syntaxiques doivent être identifié correctement. Une fois
identifiés, c'est le bon sujet qui doit être comparé avec le sujet VerbNet.
Cependant, la plupart des analyseurs syntaxiques n'identifient pas de tels
liens de syntaxe profonde (voix passive, coordination, etc.). Il est donc
important d'avoir une étape intermédiaire entre l'analyse syntaxique et
l'annotation en rôles sémantiques \citep{bonfante2011modular,
ribeyre2013systeme}. Cette étape intermédiaire pourra identifier les sujets et
objets profonds de tous les verbes considérés, évitant ainsi des erreurs lors
de l'annotation en rôles sémantiques. 

Afin de valider cette hypothèse, nous nous sommes concentrés sur la voix
passive qui était le phénomène de syntaxe le plus présent dans notre corpus.
% TODO le faire correctement en allant vers VerbNet plutôt qu'en changeant
% VerbNet
Nous avons ainsi transformé les frames VerbNet dans les phrases impliquant une
voix passive, c'est-à-dire les verbes au participe passé gouvernés par une
forme du verbe \emph{to be}. Étant donné une frame VerbNet telle que
\textit{NP.Agent V NP.Recipient NP.Theme}, nous produisons :

\begin{itemize}
    \item NP.Recipient V NP.Theme
    \item NP.Recipient V NP.Theme by NP.Agent
\end{itemize}

Ce sont ces frames VerbNet transformées qui sont utilisées lorsque qu'une voix
passive est utilisée. Cette opération donne de meilleurs résultats étant donné
qu'une voix passive mal détectée cause systématiquement une erreur
(Table~\ref{table:results}).

\section{Evaluation}
\label{srl:evaluation}

L'intérêt de l'algorithme ne réside pas dans l'annotation d'un large corpus
annoté tel que FrameNet pour lequel les méthodes d'apprentissage supervisées
sont les plus efficaces \citep{das2014frame}. En effet, cette méthode est
destinée à annoter de nouveaux domaines non annotés de manière efficace, ce qui
est l'objet du prochain chapitre~(Chapitre\ref{ch:domainsrl}). Néanmoins,
FrameNet est un point de comparaison utilisé par de nombreux systèmes
d'annotation en rôles sémantiques, et il est intéressant d'analyser la
différence entre différents types d'algorithmes.

FrameNet et VerbNet n'utilisant ni la même séparation en rôles ni la même
séparation en classes, le mapping VerbNet-FrameNet maintenu par le projet
SemLink est utilisé. Il est possible que la création de ce mapping ait causé
directement ou indirectement l'ajout de nouvelles information dans VerbNet.
Autrement dit, il est possible que VerbNet soit davantage tourné vers FrameNet
qu'un autre corpus, et que par conséquent les résultats soient meilleurs que
pour un autre corpus. Néanmoins, si c'est le cas, c'est une situation
encourageante : il suffirait d'ajouter des informations dans VerbNet pour avoir
de meilleurs scores dans d'autres domaines.

Le corpus FrameNet est équilibré et inclue des textes de diverses sources : le
Wall Street Journal, les corpus AQUAINT et MASC, ainsi que d'autres textes
divers.

\subsection{Corpora and tools}

Nous utilisons le corpus full-text de FrameNet 1.5 (mais pas le corpus
d'exemples), VerbNet 3.2, le mapping VerbNet-FrameNet 1.2.2c
\footnote{\url{http://verbs.colorado.edu/semlink/1.2.2c/vn-fn/}}. Seuls les
arguments Core sont annotés : ils correspondent à la philosophie de VerbNet sur
ce point et correspondent en pratique aux arguments VerbNet.

% TODO de toute façon il faut utiliser ce qui est fourni par SEMAFOR
En travaillant sur la tâche complète qui inclut l'identification des arguments,
nous utilisons le parser MST dans sa version 0.5.0
\citep{mcdonald2006multilingual} entraîné sur une version modifiée du Wall
Street Journal. Nous avons d'abord transformé l'encodage des syntagmes nominaux
\footnote{\url{http://sydney.edu.au/engineering/it/~dvadas1/}}
\citep{vadas2007adding} puis appliqué l'outil de conversion du LTC pour une
conversion au format CoNLL
\footnote{\url{http://nlp.cs.lth.se/software/treebank_converter/}}
\citep{johansson2007extended}.

FrameNet incluant des parties du corpus du Wall Street Journal, nous avons
supprimé les fichiers 0558, 0089, 0456, 1778, 1286 et 1695 avant d'entraîner
l'analyseur syntaxique. Ceci évite à l'analyseur d'avoir à analyser des phrases
déjà observées dans le corpus d'entraînement, ce qui pourrait améliorer les
résultats artificiellement.

Enfin, les étiquette morphosyntaxique de FrameNet, utilisées lors de l'analyse
syntaxique, ont étés converties de la convention BNC à la convention WSJ en
utilisant des règles manuelles (Table~\ref{table:tagset_rules}). Sur les six
fichiers du Wall Street Journal, ceci réduit les erreurs de 23\% à 3\%.

\begin{table}[hb]
    \centering
    \begin{tabular}{ccc|ccc}
        \toprule
        JJ   &$\to$& ADJ    & JJR  &$\to$& NP     \\
        JJS  &$\to$& NP     & MD   &$\to$& S      \\
        NN   &$\to$& NP     & NNP  &$\to$& NP     \\
        NNPS &$\to$& NP     & NNS  &$\to$& NP     \\
        NP   &$\to$& NP     & NPS  &$\to$& NP     \\
        PP   &$\to$& PP     & PRP  &$\to$& NP     \\  
        RB   &$\to$& ADV    & TO   &$\to$& to S   \\
        VB   &$\to$& S      & VBD  &$\to$& S      \\
        VBG  &$\to$& S\_ING & VBN  &$\to$& ADJ    \\
        VBP  &$\to$& S      & VBZ  &$\to$& S      \\
        WDT  &$\to$& NP     & \$   &$\to$& NP     \\  
        CD   &$\to$& NP     & DT   &$\to$& NP     \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:tagset_rules}BNC to WSJ conversion rules}
\end{table}

% FUTURE SEMAFOR MXPOST/MST

\subsection{Evaluation procedure}

\begin{table*}[t!]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Task                                           & F1        & Accuracy \\
        \midrule
        FM                                             & 70.48\%   & 53.09\%  \\
        FM + predicate-slot (gold args)                & 72.02\%   & 58.28\%  \\
        FM + passive + predicate-slot (gold args)      & 76.40\%   & 62.72\%  \\
        \midrule
        Identification + FM                            & 46.75\%   & 29.12\%  \\
        Identification + FM + predicate-slot           & 46.78\%   & 33.49\%  \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:results}Results on different tasks. FM is frame matching. Lines with \emph{passive} include the passsive voice detection. Identification is argument identification.}
\end{table*}

We feed each FrameNet sentence in the corpus to our system which performs
semantic role labeling. For each role filler annotated
in the FrameNet corpus with a verbal predicate, we use the mapping to know what
is the set of possible VerbNet roles given the FrameNet frame. This is possibly
ambiguous, mostly because a FrameNet frame can refer to multiple VerbNet
classes: we don't evaluate against those roles.

Another difficulty is that the mapping isn't complete: some VerbNet classes
cannot be mapped from FrameNet to VerbNet. Indeed, only 4605 out of 10052 roles
are mapped: we only evaluate against those frames.

We measure precision, recall, and accuracy of correct role/role filler
associations out of the FrameNet ones that have been converted to VerbNet-style
roles. 10\% of the corpus was used as a test set, while the other 90\% were
manually scanned to check for issues in our algorithm.

Table~\ref{table:results} shows results on different tasks. The first three
tasks are evaluated on gold arguments: argument identification was not needed, which
definitely helps the models. The following two tasks are evaluated on the full
frame-semantic parsing task: arguments are identified automatically based on
automatic parses. The full model is "Identification + FM + predicate-slot": it
associates argument identification, frame matching and the predicate-slot
probabilistic model.

The main takeaway is that argument identification needs to be improved
significantly to help our model reach acceptable levels of performance. The
first issue is that only around 76\% of arguments are present in our dependency
parses. The second issue is the heuristics we used for argument identification:
they need to be analysed more thoroughly or replaced with alternatives
approaches \citep{abend2009unsupervised}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Model          & Precision & Coverage \\
        \midrule
        slot           & 52.45\% & 100\% \\
        predicate-slot & 68.33\% & 38.33\% \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:probabilisticresults}Results for probabilistic models}
\end{table}

Table~\ref{table:probabilisticresults} highlights the complementary nature of
our models: the predicate slot model has better precision but lower coverage
than the slot model.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{fig/slot-predicate-percents.png}
    \caption{\label{fig:slot_predicate} slot-predicate model performance when training over a part of the training corpus: from 0 to 100\%.}
\end{figure}

Figure~\ref{fig:slot_predicate} highlights that we don't need the whole corpus
to attain this level of performance. This is interesting because our methods
can operate efficiently on a small domain-specific corpus and because the full
potential of this corpus is not fully realized.

\subsection{SEMAFOR comparison}

SEMAFOR \citep{das2014frame} is currently the state-of-the-art supervised
frame-semantic parser on the SemEval test sets and FrameNet full-text corpus.
It annotates all FrameNet parts-of-speech while we only concentrate on verbs.
It achieves 46.49\% F1 score on the full task, which should be compared with
the 25\% F1 score of our system.  SEMAFOR uses three stages for semantic
parsing:

\begin{itemize}
    \item target identification
    \item frame identification
    \item argument identification
\end{itemize}

As such, it cannot be compared directly with our system which does not solve
the task using the same structure. However, it is interesting to note how
important is the training data: for frame identification with gold
targets\footnote{Results for automatic targets were only given for the SemEval
2007 dataset}, the same models grows from 74.21\% to 90.51\% in F1-score for
frame identification when switching from the SemEval 2007 dataset to the
FrameNet 1.5 dataset. Likewise, for argument identification and gold
frames\footnotemark[\value{footnote}], the results grow from 48.09\% to
68.93\%. The size of the training data is extremely important, while our
approach is better suited to domain adaptation where large annotated corpora
are seldom available.

\section{Future work}

Future includes evaluating our approach on domain-specific corpora such as the
Kicktionary \citep{schmidt2009kicktionary}, the Robocup dataset
\citep{chen2008learning} or the Situated Language dataset
\citep{bordes2010towards} and compare our method with existing
domain-specific semantic role labeling work
\citep{wyner2010lexical,hadouche2011annotation,goldwasser2013leveraging}.

We also plan to incorporate more domain-specific information such as semantic
similarity between existing role fillers to detect roles that are placed in an
unusual way.

Finally, in the same way that handling the passive voice produced better
results, we plan to integrate deep structure handling to our system. A common
case of errors is coordination. When two verbs share the same subject, the
syntactic analysis should properly link from each verb to the subject. Here are
two examples: first with the verbs blunder and, then with the verbs steal and
share:

\begin{itemize}
    \item You are not fair when you belittle Sheik Bin Baz 's blunder and
          exaggerate the one by Sheik Maqdasi ...
    \item Hostile and even friendly nations routinely steal information from
          U.S. companies and share it with their own companies
\end{itemize}

More specifically, we currently plan to integrate the system from
\citep{ribeyre2013systeme} as it handles complex deep structure situations
by adding simple rules to the system to take into account new syntactic
constructions, and will allow to handle all considered deep structure links in
a cohesive way.

\section{Conclusion}

We have implemented a knowledge-based semantic role labeling system. We used
publicly available versions of data and tools which make our work easily
reproducible, now and in the future. We have started to improve the basic
algorithm with enhancements that improve its results. The current results are
probably still insufficient to improve the results of natural language
processing applications such as information retrieval or text summarization,
but the foreseeable improvements make the approach promising. The independence
of the approach with respect to annotated corpus makes it interesting even if
raw performance is as expected lower than the one of supervised approaches.
Besides the future work above, our forthcoming introduction of corpus-based
syntactico-semantic selectional restrictions is a next step.
