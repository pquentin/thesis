% vim: set spelllang=fr:
\chapter{Annotation en rôles sémantiques basée sur la connaissance}
\label{ch:srl}


%\citep{simmons1973semantic} is the earliest work on Semantic Role Labeling.
%Already based on \citep{fillmore1968case}, it parsed a sentence into what we
% know call semantic roles. (check)

Contrairement aux autres ressources pour l'annotation en rôles sémantiques,
VerbNet couvre à la fois l'ensemble des verbes fréquents du vocabulaire tout en
étant conçu sur des préceptes robustes le rendant utile pour les tâches de
Traitement Automatique des Langues (Section~\ref{ch:verbnet:sec:verbnet}).

La tâche d'annotation en rôles sémantiques a reçu beaucoup d'attention ces
dernières années, à la fois pour les approches supervisées et semi-supervisées.
Cependant, les approches basées sur la connaissance ont été négligées alors
qu'elles sont complémentaires par rapport aux autres approches. Ce chapitre
présente un système d'annotation en rôles sémantiques qui se veut simple et
facile à reproduire. Nous montrons aussi comment la prise en compte de la voix
passive réduit le taux d'erreur de 15.7~\%, ce qui met en avant les
possibilités offertes par un tel système : une approche simple qui facilite
l'analyse des erreurs, qui n'a pas besoin d'un corpus annoté manuellement et
qui n'est a priori pas spécifique à un domaine en particulier.

 Les chapitres suivants montreront l'utilisation du système dans des contextes
différents : en domaine spécifique (Chapitre~\ref{ch:domainsrl}) et pour le
français (Chapitre~\ref{ch:frenchsrl}).


\section{Tâche}

L'objectif de ce chapitre est de présenter notre système d'annotation en rôles
sémantiques dans un cadre général en l'utilisant sur le corpus FrameNet.
Considérons l'exemple suivant :

\begin{quote}
However, in 2002 Russia declared it will eliminate its tactical nuclear weapons by the end of 2004.
\end{quote}

L'objectif est d'aboutir à la représentation suivante :

\begin{itemize}
    \item \emph{declare} déclenche la frame Statement dont les rôles sont :
    \begin{itemize}
        \item Speaker : Russia
        \item Message : it will eliminate its tactical nuclear weapons by the end of 2004
        \item Time : in 2002
    \end{itemize}
    \item \emph{eliminate} déclenche la frame Removing dont les rôles sont :
    \begin{itemize}
        \item Agent: it
        \item Theme: its tactical nuclear weapons
        \item Source: \emph{non instancié}
    \end{itemize}
\end{itemize}

Toute interprétation supplémentaire est en dehors du cadre de l'annotation en
rôles sémantiques, ce qui est la raison pour laquelle la tâche est aussi connue
sous le nom d'analyse sémantique de surface. Il est néanmoins intéressant de
garder à l'esprit les applications possibles de tels travaux. Par exemple, un
système de question-réponse pourrait utiliser la représentation de ces deux
\emph{frames} pour répondre à la question \emph{Does Russia possess tactical
nuclear weapons ?}. L'annotation ci-dessous est une information utile, mais
elle ne serait pas suffisante pour répondre à la question : il faut aussi
comprendre la question, annoter les coréférences (\emph{it} fait référence à
\emph{Russia}), établir la crédibilité du \emph{Speaker}, s'intéresser à
d'autres phrases potentiellement contradictoires, etc.

\section{Algorithme}

L'algorithme présenté est similaire au système de
\cite{swier2004unsupervised,swier2005exploiting}. Cependant, dix ans après, il
est important d'évaluer à nouveau cette approche pour savoir où elle se situe
par rapport à l'état de l'art et comprendre les améliorations possibles.

Les informations de VerbNet (Chapitre~\ref{ch:verbnet}) sur l'interface entre
la syntaxe et la sémantique sont directement utilisées. La notation NP.Agent V
NP.Theme indique que dans certaines phrases de type NP V NP (eg. \emph{Sally
pushed the chair}), le premier syntagme nominal (\emph{Sally}) est Agent alors
que le second (\emph{the chair}) est Theme.

Pour une phrase donnée, l'algorithme commence par identifier les verbes de
cette phrase. Pour chacun de ces verbes, un ensemble de classes VerbNet est
identifié. Ainsi, pour la phrase d'exemple ci-dessus, les classes possibles
pour le verbe \textit{declared} sont \texttt{declare-29-4-1-1-1},
\texttt{say-37.3-1} et \texttt{reflexive\_appearance-48.1.2}. Le choix correct
est \texttt{say-37.3-1}, mais il n'est pas possible de le déterminer avant de
considérer les frames de ces différentes classes.

Par exemple, \texttt{reflexive\_appearance-48.1.2} contient la frame NP.Agent V
NP.Theme. Ainsi, un verbe :
\begin{itemize}
    \item dont le sens appartient à la classe \texttt{reflexive\_appearance-48.1.2}
    \item et utilisé avec un sujet syntagme nominal et un objet syntagme nominal
\end{itemize}
aura un sujet Agent et un objet Theme.

La phrase d'exemple ci-dessous ne correspond pas à NP V NP mais à NP V that S.
La seule occurrence de cette frame VerbNet est dans \texttt{say-37.7-1} :
NP.Agent V that S.Topic ("He ordered that he go"). Il est donc possible
d'établir que la classe VerbNet qui convient est \texttt{say-37.7-1}, que
\emph{Russia} est Agent, et que \textit{it will eliminate its tactical nuclear
weapons by the end of 2004} est Topic. Enfin, le mapping entre VerbNet et
FrameNet nous informe que la classe VerbNet 37.7 correspond à la frame
Statement, que Agent correspond à Speaker et que Topic correspond à Topic.
Nous aboutissons ainsi pour le verbe \emph{eliminate} à la représentation
ci-dessus.

Dans d'autres cas l'algorithme ne peut pas déterminer la classe correcte car
plusieurs classes peuvent inclure les mêmes cadres de sous-catégorisation. Par
exemple, \texttt{reflexive\_appearance-48.1.2} et \texttt{say-37.7-1}
contiennent toutes les deux le cadre NP V NP. Sans corpus annoté, ces
ambiguïtés ne peuvent pas être résolues. Cependant, une fois qu'une première
série de correspondances a été effectuée, il est possible d'utiliser les
connaissances du domaine étudié pour annoter de nouveaux syntagmes nominaux.

Les sections suivantes décrivent le déroulement précis de l'algorithme en le
découpant en trois étapes.

\subsection{Identification des arguments}

% TODO predicate identification ?

Cette première étape identifie les syntagmes amenés à recevoir un rôle
sémantique lors de l'annotation. La phrase est d'abord analysée syntaxiquement
afin de déterminer les syntagmes qui peuvent jouer un rôle sémantique. Pour
cette étape, nous suivons \cite{lang2011unsupervised} qui propose huit règles
proposant chacune des syntagmes candidats. Ces règles génèrent trop de
candidats : il faudra par la suite soit assigner un rôle au candidat soit
déclarer qu'il n'a pas de rôle dans la phrase.

% TODO Examples
% TODO Règles vraiment utilisées

Cette première étape est facultative : afin de mieux comprendre les
performances des étapes suivantes, ce seront parfois les arguments parfaits de
la vérité-terrain qui seront utilisés lors de l'évaluation. En effet,
l'objectif est d'évaluer l'apport de VerbNet à la tâche de l'annotation en
rôles sémantiques, et l'identification des arguments, bien qu'une partie
intégrante de tout système d'annotation en frames, n'est pas de l'annotation en
rôles sémantiques \citep{das2010probabilistic}.

\subsection{Frame matching}

Cette étape associe zero, un ou plusieurs rôles sémantiques à chaque syntagme
candidat identifié lors de l'étape précédente.

% TODO dans l'état de l'art ?
Cette étape inclut deux étapes traditionnellement utilisées dans les systèmes
d'annotation en rôles sémantiques : \citep{gildea2002automatic,das2014frame} :
l'identification des frames FrameNet et l'assignation de rôles aux arguments
identifiés.  En effet, c'est l'assignation des rôles qui permet d'identifier la
bonne classe VerbNet et donc la frame FrameNet.

Premièrement, les syntagmes candidats sont représentés au format VerbNet. Par
exemple, si trois syntagmes nominaux ont été identifiés comme arguments, dont
un avant le verbe, la représentation VerbNet de la phrase devient NP V NP NP.
Pour comparer la représentation VerbNet de la phrase aux frames VerbNet, nous
identifions toutes les classes VerbNet incluant le prédicat. Par exemple, le
prédicat \textit{classify} est présent dans deux classes VerbNet~:
\textit{characterize-29.2} et \textit{classify-29.10}. Les frames possibles
sont~:

\begin{itemize}
    \item NP.Agent V NP.Theme (as) S\_ING.Attribute
    \item NP.Agent V NP.Theme to be ADJ.Attribute
    \item NP.Agent V NP.Theme as PP.Attribute
    \item NP.Agent V NP.Theme
    \item NP.Agent V NP.Theme as PP.Goal
    \item NP.Agent V NP.Theme in PP.Location
\end{itemize}

% TODO documenter la gestion des PP - demande de savoir comment on unifie avec
% domainsrl
Prenons un example. La phrase \emph{The company also classifies short and wide
radius ruts according to their severity} est transformée en NP V NP according
PP.

Dans ce cas, seuls les deux premiers syntagmes peuvent être mis en
correspondance avec les cadres de sous-catégorisation VerbNet. Pour ces deux
arguments, la correspondance n'est pas ambiguë : les deux premiers syntagmes
jouent le rôle d'Agent et de Theme. Il n'y a pas de correspondance possible
pour le troisième argument : Verbnet n'encode pas \emph{according} comme une
préposition possible alors que \emph{in} et \emph{as} sont acceptées. Les
auteurs de VerbNet travaillent actuellement sur la couverture de la ressource
en ajoutant des informations syntaxiques et lexicales issues de très larges
corpus \citep{bonial2013expanding}. Le résultat de ces travaux n'est cependant
pas encore disponible.

% TODO un exemple de mapping ambigu
% TODO meilleure terminologie pour ambigu/non ambigu

\subsection{Modèles de probabilité}
\label{subsec:probability}

Maintenant qu'une partie du corpus a été annotée, nous pouvons utiliser cette
information pour annoter de nouveaux arguments ambigus. La méthode reste non
supervisée : bien que nous entraînons une forme simple d'algorithme supervisé,
nous le faisons sur des données non annotées simplement obtenues de manière
automatique sur le corpus existant. Nous faisons ici l'hypothèse qu'un corpus
entier est à annoter, mais dans le cas où un corpus serait inexistant, il
serait possible d'abandonner cette étape ou d'alimenter les modèles de
probabilité au fur et à mesure des annotations.

Chacun de ces modèles de probabilité assigne une probabilité aux différents
rôles possibles selon les rôles identifiés de manière non ambiguë auparavant.
Une correspondance est ambiguë quand deux rôles ou plus sont possibles, et est
non ambiguë quand un seul rôle est possible. Deux modèles de probabilités issus
de \citep{swier2005exploiting} sont considérés.

% TODO belles formules comme dans gildea2002automatic

% TODO clarifier
Dans ces deux modèles, la fonction est la fonction grammaticale identifiée
d'après l'analyse syntaxique : si un syntagme apparaît avant le verbe, il est
sujet, si il est après le verbe, et il est objet, et ainsi de suite. Les
syntagmes prépositionnels sont traités à part : la préposition qui introduit le
syntagme est considérée à part.

\paragraph{prédicat-fonction}

Le modèle \emph{prédicat-fonction} utilise l'information du prédicat et la
fonction grammaticale détectée. Par exemple, dans notre corpus
(section~\ref{subsec:corpora_tools}), l'objet direct du verbe 'négliger' est le
plus souvent 'Theme'.

La précision pour ce modèle est forte, mais il n'assigne des rôles que pour
40~\% des arguments : dans les autres cas, nous ne disposons pas d'assez
d'informations pour cette paire (prédicat, fonction grammaticale) précise.

\paragraph{fonction}

Étant donné que \emph{prédicat-fonction} ne donne pas de résultats dans tous les
cas, un modèle plus simple a aussi été utilisé : \emph{fonction}. Ce modèle se
concentre uniquement sur la fonction grammaticale (en incluant la préposition
considérée). Par exemple, un syntagme prépositionnel \emph{of NP} correspond
dans l'ordre à Attribute, Theme et Topic dans le corpus utilisé dans cette
expérience. Dans un contexte précis, c'est le premier rôle possible qui sera
choisi. Ainsi, si l'étape précédente nous indique que Topic et Recipient sont
possibles, Topic sera choisi.

Ces deux modèles probabilistes simples sont complémentaires : un est précis
sans couvrir une large partie du corpus, alors que l'autre permet d'assigner un
rôle à chaque verbe. Cependant, la faible précision du second modèle nous
empêche de l'utiliser en l'état. % TODO que ce soit clair + future work

\section{Gestion de la voix passive}
\label{sec:passif}

Une analyse d'erreur a révélé que la voix passive était une source d'erreur
importante dans notre corpus FrameNet. En effet, VerbNet n'encode pas la voix
passive qui est un phénomène syntaxique : c'est à l'analyse syntaxique que les
sujets et objets syntaxiques doivent être identifiés correctement. Une fois
identifiés, c'est le sujet profond qui doit être comparé avec le sujet VerbNet.
Cependant, la plupart des analyseurs syntaxiques n'identifient pas de tels
liens de syntaxe profonde (voix passive, coordination, etc.). Il est donc
important d'avoir une étape intermédiaire entre l'analyse syntaxique et
l'annotation en rôles sémantiques \citep{bonfante2011modular,
ribeyre2013systeme}. Cette étape intermédiaire pourra identifier les sujets et
objets profonds de tous les verbes considérés, évitant ainsi des erreurs lors
de l'annotation en rôles sémantiques. 

Afin de valider cette hypothèse, nous nous sommes concentrés sur la voix
passive qui était le phénomène de syntaxe le plus présent dans notre corpus.
% TODO le faire correctement en allant vers VerbNet plutôt qu'en changeant
% VerbNet
Nous avons ainsi transformé les frames VerbNet dans les phrases impliquant une
voix passive, c'est-à-dire les verbes au participe passé gouvernés par une
forme du verbe \emph{to be}. Étant donné une frame VerbNet telle que
\textit{NP.Agent V NP.Recipient NP.Theme}, nous produisons :

\begin{itemize}
    \item NP.Recipient V NP.Theme
    \item NP.Recipient V NP.Theme by NP.Agent
\end{itemize}

Ce sont ces frames VerbNet transformées qui sont utilisées lorsque qu'une voix
passive est utilisée, ce qui améliore les résultats (Table~\ref{table:results}).

\section{Evaluation}
\label{srl:evaluation}

L'intérêt de l'algorithme ne réside pas dans l'annotation d'un large corpus
annoté tel que FrameNet pour lequel les méthodes d'apprentissage supervisées
sont les plus efficaces \citep{das2014frame}. En effet, notre méthode est
destinée à annoter de nouveaux domaines ne disposant pas de corpus annoté
(Chapitre~\ref{ch:domainsrl}). Néanmoins, FrameNet est un point de comparaison
utilisé par de nombreux systèmes d'annotation en rôles sémantiques auxquels il
est intéressant de se comparer.

FrameNet et VerbNet n'utilisant ni la même séparation en rôles ni la même
séparation en classes, le mapping VerbNet-FrameNet maintenu par le projet
SemLink est utilisé. Il est possible que la création de ce mapping ait causé
directement ou indirectement l'ajout de nouvelles information dans VerbNet.
Autrement dit, il est possible que VerbNet soit davantage tourné vers FrameNet
qu'un autre corpus, et que par conséquent les résultats soient meilleurs que
pour un autre corpus. Néanmoins, si c'est le cas, c'est une situation
encourageante : il suffirait d'ajouter des informations dans VerbNet pour avoir
de meilleurs scores dans d'autres domaines.

Le corpus FrameNet est équilibré et inclut des textes de diverses sources : le
Wall Street Journal, les corpus AQUAINT et MASC, ainsi que d'autres textes
divers.

\subsection{Détails expérimentaux}
\label{subsec:corpora_tools}

Nous utilisons le corpus full-text de FrameNet 1.5 (mais pas le corpus
d'exemples), VerbNet 3.2, le mapping VerbNet-FrameNet 1.2.2c
\footnote{\url{http://verbs.colorado.edu/semlink/1.2.2c/vn-fn/}}. Seuls les
arguments Core sont annotés étant donné que ce sont généralement les arguments
annotés par VerbNet.

% TODO de toute façon il faut utiliser ce qui est fourni par SEMAFOR
En travaillant sur la tâche complète qui inclut l'identification des arguments,
nous utilisons le parser MST dans sa version 0.5.0
\citep{mcdonald2006multilingual} entraîné sur une version modifiée du Wall
Street Journal. Nous avons d'abord transformé l'encodage des syntagmes nominaux
\footnote{\url{http://sydney.edu.au/engineering/it/~dvadas1/}}
\citep{vadas2007adding} puis appliqué l'outil de conversion du LTH pour une
conversion au format CoNLL
\footnote{\url{http://nlp.cs.lth.se/software/treebank_converter/}}
\citep{johansson2007extended}.

FrameNet incluant des parties du corpus du Wall Street Journal, nous avons
supprimé les fichiers 0558, 0089, 0456, 1778, 1286 et 1695 du corpus
d'entraînement de l'analyseur syntaxique. Ceci évite à l'analyseur d'avoir à
analyser des phrases déjà observées dans le corpus d'entraînement, ce qui
aurait pu améliorer les résultats artificiellement.

Enfin, les étiquettes morphosyntaxique de FrameNet, utilisées lors de l'analyse
syntaxique, ont étés converties de la convention BNC à la convention WSJ en
utilisant des règles manuelles (Table~\ref{table:tagset_rules}). Sur les six
fichiers du Wall Street Journal, ceci réduit les erreurs de 23\% à 3\%.

\begin{table}[ht]
    \centering
    \begin{tabular}{ccc|ccc}
        \toprule
        JJ   &$\to$& ADJ    & JJR  &$\to$& NP     \\
        JJS  &$\to$& NP     & MD   &$\to$& S      \\
        NN   &$\to$& NP     & NNP  &$\to$& NP     \\
        NNPS &$\to$& NP     & NNS  &$\to$& NP     \\
        NP   &$\to$& NP     & NPS  &$\to$& NP     \\
        PP   &$\to$& PP     & PRP  &$\to$& NP     \\  
        RB   &$\to$& ADV    & TO   &$\to$& to S   \\
        VB   &$\to$& S      & VBD  &$\to$& S      \\
        VBG  &$\to$& S\_ING & VBN  &$\to$& ADJ    \\
        VBP  &$\to$& S      & VBZ  &$\to$& S      \\
        WDT  &$\to$& NP     & \$   &$\to$& NP     \\  
        CD   &$\to$& NP     & DT   &$\to$& NP     \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:tagset_rules}BNC to WSJ conversion rules}
\end{table}

% FUTURE SEMAFOR MXPOST/MST

\subsection{Procédure d'évaluation}

\begin{table*}[ht]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Task                                           & F1        & Accuracy \\
        \midrule
        FM                                             & 70.48\%   & 53.09\%  \\
        FM + prédicat-fonction (gold args)                & 72.02\%   & 58.28\%  \\
        FM + passive + prédicat-fonction (gold args)      & 76.40\%   & 62.72\%  \\
        \midrule
        Identification + FM                            & 46.75\%   & 29.12\%  \\
        Identification + FM + prédicat-fonction           & 46.78\%   & 33.49\%  \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:results}Results on different tasks. FM is frame matching. Lines with \emph{passive} include the passsive voice detection. Identification is argument identification.}
\end{table*}

Les rôles annotés de chaque phrase FrameNet sont d'abord transformés en rôles
VerbNet. Pour chaque classe FrameNet, un rôle FrameNet peut correspondre à 0, 1
ou plusieurs rôles VerbNet :

\begin{itemize}

    \item zéro : n'y pas de correspondance pour les frames FrameNet trop éloignées
    des classes VerbNet. C'est un problème répandu :  seulement 4605 rôles sur le
    10052 rôles présents dans FrameNet ont au moins une association VerbNet.
    % TODO Cependant, en pratique, dans le corpus FrameNet, XX~\% des occurrences
    % de rôles sont associées à plus d'un rôle VerbNet

    \item plusieurs : Il y a plusieurs correspondances quand un rôle FrameNet
    est ambigu par rapport à un rôle VerbNet. Les rôles FrameNet étant définis plus
    précisément, c'est un problème rare. % TODO dans quelle mesure ?

\end{itemize}

Dans tous les autres cas, un rôle FrameNet correspond à un rôle VerbNet unique,
et c'est ce rôle VerbNet que notre système doit déterminer. Nous mesurons la
précision, le rappel et l'exactitude (\emph{accuracy}) des associations
role/syntagmes candidats.  10~\% du corpus a été utilisé pour obtenir les
résultats, le reste est un corpus "d'entraînement" : le corpus qui a été
examiné manuellement pour identifier les erreurs de notre algorithme.

La table~\ref{table:results} montre les résultats des deux tâches sur
lesquelles nous nous évaluons : le frame matching seul d'une part et le frame
matching accompagné de l'identification des arguments en amont d'autre part. Le
frame matching seul utilise les arguments golds : on sait que ce syntagme joue
un rôle, il faut alors déterminer lequel. L'identification des arguments
implique que le texte de départ est une phrase brute : il faut l'analyser
syntaxiquement, identifier les arguments, puis réaliser le frame matching à
proprement parler.

Enfin, différents modèles sont appliqués :

\begin{itemize}

    \item \emph{passif} indique que la voix passive a été prise en compte
    (section~\ref{sec:passif}).

    \item \emph{prédicat-fonction} indique que le modèle de probabilité
    \emph{prédicat-fonction} a été appliqué (section~\ref{subsec:probability}).

\end{itemize}

\section{Résultats}

\subsection{Analyse des résultats}

Premièrement, l'identification des arguments doit être améliorée de manière
significative car elle pénalise les étapes ultérieures.

La première raison est que seulement 76~\% des syntagmes jouant un rôle font
effectivement partie de l'analyse syntaxique.
% TODO inclure les mots seuls ? Vérifier la citation de SEMAFOR
La seconde raison provient des heuristiques utilisées pour l'identification
elle-même : une analyse plus poussée permettrait de mieux comprendre les
erreurs qu'elles causent. Des alternatives existent, supervisées ou non
supervisées \citep{abend2009unsupervised}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Model          & Precision & Coverage \\
        \midrule
        fonction           & 52.45\% & 100\% \\
        prédicat-fonction & 68.33\% & 38.33\% \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:probabilisticresults}Résultats pour les modèles de probabilité}
\end{table}

La table~\ref{table:probabilisticresults} montre la complémentarité de nos
modèles : le modèle prédicat-fonction est précis mais couvre peu de verbes, alors
que le modèle fonction couvre l'ensemble des verbes.
% TODO narrative logique, soit on prend soit on prend pas.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{fig/slot-predicate-percents.png}
    \caption{\label{fig:fonction_predicate}Performance du modèle fonction-prédicat en entraînant le modèle sur une sous-partie du corpus : de 0 à 100~\%.}
\end{figure}

La figure~\ref{fig:fonction_predicate} montre que ce niveau de performance ne
demande pas un gros corpus. C'est un enseignement intéressant pour deux
raisons~:

\begin{itemize}

    \item Un corpus de petite taille suffit pour obtenir cette performance, ce
    qui est adapté à des domaines où les corpus même non annotés peuvent être
    petits.

    \item L'algorithme ne fait pas de sur-apprentissage en ayant un biais fort
    et une variance faible, ce qui est souhaitable ici.

\end{itemize}

\subsection{Comparaison avec SEMAFOR}

% TODO scores sont en XX
SEMAFOR \citep{das2014frame} est l'état de l'art de l'annotation en rôle
sémantiques supervisée : c'est le système qui obtient les meilleurs résultats
sur le corpus full-text de FrameNet 1.5. Toutes les parties du discours sont
annotées alors que nous ne nous concentrons que sur les verbes. Sur la tâche
complète, SEMAFOR obtient un score F1 de XX~\%, score à comparer avec les XX\~
de notre système. Le système de SEMAFOR découpe la tâche en trois parties :

\begin{itemize}
    \item identification des prédicats déclencheurs
    \item identification des frames FrameNet
    \item identification des arguments et annotation en rôles sémantiques 
\end{itemize}

Une comparaison directe n'est pas possible étant donné que les tâches sont
découpées différemment dans notre système. Cependant, il est intéressant de
noter l'importance des données d'entraînement : pour l'identification des
frames FrameNet avec des prédicats golds\footnote{les résultats pour des
déclencheurs identifiés manuellement n'ont été donné que pour le corpus
SemEval, un sous-ensemble du corpus FrameNet 1.5 complet}, les mêmes modèles
grimpent de 74.21~\% à 90.51~\% quand la taille du corpus augmente en passant
du corpus SemEval (XXX phrases) au corpus FrameNet 1.5 (XXX phrases). De la
même manière pour l'identification des arguments avec des frames FrameNet gold,
les résultats augmente de 48.09~\% 68.83~\%. C'est très encourageant pour les
domaines disposant de très gros corpus, mais suggère que d'autres solutions
sont à identifier pour les domaines où de tels corpus ne sont pas disponibles.

\section{Travaux futurs}

La suite naturelle de ce travail est de l'appliquer à des domaines spécifiques
(Chapitre~\ref{ch:domainsrl}) pour les domaines du football, du réchauffement
climatique et de l'informatique.

Nous pensons aussi prendre en compte la similarité entre les remplisseurs déjà
identifiés et les remplisseurs pour lesquels un rôle reste encore à identifier
afin d'améliorer nos modèles de probabilité. En effet, l'information des cadres
de sous-catégorisation est cruciale pour identifier les arguments, mais
l'information sémantique concernant le contenu des remplisseurs est aussi utile
pour déterminer le rôle correct.
% TODO ref vers chapitre similarité ASFALDA

Enfin, de la même manière que la prise en compte de la voix passive a amélioré
les résultats, d'autres phénomènes de syntaxe profonde doivent être pris en
compte. La coordination est une autre source commune d'erreur. En effet, quand
deux verbes partagent le même sujet, une analyse syntaxique profonde indique à
chaque fois quel est le sujet profond. Voici deux exemples tirés de notre
corpus FrameNet :

\begin{itemize}
    \item You are not fair when you belittle Sheik Bin Baz 's blunder and
          exaggerate the one by Sheik Maqdasi ...
    \item Hostile and even friendly nations routinely steal information from
          U.S. companies and share it with their own companies
\end{itemize}

L'objectif est de traiter ces phénomènes de manière plus générale en intégrant
le système de \cite{ribeyre2013systeme} qui permet de prendre en compte de
nouveaux phénomènes en ajoutant de simple aux règles au système. Ainsi, les
différents phénomènes seront prise en compte de manière cohérente.

\section{Conclusion}

Nous avons implémenté un système d'annotation en rôles sémantiques basé sur la
connaissance. Nous avons utilisé des outils et des corpus disponibles
publiquement qui rendent notre travail facilement reproductible et facilitent
le travail de comparaison, maintenant et dans le futur. Nous avons commencé à
améliorer le système initial, montrant son potentiel. L'indépendance de
l'approche avec le corpus considéré la rendent attractive pour annoter des
domaines ne disposant que de peu ou pas de corpus annotés en rôles sémantiques.
