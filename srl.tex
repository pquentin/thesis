% vim: set spelllang=fr:
\chapter{Annotation en rôles sémantiques fondée sur la connaissance}
\label{ch:srl}


%\citep{simmons1973semantic} is the earliest work on Semantic Role Labeling.
%Already based on \citep{fillmore1968case}, it parsed a sentence into what we
% know call semantic roles. (check)

%Contrairement aux autres ressources pour l'annotation en rôles sémantiques,
%VerbNet couvre l'ensemble des verbes fréquents du vocabulaire tout en étant
%conçu sur des préceptes robustes le rendant utile pour les tâches de Traitement
%Automatique des Langues (Section~\ref{ch:verbnet:sec:verbnet}).

La tâche d'annotation en rôles sémantiques a reçu beaucoup d'attention ces
dernières années, à la fois pour les approches supervisées et semi-supervisées.
Cependant, les approches fondées sur la connaissance ont été négligées alors
qu'elles sont complémentaires par rapport aux autres approches. Ce chapitre
présente un système d'annotation en rôles sémantiques qui se veut facile à
reproduire. Dans ce système, la prise en compte de la voix passive réduit le
taux d'erreur de 15.7~\%. Ainsi, l'approche est simple, facilite l'analyse des
erreurs, n'a pas besoin d'un corpus annoté manuellement et n'est a priori pas
spécifique à un domaine en particulier.

Ce chapitre se concentre sur notre système d'annotation en rôles sémantiques
dans un cadre général en l'utilisant sur le corpus FrameNet anglais. Les
chapitres suivants montreront la versatilité de ce système dans des contextes
différents : en domaine spécifique (Chapitre~\ref{ch:domainsrl}) et pour le
français (Chapitre~\ref{ch:frenchsrl}).

\section{Tâche}

La figure~\ref{fig:srlrussia} est un exemple d'annotation en rôles sémantiques.
Pour une phrase donnée,  les prédicats verbaux sont identifiés, ainsi que la
% TODO qu'est-ce qu'une frame ?
frame qu'ils évoquent. Leurs arguments sémantiques (remplisseurs de rôles ou
simplement remplisseurs) sont identifiés. Chaque remplisseur remplit un rôle
précis identifié au préalable identifié dans la frame.

\begin{figure}[ht]
    \begin{quote}
    However, in 2002 Russia declared it will eliminate its tactical nuclear weapons by the end of 2004.
    \end{quote}

    L'objectif est d'aboutir à la représentation suivante :

    \begin{itemize}
        \item \emph{declare} déclenche la frame Statement dont les rôles sont remplis ainsi :
        \begin{itemize}
            \item Speaker : Russia
            \item Message : it will eliminate its tactical nuclear weapons by the end of 2004
            \item Time : in 2002
        \end{itemize}
        \item \emph{eliminate} déclenche la frame Removing dont les rôles sont remplis ainsi :
        \begin{itemize}
            \item Agent: it
            \item Theme: its tactical nuclear weapons
            \item Source: \emph{non instancié}
        \end{itemize}
    \end{itemize}
    \caption{\label{fig:srlrussia}Exemple d'annotation en rôles sémantiques}
\end{figure}

Toute interprétation supplémentaire est en dehors du cadre de l'annotation en
rôles sémantiques, ce qui est la raison pour laquelle la tâche est aussi connue
sous le nom d'analyse sémantique de surface (\emph{shallow semantic parsing}.
Il est néanmoins intéressant de garder à l'esprit les applications possibles de
tels travaux. Par exemple, un système de question-réponse pourrait utiliser la
représentation de ces deux \emph{frames} pour répondre à la question \emph{Does
Russia possess tactical nuclear weapons?}. L'annotation ci-dessous est une
information utile, mais elle ne serait pas suffisante pour répondre à la
question : il faut aussi comprendre la question, annoter les coréférences
(\emph{it} fait référence à \emph{Russia}), établir la crédibilité du
\emph{Speaker}, s'intéresser à d'autres phrases potentiellement
contradictoires, etc.

\section{Algorithme}

L'algorithme présenté est similaire au système de
\cite{swier2004unsupervised,swier2005exploiting}. Cependant, dix ans après, il
est important d'évaluer à nouveau cette approche pour savoir où elle se situe
par rapport à l'état de l'art et pour proposer certaines améliorations.

Les informations de VerbNet (Chapitre~\ref{ch:verbnet}) sur l'interface entre
la syntaxe et la sémantique sont directement utilisées. La notation NP.Agent V
NP.Theme indique que dans certaines phrases de type NP V NP (eg. \emph{Sally
pushed the chair}), le premier syntagme nominal (\emph{Sally}) est Agent alors
que le second (\emph{the chair}) est Theme. L'interprétation des rôles Theme et
Agent dépend de la classe VerbNet considérée.

Un analyseur linguistique étiquète la phrase à annoter en morphosyntaxe, puis
l'analyse syntaxiquement : c'est l'entrée de notre algorithme, la sortie étant
l'annotation en rôles montrée par exemple pour la Figure~\ref{fig:srlrussia}.

Pour une phrase donnée, l'algorithme commence par
identifier les verbes de cette phrase. Pour chacun de ces verbes, un ensemble
de classes VerbNet est identifié. Ainsi, pour la phrase d'exemple ci-dessus,
les classes possibles pour le verbe \textit{declared} sont
\texttt{declare-29-4-1-1-1}, \texttt{say-37.3-1} et
\texttt{reflexive\_appearance-48.1.2}. Le choix correct est
\texttt{say-37.3-1}, mais il n'est pas possible de le déterminer avant de
considérer les frames syntaxiques listées par VerbNet dans ces différentes
classes.

% TODO wait wat

Par exemple, \texttt{reflexive\_appearance-48.1.2} contient la frame NP.Agent V
NP.Theme. Ainsi, un verbe :
\begin{itemize}
    \item dont le sens appartient à la classe \texttt{reflexive\_appearance-48.1.2}
    \item et utilisé avec un sujet syntagme nominal et un objet syntagme nominal
\end{itemize}
aura un sujet Agent et un objet Theme.

La phrase d'exemple ci-dessous ne correspond pas à NP V NP mais à NP V that S,
\emph{that S} étant la notation VerbNet pour les complétives introduites par
\emph{that}. La seule occurrence de cette frame VerbNet est dans
\texttt{say-37.7-1} : NP.Agent V that S.Topic ("He ordered that he go"). Il est
donc possible d'établir que la classe VerbNet qui convient est
\texttt{say-37.7-1}, que \emph{Russia} est Agent, et que \textit{it will
eliminate its tactical nuclear weapons by the end of 2004} est Topic. Enfin, le
mapping entre VerbNet et FrameNet nous informe que la classe VerbNet 37.7
correspond à la frame Statement, que Agent correspond à Speaker et que Topic
correspond à Topic.  Nous aboutissons ainsi pour le verbe \emph{eliminate} à la
représentation ci-dessus.

Dans d'autres cas l'algorithme ne peut pas déterminer la classe correcte car
plusieurs classes peuvent inclure les mêmes cadres de sous-catégorisation. Par
exemple, \texttt{reflexive\_appearance-48.1.2} et \texttt{say-37.7-1}
contiennent toutes les deux le cadre NP V NP. Sans corpus annoté, ces
ambiguïtés ne peuvent pas être résolues. Cependant, une fois qu'une première
série de correspondances a été effectuée, il est possible d'utiliser les
connaissances du domaine étudié pour annoter de nouveaux syntagmes nominaux.

Les sections suivantes décrivent le déroulement précis de l'algorithme en le
découpant en trois étapes.

\subsection{Identification des arguments}

% TODO predicate identification ?

Cette première étape identifie les syntagmes amenés à recevoir un rôle
sémantique lors de l'annotation. La phrase est d'abord analysée syntaxiquement
afin de déterminer les syntagmes qui peuvent jouer un rôle sémantique. Pour
cette étape, nous suivons \cite{lang2011unsupervised} qui propose huit règles
basées sur l'analyse syntaxe de la phrase et aboutissant à une liste de
syntagmes candidats.

\begin{enumerate}
    \item Éliminer les déterminants, les marqueurs d'infinitifs, les conjonctions de coordinations et la ponctuation.
    \item Éliminer les candidats dont le chemin du prédicat au candidat finit par une coordination, une subordination, etc.
    \item Conserver les candidats qui sont les sujets les plus proches à gauche du prédicat et dont les relations du prédicat p à la tête t du candidat sont toutes montantes (g -> p)
    \item Éliminer les candidats  dont le chemin entre le prédicat et le candidat, sauf la dernière relation, contient une relation de sujet, une relation de modifieur adjectival, etc.
    \item Éliminer les verbes auxiliaires
    \item Conserver les candidats directement sous le prédicat
    \item Conserver les candidats si le chemin du prédicat au candidat traverse plusieurs noeuds verbaux
    \item Éliminer tous les candidats restants.
\end{enumerate}

% TODO check code Guilhem
% TODO exemples
% TODO appendix

Ces règles génèrent trop de candidats : il faudra par la suite soit assigner un
rôle au candidat soit déclarer qu'il n'a pas de rôle dans la phrase.

% TODO Examples
% TODO Règles vraiment utilisées

Cette première étape est facultative : afin de mieux comprendre les
performances des étapes suivantes, ce seront parfois les arguments parfaits de
la vérité-terrain qui seront utilisés lors de l'évaluation. En effet,
l'objectif est d'évaluer l'apport de VerbNet à la tâche de l'annotation en
rôles sémantiques, et l'identification des arguments, bien qu'une partie
intégrante de tout système d'annotation en frames, n'est pas de l'annotation en
rôles sémantiques \citep{das2010probabilistic} et ne fait pas partie de nos
contributions.
% TODO honêteté, c'est important mais on veut pas se mouiller

\subsection{Frame matching}

Cette étape associe zéro, un ou plusieurs rôles sémantiques à chaque syntagme
candidat identifié lors de l'étape précédente.

% TODO dans l'état de l'art ?
Cette étape inclut deux étapes traditionnellement utilisées dans les systèmes
d'annotation en rôles sémantiques : \citep{gildea2002automatic,das2014frame} :
l'identification des frames FrameNet puis l'assignation de rôles aux arguments
identifiés. Ici, au contraire, ce sont les contraintes syntaxiques définies
dans les frames VerbNet qui permettent de restreindre les classes VerbNet
possibles. Une fois la frame identifiée syntaxiquement, les rôles peuvent être
attribués aux syntagmes.

Premièrement, les syntagmes candidats sont représentés au format VerbNet. Par
exemple, si trois syntagmes nominaux ont été identifiés comme arguments, dont
un avant le verbe, la représentation VerbNet de la phrase devient NP V NP NP.
Pour comparer la représentation VerbNet de la phrase aux frames VerbNet, nous
identifions toutes les classes VerbNet incluant le prédicat. Par exemple, le
prédicat \textit{classify} est présent dans deux classes VerbNet~:
\textit{characterize-29.2} et \textit{classify-29.10}. Les frames possibles
sont~:

\begin{itemize}
    \item NP.Agent V NP.Theme (as) S\_ING.Attribute
    \item NP.Agent V NP.Theme to be ADJ.Attribute
    \item NP.Agent V NP.Theme as PP.Attribute
    \item NP.Agent V NP.Theme
    \item NP.Agent V NP.Theme as PP.Goal
    \item NP.Agent V NP.Theme in PP.Location
\end{itemize}

% TODO documenter la gestion des PP - demande de savoir comment on unifie avec
% domainsrl
Prenons un example. La phrase \emph{The company also classifies short and wide
radius ruts according to their severity} est transformée en NP V NP according
PP.

Dans ce cas, seuls les deux premiers syntagmes peuvent être mis en
correspondance avec les cadres de sous-catégorisation VerbNet. Pour ces deux
arguments, la correspondance n'est pas ambiguë : les deux premiers syntagmes
jouent le rôle d'Agent et de Theme. Il n'y a pas de correspondance possible
pour le troisième argument : VerbNet n'encode pas \emph{according} comme une
préposition possible alors que \emph{in} et \emph{as} sont acceptées. De tels
arguments ne sont pas annotés lors de cette étape. C'est ici un problème de
couverture. Les auteurs de VerbNet travaillent actuellement sur la couverture
de la ressource en ajoutant des informations syntaxiques et lexicales issues de
très larges corpus \citep{bonial2013expanding}. Le résultat de ces travaux
n'est cependant pas encore disponible.

% TODO un exemple de mapping ambigu
% TODO meilleure terminologie pour ambigu/non ambigu

\subsection{Modèles de probabilité}
\label{subsec:probability}

Maintenant que la partie non-ambiguë du corpus a été annotée, nous pouvons
utiliser cette information pour annoter de nouveaux arguments qui sont pour
l'instant restés ambigus. La méthode reste non supervisée : bien que nous
entraînions une forme simple d'algorithme supervisé, nous le faisons sur des
données non annotées simplement obtenues de manière automatique sur le corpus
existant. Nous faisons ici l'hypothèse qu'un corpus entier est à annoter, mais
dans le cas où l'annotation se ferait phrase par phrase, il serait possible
d'abandonner cette étape ou d'alimenter les modèles de au fur et à mesure des
annotations.

L'apprentissage se fait sous la forme de modèle de probabilités simples.
Chacun de ces modèles de probabilité assigne une probabilité aux différents
rôles possibles selon les rôles identifiés de manière non ambiguë auparavant.
Une correspondance est ambiguë quand deux rôles ou plus sont possibles, et est
non ambiguë quand un seul rôle est possible. Deux modèles de probabilités issus
de \citep{swier2005exploiting} sont envisagés.

% TODO belles formules comme dans gildea2002automatic

% TODO clarifier
Dans ces deux modèles, la fonction est la fonction grammaticale identifiée
d'après l'analyse syntaxique : si un syntagme apparaît avant le verbe, il est
sujet, si il est après le verbe, et il est objet, et ainsi de suite. Les
syntagmes prépositionnels sont traités à part : la préposition qui introduit le
syntagme est considérée à part.

\paragraph{prédicat-fonction}

Le modèle \emph{prédicat-fonction} utilise l'information du prédicat et la
fonction grammaticale détectée. Par exemple, dans notre corpus
(section~\ref{subsec:corpora_tools}), l'objet direct du verbe 'négliger' est le
plus souvent 'Theme'.

La précision pour ce modèle est forte, mais il n'assigne des rôles que pour
40~\% des arguments : dans les autres cas, nous ne disposons pas d'informations
pour cette paire (prédicat, fonction grammaticale).

\paragraph{fonction}

Étant donné que \emph{prédicat-fonction} ne donne pas de résultats dans tous les
cas, un modèle plus simple a aussi été utilisé : \emph{fonction}. Ce modèle se
concentre uniquement sur la fonction grammaticale (en incluant la préposition
considérée). Par exemple, un syntagme prépositionnel \emph{of NP} correspond
dans l'ordre à Attribute, Theme et Topic dans le corpus utilisé dans cette
expérience. Dans un contexte précis, c'est le premier rôle possible qui sera
choisi. Ainsi, si l'étape précédente nous indique que Topic et Recipient sont
possibles, Topic sera choisi.

Ces deux modèles probabilistes simples sont complémentaires : l'un est précis
sans couvrir une large partie du corpus, alors que l'autre permet d'assigner un
rôle à chaque verbe. Cependant, la faible précision du second modèle nous
empêche de l'utiliser en l'état. % TODO que ce soit motivé + clair + future
% work

\section{Gestion de la voix passive}
\label{sec:passif}

Une analyse d'erreur a révélé que la voix passive était une source d'erreur
importante dans notre corpus FrameNet. En effet, VerbNet n'encode pas la voix
passive qui est un phénomène syntaxique : c'est à l'analyse syntaxique que les
sujets et objets syntaxiques doivent être identifiés correctement. Une fois
identifiés, c'est le sujet profond qui doit être comparé avec le sujet VerbNet.
Cependant, les analyseurs syntaxiques n'identifient pas de tels liens de
syntaxe profonde (voix passive, coordination, etc.). Il est donc important
d'avoir une étape intermédiaire entre l'analyse syntaxique et l'annotation en
rôles sémantiques \citep{bonfante2011modular, ribeyre2013systeme}. Cette étape
intermédiaire pourra identifier les sujets et objets profonds de tous les
verbes considérés, évitant ainsi des erreurs lors de l'annotation en rôles
sémantiques. 

Afin de valider cette hypothèse, nous nous sommes concentrés sur la voix
passive qui était le phénomène de syntaxe le plus présent dans notre corpus.
% TODO le faire correctement en allant vers VerbNet plutôt qu'en changeant
% VerbNet
Nous avons ainsi transformé les frames VerbNet dans les phrases impliquant une
voix passive, c'est-à-dire les verbes au participe passé gouvernés par une
forme du verbe \emph{to be}. Étant donné une frame VerbNet telle que
\textit{NP.Agent V NP.Recipient NP.Theme}, nous produisons :

\begin{itemize}
    \item NP.Recipient V NP.Theme
    \item NP.Recipient V NP.Theme by NP.Agent
\end{itemize}

Ce sont ces frames VerbNet transformées qui sont utilisées lorsque qu'une voix
passive est utilisée, ce qui améliore les résultats (Table~\ref{table:results}).

\section{Evaluation}
\label{srl:evaluation}

L'intérêt de l'algorithme ne réside pas dans l'annotation d'un large corpus
annoté tel que FrameNet pour lequel les méthodes d'apprentissage supervisées
sont les plus efficaces \citep{das2014frame}. En effet, notre méthode est
destinée à annoter de nouveaux domaines ne disposant pas de corpus annoté
(Chapitre~\ref{ch:domainsrl}). Néanmoins, FrameNet est un point de comparaison
utilisé par de nombreux systèmes d'annotation en rôles sémantiques auxquels il
est intéressant de se comparer.

% TODO mettre avant mention dans 6.2
FrameNet et VerbNet n'utilisant ni la même séparation en rôles ni la même
séparation en classes, nous utilisons le mapping VerbNet-FrameNet maintenu par
le projet SemLink.

Le corpus FrameNet est équilibré et inclut des textes de diverses sources : le
Wall Street Journal, les corpus AQUAINT et MASC, ainsi que d'autres textes
divers.

\subsection{Détails expérimentaux}
\label{subsec:corpora_tools}

FrameNet dispose de deux corpus. Le premier corpus est le corpus d'exemples :
des phrases tirées du British National Corpus illustraient la diversité de
réalisation des frames annotés. Plus tard, les versions 1.3, 1.4 et 1.5 de
FrameNet ont apporté un corpus dit full-text, plus adapté à la tâche
d'annotation en rôles sémantiques : au lieu d'identifier des exemples pour
toutes les frames, tous les prédicats présents dans un texte donné sont annotés
en rôles sémantiques. Ici, nous utilisons le corpus full-text de FrameNet 1.5
et pas le corpus d'exemples.

Nous utilisons VerbNet 3.2 et le mapping VerbNet-FrameNet 1.2.2c
\footnote{\url{http://verbs.colorado.edu/semlink/1.2.2c/vn-fn/}}. Notre
algorithme n'annote que les arguments Core étant donné que ce sont généralement
les arguments étudiés par VerbNet.

% TODO de toute façon il faut utiliser ce qui est fourni par SEMAFOR
Pour la tâche complète qui inclut l'identification des arguments, nous
utilisons le parser MST dans sa version 0.5.0 \citep{mcdonald2006multilingual}
entraîné sur une version modifiée du Wall Street Journal de la manière
suivante. Nous avons d'abord transformé l'encodage des syntagmes nominaux
\footnote{\url{http://sydney.edu.au/engineering/it/~dvadas1/}}
\citep{vadas2007adding} puis appliqué l'outil de conversion
constituants-dépendances du LTH pour une conversion au format CoNLL
\footnote{\url{http://nlp.cs.lth.se/software/treebank_converter/}}
\citep{johansson2007extended}. De plus, FrameNet incluant des parties du corpus
du Wall Street Journal, nous avons supprimé les fichiers 0558, 0089, 0456,
1778, 1286 et 1695 du corpus d'entraînement de l'analyseur syntaxique. Ceci
évite que l'analyseur traite des phrases déjà observées dans le corpus
d'entraînement, ce qui aurait pu améliorer les résultats artificiellement.

Enfin, les étiquettes morphosyntaxiques de FrameNet, utilisées lors de
l'analyse syntaxique, ont été converties de la convention BNC à la convention
WSJ en utilisant des règles manuelles (Table~\ref{table:tagset_rules}). Sur les
six fichiers du Wall Street Journal pour lesquels on connaît l'annotation
correcte au format WSJ, ceci réduit les erreurs de 23\% à 3\%.

\begin{table}[ht]
    \centering
    \begin{tabular}{ccc|ccc}
        \toprule
        JJ   &$\to$& ADJ    & JJR  &$\to$& NP     \\
        JJS  &$\to$& NP     & MD   &$\to$& S      \\
        NN   &$\to$& NP     & NNP  &$\to$& NP     \\
        NNPS &$\to$& NP     & NNS  &$\to$& NP     \\
        NP   &$\to$& NP     & NPS  &$\to$& NP     \\
        PP   &$\to$& PP     & PRP  &$\to$& NP     \\  
        RB   &$\to$& ADV    & TO   &$\to$& to S   \\
        VB   &$\to$& S      & VBD  &$\to$& S      \\
        VBG  &$\to$& S\_ING & VBN  &$\to$& ADJ    \\
        VBP  &$\to$& S      & VBZ  &$\to$& S      \\
        WDT  &$\to$& NP     & \$   &$\to$& NP     \\  
        CD   &$\to$& NP     & DT   &$\to$& NP     \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:tagset_rules}BNC to WSJ conversion rules}
\end{table}

% FUTURE SEMAFOR MXPOST/MST

\subsection{Procédure d'évaluation}

\begin{table*}[ht]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Task                                           & F1        & Accuracy \\
        \midrule
        FM                                             & 70.48\%   & 53.09\%  \\
        FM + prédicat-fonction (gold args)                & 72.02\%   & 58.28\%  \\
        FM + passif + prédicat-fonction (gold args)      & 76.40\%   & 62.72\%  \\
        \midrule
        Identification + FM                            & 46.75\%   & 29.12\%  \\
        Identification + FM + prédicat-fonction           & 46.78\%   & 33.49\%  \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:results}Résultats pour différentes tâches. FM est le frame matching. \emph{passif} indique la détection de la voix passive. \emph{Identification} est l'identification des arguments.}
\end{table*}

Les rôles annotés de chaque phrase FrameNet sont d'abord transformés en rôles
VerbNet. Pour chaque classe FrameNet, un rôle FrameNet peut correspondre à 0, 1
ou plusieurs rôles VerbNet :

\begin{itemize}

    \item zéro : n'y pas de correspondance pour les frames FrameNet trop éloignées
    des classes VerbNet. C'est un problème répandu :  seulement 4605 rôles sur les
    10052 rôles présents dans FrameNet ont au moins une association VerbNet.
    % TODO Cependant, en pratique, dans le corpus FrameNet, XX~\% des occurrences
    % de rôles sont associées à plus d'un rôle VerbNet

    \item plusieurs : il y a plusieurs correspondances quand un rôle FrameNet
    est ambigu par rapport à un rôle VerbNet. Les rôles FrameNet étant définis plus
    précisément, c'est un problème rare. % TODO dans quelle mesure ?

\end{itemize}

Dans tous les autres cas, un rôle FrameNet correspond à un rôle VerbNet unique,
et c'est ce rôle VerbNet que notre système doit déterminer. Nous mesurons la
précision, le rappel et l'exactitude (\emph{accuracy}) des associations
role/syntagmes candidats.  10~\% du corpus a été utilisé pour obtenir les
résultats, le reste est un corpus "d'entraînement" : le corpus qui a été
examiné manuellement pour identifier les erreurs de notre algorithme.

La table~\ref{table:results} montre les résultats des deux tâches sur
lesquelles nous nous évaluons : le frame matching seul d'une part et le frame
matching accompagné de l'identification des arguments en amont d'autre part. Le
frame matching seul utilise les arguments de la vérité-terrain: on sait que ce
syntagme joue un rôle, il faut alors déterminer lequel. L'identification des
arguments implique que le texte de départ est une phrase brute : il faut
l'analyser syntaxiquement, identifier les arguments, puis réaliser le frame
matching à proprement parler.

Enfin, différents modèles sont appliqués :

\begin{itemize}

    \item \emph{passif} indique que la voix passive a été prise en compte
    (section~\ref{sec:passif}).

    \item \emph{prédicat-fonction} indique que le modèle de probabilité
    \emph{prédicat-fonction} a été appliqué (section~\ref{subsec:probability}).

\end{itemize}

\section{Résultats}

\subsection{Analyse des résultats}

Premièrement, l'identification des arguments doit être améliorée de manière
significative car elle pénalise les étapes ultérieures.

La première raison est que seulement 76~\% des syntagmes jouant un rôle sont
effectivement des sous-arbres de l'analyse syntaxique.
% TODO inclure les mots seuls ? Vérifier la citation de SEMAFOR
La seconde raison provient des heuristiques utilisées pour l'identification
elle-même : une analyse plus poussée permettrait de mieux comprendre les
erreurs qu'elles causent. Des alternatives existent, supervisées ou non
supervisées \citep{abend2009unsupervised}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Model          & Precision & Coverage \\
        \midrule
        fonction           & 52.45\% & 100\% \\
        prédicat-fonction & 68.33\% & 38.33\% \\
        \bottomrule
    \end{tabular}
    \caption{\protect\centering\label{table:probabilisticresults}Résultats pour les modèles de probabilité}
\end{table}

La table~\ref{table:probabilisticresults} montre la complémentarité de nos
modèles : le modèle prédicat-fonction est précis mais couvre peu de verbes, alors
que le modèle fonction couvre l'ensemble des verbes.
% TODO narrative logique, soit on prend soit on prend pas.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{fig/slot-predicate-percents.png}
    \caption{\label{fig:fonction_predicate}Performance du modèle fonction-prédicat en entraînant le modèle sur une sous-partie du corpus : de 0 à 100~\%.}
\end{figure}

La figure~\ref{fig:fonction_predicate} montre que ce niveau de performance ne
demande pas un gros corpus. C'est un enseignement intéressant pour deux
raisons~:

\begin{itemize}

    \item Un corpus de petite taille suffit pour obtenir cette performance, ce
    qui est adapté à des domaines où les corpus même non annotés peuvent être
    petits.

    \item L'algorithme ne fait pas de sur-apprentissage en ayant un biais fort
    et une variance faible, ce qui est souhaitable ici.

\end{itemize}

\subsection{Comparaison avec SEMAFOR}

% TODO scores sont en XX
SEMAFOR \citep{das2014frame} est la référence actuelle en annotation en rôle
sémantiques supervisée : c'est le système qui obtient les meilleurs résultats
sur le corpus full-text de FrameNet 1.5. Toutes les parties du discours sont
annotées alors que nous ne nous concentrons que sur les verbes. Sur la tâche
complète, SEMAFOR obtient un score F1 de XX~\%, score à comparer avec les XX\~
de notre système. Le système de SEMAFOR découpe la tâche en trois parties :

\begin{itemize}
    \item identification des prédicats déclencheurs
    \item identification des frames FrameNet
    \item identification des arguments et annotation en rôles sémantiques 
\end{itemize}

Une comparaison directe n'est pas possible étant donné que les tâches sont
découpées différemment dans notre système. Cependant, il est intéressant de
noter l'importance des données d'entraînement : pour l'identification des
frames FrameNet avec des prédicats de la vérité-terrain\footnote{les résultats
pour des déclencheurs identifiés manuellement n'ont été donnés que pour le
corpus SemEval, un sous-ensemble du corpus FrameNet 1.5 complet}, les mêmes
modèles grimpent de 74.21~\% à 90.51~\% quand la taille du corpus augmente en
passant du corpus SemEval (XXX phrases) au corpus FrameNet 1.5 (XXX phrases).
De la même manière pour l'identification des arguments avec des frames FrameNet
de la vérité-terrain, les résultats augmentent de 48.09~\% 68.83~\%. C'est très
encourageant pour les domaines disposant de très gros corpus, mais suggère que
d'autres solutions sont à identifier pour les domaines où de tels corpus ne
sont pas disponibles.

\section{Travaux futurs}

Nous avons appliqué ce travail à des domaines spécifiques et variés : football,
réchauffement climatique et informatique (Chapitre~\ref{ch:domainsrl}).

Nous pensons aussi prendre en compte la similarité entre les remplisseurs déjà
identifiés et les remplisseurs pour lesquels un rôle reste encore à identifier
afin d'améliorer nos modèles de probabilité. En effet, l'information des cadres
de sous-catégorisation est cruciale pour identifier les arguments, mais
l'information sémantique concernant le contenu des remplisseurs est aussi utile
pour déterminer le rôle correct.
% TODO ref vers chapitre similarité ASFALDA

Enfin, de la même manière que la prise en compte de la voix passive a amélioré
les résultats, d'autres phénomènes de syntaxe profonde doivent être pris en
compte. La coordination est une autre source commune d'erreur. En effet, quand
deux verbes partagent le même sujet, une analyse syntaxique profonde indique à
chaque fois quel est le sujet profond. Voici deux exemples tirés du corpus
FrameNet :

\begin{itemize}
    \item You are not fair when you belittle Sheik Bin Baz 's blunder and
          exaggerate the one by Sheik Maqdasi ...
    \item Hostile and even friendly nations routinely steal information from
          U.S. companies and share it with their own companies
\end{itemize}

L'objectif est de traiter ces phénomènes de manière plus générale en intégrant
le système de \cite{ribeyre2013systeme} qui permet de prendre en compte de
nouveaux phénomènes en ajoutant de nouvelles règles au système. Ainsi, les
différents phénomènes seront prise en compte de manière cohérente.

\section{Conclusion}

Nous avons implémenté un système d'annotation en rôles sémantiques basé sur la
connaissance. Nous avons utilisé des outils et des corpus disponibles
publiquement qui rendent notre travail facilement reproductible et facilitent
le travail de comparaison, maintenant et dans le futur. Nous avons commencé à
améliorer le système initial, montrant son potentiel. L'indépendance de
l'approche par rapport au corpus considéré la rend attractive pour annoter des
domaines ne disposant que de peu ou pas de corpus annotés en rôles sémantiques.
