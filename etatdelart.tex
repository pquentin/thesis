% vim: set spelllang=fr:

\setchapterpreamble[ur][.5\textwidth]{%
  \dictum[Robin Hobb, \textit{Assassin's Quest}]{%
As for me, I had been amazed at how many pieces there were to something that had seemed to be all one thing when I had started in on it. [...] All these sounds to make a word, all these words to frame a thought. Language came apart in my hands. I had never stopped to consider it before.}}

\chapter{État de l'art} 
\label{ch:etatdelart} 
% Questions :
%   - inventaires vs. inventaires de sens
%   - qui citer pour sème ?
%   - qui citer pour plus d'un sens ?
%   - réutiliser la figure d'un papier ?


La représentation du sens des mots (section~\ref{sec:mots}) occupe une place
importante dans nos travaux, en particulier pour la traduction de la ressource
WordNet (Chapitre~\ref{ch:wonef}). La traduction de ressources lexicales
(section~\ref{sec:translation}) est un moyen de faire profiter une langue cible
de l'effort fourni pour une langue source, comme nous le faisons dans la
partie~\ref{part:translation}. Enfin, l'annotation en rôles sémantiques
(section~\ref{sec:srl}) sera elle surtout utile pour la partie~\ref{part:srl}.

\section{Représentation des mots}
\label{sec:mots}

% TODO \citep{harnad1990symbol} ?
Nous étudions ici différentes façons de représenter les mots et leur sens en
commençant par les dictionnaires, puis en étudiant différentes ressources
lexicales représentant le sens des mots avant d'étudier les modèles de langue
qui sont une manière plus directe de représenter les mots à partir d'un corpus.

\subsection{Représentation du sens des mots}

La lexicographie est la science qui consiste à recenser les mots, les classer,
les définir et les illustrer, par des exemples ou des expressions, pour rendre
compte de l'ensemble de leurs significations et de leurs acceptions au sein
d'une langue, afin de constituer un dictionnaire
\citep{wikipedia2014lexicographie}. La lexicographie est donc un socle sur
lequel le Traitement Automatique des Langues peut s'appuyer pour représenter le
sens des mots.

Pour pouvoir identifier les différents sens d'un mot, les lexicographes
n'opèrent pas par intuition linguistique \citep{kilgarriff1997don}. Ils
commencent par établir un corpus équilibré et de taille assez importante pour
représenter la langue étudiée. Ce corpus peut par exemple être constitué de
textes de journaux, de fiction, ou encore de blogs, le tout étant supposé être
représentatif de ce qu'une personne lambda lit durant sa vie. Pour un mot
donné, le lexicographe examine ses différents usages dans ce corpus dans le but
de séparer ces différents usages en sens. Certains sens, jugés trop peu
fréquents, sont laissés de côté. Une fois la séparation effectuée, le
lexicographe l'étudie pour établir des critères objectifs distinguer les
différents sens du mot étudié. Une phase d'ajustement de la séparation suit
pour vérifier que les critères ont été correctement appliqués, ce qui peut
amener à raffiner ces critères. Une fois le processus fini, ces critères
serviront pour écrire la définition, et les occurrences de mots dans le corpus
pourront servir d'exemples. L'avantage principal est que le processus
lexicographique est désormais basé sur des données réelles et non pas sur des
intuitions linguistiques.

% exemple

Ainsi, les sens ne sont pas définis en tant que tels, mais sont avant tout des
occurrences dans un contexte donné. C'est une des façons de comprendre la
citation de \citep{firth1957synopsys} : \emph{You shall know a word by the
company it keeps}\footnote{Vous devriez connaître un mot par ce qui
l'accompagne.}. En effet, selon \citep{kilgarriff1997don}, un ensemble de sens
n'est défini que par rapport à un corpus donné, et il est illusoire de vouloir
définir un dictionnaire parfait pour tous les sens possibles d'un mot.
Néanmoins, il n'est pas concevable de réaliser manuellement un dictionnaire par
corpus ; et il faut simplement être conscient des difficultés théoriques posées
par le sens des mots.

Sans s'attarder sur des difficultés théoriques, on considèrera dans ce travail
que les sens définis dans un dictionnaire classique relèvent du «~domaine
général~», et que les sens qui apparaissent dans d'autres domaines sont des
sens «~spécialisés~». Par exemple, le dictionnaire DicoInfo \citep{corpusolst}
spécialisé dans les domaines de l'Informatique et d'Internet mentionne un sens
spécifique pour le nom \emph{compilation} : \emph{action effectuée par un
compilateur qui consiste à transformer du code créé au moyen d'un langage de
programmation évolué en un langage compréhensible par l'ordinateur}. Ce sens
est par exemple absent du TLFi \citep{TLFi} parce qu'il ne faisait pas partie
des sens du mot dans le corpus utilisé pour établir les définitions.

Les premiers inventaires de sens disponibles étant les dictionnaires, ils ont
étés naturellement utilisés par les premiers systèmes cherchant une couverture
exhaustive de tous les mots d'un texte, tel que le système de désambiguïsation
de \cite{lesk1986automatic}. La première campagne d'évaluation Senseval
organisée en 1998 \citep{kilgarriff2000introduction} a d'ailleurs fourni un
dictionnaire par langue pour les systèmes de désambiguïsation lexicale.

% Ce paragraphe doit résumer les défauts (tous relatifs) des dictionnaires
Les dictionnaires ont cependant été rapidement abandonnés dans les campagnes
suivantes. La qualité du travail lexicographique exposé dans les dictionnaires
n'a pas été remise en cause, mais l'Informatique a rendu possible d'autres
formats pour les dictionnaires :

\begin{itemize}

    \item il n'est pas nécessaire de trier les entrées par ordre alphabétique
        pour pouvoir identifier un mot rapidement
        \citep{miller1990introduction},

    \item le lexique est mieux représenté par un graphe que par un texte
        \citep{polguere2013tissage}.

\end{itemize}

De plus, l'utilisation de dictionnaires récents implique un coût d'achat et le
respect de la licence restrictive, ce qui explique que ces dictionnaires ont
rapidement étés abandonnés au profit d'autres ressources disponibles sous une
ressource libre comme WordNet \citep{edmonds2002introduction} ou le
Wiktionnaire \citep{mouton2010jaws,nguyen2012using}. En effet, ces deux
ressources autorisent une utilisation à la fois à des fins de recherche mais
aussi pour un usage commercial, ce qui leur a assuré une large diffusion.

% Histoire, intéret de WordNet
La première ressource lexicale à tirer partie de la possibilité de représenter
le lexique sous la forme d'un graphe est WordNet, dont l'élaboration a commencé
en 1985 \citep{miller1990introduction}. Établi sur des principes
psycholinguistiques, WordNet propose quatre graphes pour les quatre parties du
discours formant une classe ouverte : noms, verbes, adjectifs, adverbes. Les
noeuds du graphes sont des ensembles de synonymes (\emph{synonym sets} ou
\emph{synsets}. Un synset regroupe plusieurs mots, une définition, et
potentiellement des exemples.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{fig/wordnet_hypernymy.png}
    \caption{\label{fig:wordnet_hypernymy}Hypéronymie dans WordNet autour du
synset day. Les synsets au-dessus de day (fond gris) sont ses hypéronymes (day
est-un time unit), alors que les synets dessous font partie de ses hyponymes
(tomorrow est-un day).}
\end{figure}

Chaque synset est lié à d'autres synsets à travers un certain nombre de
relations telles que l'hypéronymie, la méronymie de partie (guidon est un
méronyme de vélo), l'antonymie, etc. Si on ne considère que l'hypéronymie,
WordNet peut être visualisé comme un arbre
(Figure~\ref{fig:wordnet_hypernymy}). En considérant les autres relations,
WordNet est un graphe (Figure~\ref{fig:wordnet_relations}).


Les sens proposés ont étés utilisés pour annoter le corpus SemCor, ce qui a
permis d'entraîner de nombreux systèmes supervisés. WordNet est rapidement
devenu le standard de la désambiguïsation lexicale et a été utilisé dans les
campagnes d'évaluation internationales d'analyse sémantique
\citep{navigli2009word}.

% Corrections apportées à WordNet (regrouper les sens, annoter plus d'un sens
% par mot..)

Depuis 2006, différents travaux \citep{navigli2007semeval,hovy2006ontonotes}
ont remarqué que les défauts attribués à WordNet
\citep{snow2007learning,ide2007making} étaient suffisamment importants pour
nécessiter une alternative avec des sens distingués plus grossièrement. Ce
problème est attribué selon \cite{edmonds2002introduction} au manque de rigueur
lexicographique de WordNet, et à la mise en avant de la similiarité entre les
mots à travers les synsets au détriment de la distinction des sens. Il s'est en
effet avéré que l'accord inter-annotateurs pour un étiquetage avec WordNet est
remarquablement faible (de l'ordre de 70\%), et qu'utiliser un autre inventaire
un moyen efficace de s'adapter à différentes applicaions
\citep{palmer2004different}. La tendance est désormais à l'utilisation
d'inventaires plus grossiers \citep{navigli2007semeval,navigli2012quick}.

% Nouveaux inventaires (mérite mieux qu'une liste)

Au-delà des approches statistiques \citep{snow2007learning}, de nouveaux
inventaires de sens ont étés developpés :

\begin{itemize}

    \item OntoNotes \citep{hovy2006ontonotes} a choisi de regrouper
        manuellement les sens WordNet jusqu'à obtenir un accord
        inter-annotateur de 90\% .

    \item DANTE\footnote{Les entrées pour les mots entre M et R sont
        disponibles sur http://webdante.com/} \citep{mccarthy2010dante} est un
        inventaire entièrement nouveau, conçu dans l'objectif de corriger les
        erreurs faites avec WordNet\citep{kilgarriff2010detailed}.

        % Sens-Texte Nancy !

\end{itemize}

Ces deux inventaires semblent plus adaptés que WordNet pour la désambiguïsation
lexicale \citep{navigli2012quick}, mais ils ne sont pas utilisables librement à
des fins commerciales et et des applications les utilisant doivent encore voir
le jour.

Une approche complètement différente est celle de la structure de qualia
\citep{johnston1996qualia} qui s'inscrit dans le contexte plus général du
lexique génératif introduit par \cite{pustejovsky1991generative} qui considère
qu'une approche énumérative n'est pas viable. Le sens d'un mot est alors défini
selon plusieurs aspects prédéfinis (constitution, rôles, facteurs impliqués
dans la création, etc.) qui peuvent se retrouver dans plusieurs mots. Par
exemple, un couteau contient une lame et sert à couper. Cette approche est
semblable à celle qui définit le sens d'un mot comme une simple suite de sèmes.

Enfin, différents travaux mentionnent la possibilité d'utiliser plus d'un sens
pour un mot donné.
% TODO citer DianaMcCarthy
En particulier, nous avons calculé que 0.3\% des occurrences de SemCor sont
étiquetées avec plus d'un sens. \cite{smith2011rumble} note quant à lui la
possibilité d'utiliser des distributions de probabilité pour définir un sens
précis qui pourrait appartenir à plusieurs sens définir dans un inventaire.


\subsection{Modèles de langue}

Nous avons jusqu'ici présupposé que les sens d'un mot était représentés sous la
forme d'une simple énumération, en signalant l'importance du lien avec un
corpus, pratique observée dans de nombreux dictionnaires depuis longtemps.
Étant donné que c'est de loin l'approche la plus utilisée en désambiguïsation
lexicale, les autres possibilités ne sont mentionnées que dans cette section.

% Attention, orienté WSD. Mais présente des idées intéressantes ?
%\subsubsection{Induction de sens}
\label{distrib}

Le sens d'un mot donné peut souvent s'inférer à partir de son contexte
\citep{pantel2002discovering}. L'hypothèse distributionnelle, attribuée à Zadig
Harris, nous permet de formaliser cette observation.
\cite[p.~786]{harris1954distributional} explique :

\begin{quote} ... si on considère que le sens de deux mots ou morphèmes A et B
    diffère davantage que le sens que A et C, alors on observe souvent que ls
    distribution de A et B diffèrent davantage que les distributions de A et C.
    Autrement dit, la différence de sens est corrélée à la différence de
    distribution.  \end{quote}

La distribution réfère ici à deux relations \citep{sahlgren2008distributional}
:

\begin{itemize}

    \item les relations syntagmatiques identifient les mots qui sont présents
        ensemble dans un texte ;

    \item les relations paradigmatiques identifient les mots qui sont présent
        dans un même contexte, sans être présent ensemble.

\end{itemize}

Étant donné les deux phrases « Je bois du café. » et « Je bois du thé. », on
peut déduire que les lemmes « boire » et « thé » sont liés par une relation
syntagmatique : ils sont présent ensemble. Au contraire, « thé » et « café » ne
sont pas présent dans la même phrase, mais apparaissent dans un même contexte
(« Je bois du ») : ils sont liés par une relation paradigmatique.

On peut utiliser ces relations pour de la désambiguïsation : s'il est possible
d'identifier pour un même mot différents usages correspondant à des relations
sémantiques avec des mots différents, alors ces deux sens sont différents. La
validité de cette approche a été vérifiée expérimentalement
citep{yarowsky1993one,pantel2002discovering,claire} et étudiée de manière plus
théorique \citep{sahlgren2006word,sahlgren2008distributional}. Comme nous le
verrons par la suite, c'est un moyen très utile de différencier le sens des
mots qui est souvent associé à des approches peu supervisées.

Différents travaux ont considéré comme vraie l'hypothèse distributionnelle et
ont établi à partir d'un corpus l'ensemble des sens présents pour chaque mot.
Par exemple,
\cite{schutze1998automatic,pantel2002discovering,niu2007three,pedersen2010duluth}
ont établi pour chaque mot des clusters correspondant chacun à un sens du mot
(voir \cite{liu2012semantic} pour un exemple récent dans un domaine
spécifique).

% Modèles de langues neuronaux

\section{Traductions de ressources linguistiques}
\label{sec:translation}

\subsection{WordNet}

Les traductions automatiques de WordNet emploient une approche dite d'extension
(\textit{extend approach}) : la structure de WordNet est préservée et seuls les
littéraux sont traduits. Trois techniques principales représentent cette
approche dans la littérature. La plus simple utilise des dictionnaires
bilingues pour faciliter le travail des lexicographes qui filtrent ensuite
manuellement les entrées proposées
\citep{vossen1998eurowordnet,pianta2002developing,tufis2004balkanet}. Une
deuxième méthode de traduction utilise des corpus parallèles, ce qui évite
l'utilisation de dictionnaires qui peuvent entraîner un biais lexicographique.
\cite{dyvik2004translations} représente cette méthode en s'appuyant sur des
\textit{back-translations} entre le norvégien et l'anglais, alors que
\citep{sagot2008construction} combinent un lexique multilingue et les
différents WordNets de BalkaNet comme autant de sources aidant à la
désambiguïsation. Enfin, plus récemment, des ressources telles que Wikipédia ou
le Wiktionnaire ont été explorées. Grâce aux nombreux liens entre les
différentes langues de ces ressources, il est possible de créer de nouveaux
wordnets \citep{demelo2009towards,navigli2010babelnet} ou d'améliorer des
wordnets existants \citep{hanoka2012wordnet}.

Concernant le français, l'EuroWordNet \citep{vossen1998eurowordnet} est la
première traduction française de WordNet. C'est une ressource d'une couverture
limitée qui demande des améliorations significatives avant de pouvoir être
utilisée \citep{jacquin2006systemes}, et qui n'est ni libre ni librement
accessible. WOLF est une seconde traduction initialement construite à l'aide de
corpus parallèles \citep{sagot2008construction} et étendue depuis avec
différentes techniques \citep{apidianaki2012applying}. WOLF est distribué sous
une licence libre compatible avec la LGPL et c'est aujourd'hui le WordNet
français standard. Enfin, JAWS \citep{mouton2010jaws} est une traduction des
noms de WordNet développée à l'aide de dictionnaires bilingues et d'un modèle
de langue syntaxique.

\cite{mouton2010jaws} ont conçu JAWS comme un algorithme faiblement supervisé
qui ne demande aucune donnée annotée manuellement. Pour traduire un wordnet
source, JAWS s'appuie sur un dictionnaire bilingue et un modèle de langue
syntaxique pour le langage cible.

Le dictionnaire bilingue est une concaténation du dictionnaire bilingue
SCI-FRAN-EurADic\footnote{\url{http://catalog.elra.info/product_info.php?products_id=666}}
et des liens entre les Wiktionnaires français et
anglais\footnote{\url{http://www.wiktionary.org/}}. Le modèle de langue
syntaxique a été entraîné sur un grand corpus extrait du web
\citep{grefenstette2007conquering}. Le corpus a été analysé par LIMA
\citep{besancon2010lima}, une chaîne d'analyse linguistique ici utilisée comme
un analyseur syntaxique à base de règles produisant des dépendances syntaxiques
fines. Pour une relation donnée $r$ et un mot $x$, le modèle de langue indique
quels sont les 100 premiers mots co-occurrant le plus fréquemment avec $x$ dans
la relation $r$. Avec le mot \textit{avion} et la relation de complément du
nom, le mot \textit{billet} modifie le plus \textit{avion} : \textit{billet
d'avion} est fréquent dans le corpus. Ce modèle de langue peut-être visualisé
sur \url{http://www.kalisteo.fr/demo/semanticmap/index.php}.

Grâce aux dictionnaires, JAWS n'a pas besoin de sélectionner les littéraux de chaque synset parmi l'ensemble du vocabulaire mais seulement parmi un petit nombre de candidats (9 en moyenne). Le processus de traduction se fait en trois étapes :
\begin{enumerate}
    \item Créer un wordnet vide : la structure de WordNet est préservée, mais les synsets eux-mêmes n'ont pas de littéraux associés.
    \item Choisir les traductions les plus faciles parmi les candidats des dictionnaires pour commencer à remplir JAWS.
    \item Étendre JAWS de manière incrémentale en utilisant le modèle de langue, les relations entre synsets et le JAWS déjà existant.
\end{enumerate}

\paragraph{Sélecteurs initiaux} Quatre algorithmes que nous nommons sélecteurs initiaux choisissent des traductions correctes parmi celles qui sont proposées par les dictionnaires. Premièrement, les mots qui apparaissent dans un seul synset ne sont pas ambigüs et il suffit d'ajouter toutes leurs traductions au WordNet français : c'est le sélecteur par monosémie. C'est le cas de \textit{grumpy} : toutes ses traductions sont validées dans le synset où il apparaît. Deuxièmement, le sélecteur par unicité identifie les mots n'ayant qu'une seule traduction et la valident dans tous les synsets où elle est présente. Les cinq synsets contenant \textit{pill} en anglais sont ainsi complétés avec \textit{pilule}. Un troisième sélecteur vise à traduire les mots qui ne sont pas dans le dictionnaire en utilisant directement la traduction anglaise : c'est le sélecteur des transfuges. Un quatrième sélecteur utilise la distance d'édition de Levenshtein : si la distance entre un mot anglais et sa traduction est petite, on peut considérer que c'est le même sens (c'est le cas par exemple pour \textit{portion} ou encore \textit{university}), malgré l'existence de certains faux amis. Ces quatre sélecteurs produisent une première version du WordNet français qui contient assez de traductions pour pouvoir ensuite utiliser le modèle de langue et continuer de compléter les synsets.

\paragraph{Expansion de JAWS} JAWS étant partiellement rempli, une nouvelle étape d'expansion tire parti des relations entre les synsets de WordNet pour valider de nouvelles traductions. Par exemple, si :

\begin{itemize}
    \item un synset S1 est méronyme d'un synset S2 dans WordNet,
    \item il existe un contexte où un littéral dans S1 est méronyme d'un littéral candidat C dans S2,
\end{itemize}
alors ce littéral est considéré comme correct. La tâche de traduction est ainsi réduite à une tâche de comparaison entre d'une part les relations lexicales entre les synsets de WordNet et d'autre part les relations lexicales entre les lexèmes du français.

Prenons l'exemple de \textit{quill} qui peut se traduire par \textit{piquant} ou \textit{plume} (Figure \ref{meronymyexample}). Dans WordNet, \textit{quill} est méronyme de \textit{porcupine} qui a déjà été traduit par \textit{porc-épic} par un sélecteur initial. Dans le modèle de langue, \textit{piquant} fait partie des compléments du noms de \textit{porc-épic} mais ce n'est pas le cas de \textit{plume}. Ici, la relation de complément du nom implique la méronymie et c'est donc \textit{piquant} qu'il faut choisir comme la traduction correcte de \textit{quill}. Le modèle de langue a permis la désambiguïsation parmi les deux traductions possibles.

\tikzstyle{block}=[draw, fill=blue!5, rectangle, minimum height=0.5cm, minimum width=3cm, text width=5cm]

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}[auto, node distance=2cm,>=latex']
    % Inspired from http://www.texample.net/tikz/examples/control-system-principles/
    % first place and connect the outer blocks that represent synsets
      \node [block, text width=5cm] (quill) {~\\\textbf{Synset S1} \\ - Anglais : quill \\ - Français : piquant? plume? \\ (a stiff hollow protective spine on a porcupine) \\ ~ };
      \node [block, text width=4.0cm, right of=quill, node distance=9cm] (porcupine) {\textbf{Synset S2} \\ - Anglais : porcupine, hedgehog \\ - Français : porc-épic \\ (rodents with sharp erectile bristles mingled with the fur)};
    \draw [<-] (porcupine) -- node[above] {méronyme de} (quill);
    \draw [<-] (porcupine) -- node[below] {(relation WordNet)} (quill);

    % then the syntactic model relations
    \node [block, below of=porcupine, text width=4.0cm, node distance=3cm] (porcupinesynt1) {porc-épic};
    \node [block, below of=quill, node distance=3cm] (quillsynt1) {\Large{mémoire}, \Large{piquant}, \large{poil}, \large{épine}, yéti, ragoût, grotte, \small{tactique}, \small{pelage}, \small{dextre}, \small{aiguille}, ...};
    \draw [<-] (porcupinesynt1) -- node[above] {complément du nom de} (quillsynt1);
    \draw [<-] (porcupinesynt1) -- node[below] {(modèle de langue)} (quillsynt1);

  \end{tikzpicture}
  \caption{\protect\centering\label{meronymyexample}Traduction via la relation de méronymie de partie.}
\end{figure}

Un problème potentiel avec cette approche est que la relation de complément du nom n'est pas limitée à la méronymie. Par exemple, le mot \textit{mémoire} qui apparaît dans le modèle de langue vient d'un livre intitulé \textit{Mémoires d'un porc-épic}. Heureusement, \textit{mémoire} n'est pas dans les candidats de \textit{quill} et ne peut pas être choisi comme une traduction. Paradoxalement, le modèle de langue ne peut pas choisir entre deux mots très différents, mais est capable de choisir la traduction correcte d'un mot polysémique. Alors que traduire WordNet automatiquement avec un dictionnaire ou un modèle de langue syntaxique est impossible, combiner les deux ressources permet de résoudre le problème.

Chaque sélecteur suit le même principe que le sélecteur par méronymie de partie et traduit de nouveaux synsets en identifiant les relations entre lexèmes via le modèle de langue syntaxique. La correspondance entre la relation de complément du nom et la relation de méronymie est directe, mais ce n'est pas le cas pour les autres relations : il n'y a par exemple pas de relation syntaxique qui exprime directement la synonymie entre deux lexèmes. Pour ces relations, il est nécessaire d'employer soit des motifs lexicaux \citep{hearst1992automatic} soit des relations syntaxiques de deuxième ordre \citep{lenci2012identifying}. Ce sont ces dernières, aussi nommées relations paradigmatiques, que JAWS utilise. Pour la synonymie, si deux mots partagent les mêmes co-occurents dans une relation syntaxique donnée, alors ils peuvent être synonymes dans ce contexte. Pour les noms, les relations syntaxiques qui donnent les meilleurs résultats sont les relations de complément du nom, d'objet du verbe et d'apposition. Concrètement, si deux noms qui modifient les mêmes noms sont les objets des mêmes verbes ou sont apposés aux mêmes noms, alors il est probable qu'ils soient synonymes et si l'un des deux est déjà dans un synset, alors on peut y ajouter le second. Par exemple, \textit{avant-propos} et \textit{préface} partagent les mêmes compléments du noms : \textit{livre, édition, ouvrage}. Le sélecteur par synonymie peut ajouter \textit{avant-propos} une fois que le littéral \textit{préface} est dans JAWS. \citep{mouton2010jaws,mouton2010phd} décrivent d'autres sélecteurs exploitant notamment les relations d'hyperonymie et d'hyponymie.


\subsection{VerbNet}

Translating Levin classes and more recently VerbNet is recognized as a useful
task in the literature. First, automatic methods have led to improvements in
VerbNet itself: new classes have been incorporated
\citep{korhonen2004extended} and new verbs have been added from the LCS
database \citep{dorr2001lcs} or using the Sketch Engine tool
\citep{bonial2013expanding}.

In other languages, \cite{merlo2002multilingual} have used crosslinguistic
similarities to convert 20 Levin classes to Italian. Automatic acquisitions
have also been led in Japanese \citep{suzuki2009classifying}, German
\citep{im2006experiments}, and Spanish \citep{ferrer2004towards}. The
only direct translations we are aware of are the Estonian VerbNet
\citep{jentson2014verbnet} and the Brazilian Portuguese VerbNet
\citep{scarton2012towards}, which uses the mappings between VerbNet and
WordNet, and between WordNet.Br and WordNet.

In French, \cite{saintdizier1996constructing} first produced a VerbNet-like
resource. To the best of our knowledge, effort on this resource has stopped and
the result is not available. Later work has focused on automatic acquisition of
subcategorization frames which were clustered according to their syntactic and
semantic similarity. \cite{sun2010investigating} used a large
subcategorization frame lexicon \citep{messiant2010acquisition} to cluster
verbs according to two types of features: syntactic (subcategorization frames)
and semantic (collocations and lexical preferences of verbs). Evaluation
against a manually created gold standard led to an F-measure of 55.1\%.
\cite{falk2012classifying} apply a different clustering algorithm, and use
different features, improving the F-measure to 70\% on a similar but easier
dataset. While those resources highlight new ways to separate French verbs, the
errors they contain are only one source of errors in applications: it is
important to correct them if possible.


\section{Annotation en rôles sémantiques}
\label{senssituation}

Après avoir établi ce que pouvait être le sens d'un mot, nous nous intéressons
ici au même problème du point de vue de l'annotation en rôles sémantiques. Nous
verrons les différentes façon de définir les rôles sémantiques, et examinerons
les deux ressources les plus utilisées dans le domaine : FrameNet et PropBank.

\subsection{Les rôles sémantiques}
\label{subsec:roles_semantiques}

Comment aller au-delà d'une analyse syntaxique pour représenter le sens d'une
phrase ? La notion de rôle sémantique semble particulièrement adaptée aux
approches statistiques que nous présentons ici. Ces rôles permettent
d'abstraire un nombre important d'alternances diathésiques présentes dans le
langage naturel. TODO ... . De nombreuses théories linguistiques semblent
proposer différentes représentations puor ces rôles ; nous nous intéresserons
ici à la théorie élaborée par \cite{fillmore1968case} qui établit que le cas
grammatical montre des relations plus profondes et sémantiques. De nombreuses
langues marquent ces relations au niveau morphologique ; l'élatif est un
exemple de cas grammatical qui exprime le lieu de l'intérieur duquel provient
un mouvement et qui est marqué morphologiquement en finnois, hongrois et
estonien.

Il n'y a pas de réel consensus sur un inventaire de cas donnés. Parmi les rôles
sémantiques généralement acceptés, on peut citer :

\begin{itemize}
    \item l'\textbf{Agent} qui est à l'origine de l'action
    \item le \textbf{Patient} qui subit un changement d'état
    \item l'\textbf{Instrument} utilisé pour réaliser l'action
    \item le \textbf{Bénéficiaire} qui tire profit de l'action
\end{itemize}

\subsection{Lexiques et corpus}

Il existe en anglais différentes ressources pour l'annotation en rôles
sémantique : nous aborderons ici FrameNet, PropBank et NomBank.

\subsubsection{FrameNet}

FrameNet repose sur la théorie des \textit{Frame Semantics}, élaborée par
Fillmore en modifiant sa théorie initiale. Ici, les rôles sémantiques
\textit{frame elements} sont spécifiques à chaque situation (\textit{frame})
tout en se recoupant par endroits. On retrouve ainsi le rôle d'agent, mais
aussi des rôles spécifiques comme \textbf{Food} dans \textbf{Apply\_heat} ou
\textbf{Completeness} dans \textbf{Activity\_pause}. Les rôles sont classifiés
selon leur importance dans la situation : centraux (nécessaires), périphériques
(toujours liés à la situation mais optionnels) et circonstanciels
(potentiellement présent dans toutes les situations, par exemple le lieu ou le
temps).

\subsubsection{PropBank}

Malgré quelques critiques \citep{riemer2011conception}, l'interface
syntaxe-sémantique nous permet d'utiliser les informations syntaxiques d'un
verbe pour distinguer différents sens et identifer ses arguments sémantique. En
introduction, nous avons d'ailleurs expliqué les différences entre les deux
sens du verbe « retenir » ainsi :

\begin{enumerate}
    \item se souvenir de quelque chose
    \item empêcher quelqu'un de faire quelque chose
\end{enumerate}

PropBank \citep{palmer2005proposition} a décidé d'utiliser les annotations
syntaxiques du Penn TreeBank \citep{marcus1993building} pour annoter en rôles
sémantiques les phrases incluant un des 5000 verbes les plus fréquents du
corpus. Pour chaque phrase, les annotateurs ont identifié les syntagmes jouant
un rôles sémantique. L'objectif principal de PropBank est de permettre
d'utiliser l'apprentissage automatique pour l'annotation en rôles sémantiques.
C'est pour cette raison que les étiquettes disponibles sont très générales.
Ainsi, il est fréquent que \textit{ARG0} désigne l'agent, \textit{ARG1} le
patient. D'autres arguments sont disponibles pour étiqueter des rôles plus
spécifiques (\textit{ARG2}, \textit{ARG3}, etc.) ainsi que des rôles
secondaires (\textit{Location}, \textit{Extent}, \textit{Manner}, etc.)

\subsubsection{NomBank}

NomBank \citep{meyers2004nombank} a été conçu à l'image de PropBank mais se
concentre, comme son nom l'indique, sur les noms communs, plus particulièrement
sur les 5 000 noms communs les plus fréquents dans le Penn TreeBank. Sur le
million de mots présent dans le corpus, 250 000 sont des noms communs. 100 000
d'entre eux sont des noms issus d'un verbe ou qui se comportent à la façon d'un
verbe. Par exemple, le nom commun français « achat » est lié au verbe « acheter
», et les arguments sémantiqes seront probablement les mêmes : dans « Il a
acheté un arbre » et « l'achat d'un arbre », \textit{ARG1} sera dans les deux
cas l'arbre. D'autres catégories incluent les noms partitifs, relationnels et
environnementaux.

Pour une phrase telle que « They gave the chefs a standing ovation », les
annotations PropBank et NomBank peuvent montrer les liens possibles entre ces
deux ressources. Cette similarité volontaire a permis de lier ces ressources
\citep{pustejovsky2005merging,verhagen2007combining}, mais des applications
utilisant de telles ressources unifiées doivent encore voir le jour.

Plus récemment, \cite{gerber2010beyond} ont étendu NomBank aux arguments
implicites, améliorant ainsi la couverture de NomBank de 65\%, c'est-à-dire en
augmentant le nombre moyen de rôles remplis dans chaque exemple annoté. Il
n'est pas rare que les arguments soient implicites mais présent dans d'autres
phrases. Les annotations étant limitées à la phrase actuelle, il n'est pas
possible de référer à un argument présent dans une prase précédente.

\subsection{Approches d'annotation}

Existing approaches to semantic role labeling are divided into two main
branches. The first one, supervised semantic role labeling, uses a
manually-annotated corpus and manually engineered features to train supervised
models on the task. The most used frame-semantics resource and associated
annotated corpus in this domain is FrameNet \citep{baker1998berkeley}.
While this approach yields the best performance \citep{das2014frame}, the
cost is high: the corpus used are annotated over several years and it would be
in general too long and costly to annotate a new corpus for each new considered
domain. To address those issues, the second mainstream approach, named semantic
role induction, uses fully unsupervised methods: given a corpus, the goal is to
cluster all verbs sharing the same behavior. While this is completely general,
the results are noisier and the semantic roles are only induced and cannot
always be mapped to human-understandable labels such as \textit{Agent} or
\textit{Topic}.

A third approach, knowledge-based semantic role labeling
\citep{swier2004unsupervised,swier2005exploiting}, has not received much
attention lately. The goal is to use external lexical-semantic resources for
each new considered language and to use those resources to annotate text. The
quality of annotation suffers, but bringing semantic role labeling to new
domains and languages becomes easier: no corpus has to be hand-annotated.

Les systèmes de désambiguïsation lexicale et d'annotation en rôles sémantiques
utilisent pour la plupart des techniques d'apprentissage automatique. Après
avoir présenté de manière générale les différentes ressources et approches, des
techniques plus spécifiques sont abordées. Enfin, les traits utilisés pour
l'apprentissage sont précisés.

Deux types de ressources sont généralement utilisées :

\begin{enumerate}

    \item Les \textbf{inventaires} examinés dans les deux sections précédentes
        permettent de fournir un socle commun à différents systèmes. Dans le
        cas de la désambiguïsation lexicale, c'est souvent WordNet. Pour
        l'annotation en rôles sémantiques, ce sera par exemple la définition
        des \textit{frames}, des \textit{frame elements}, et des prédicats
        possibles.

    \item Les \textbf{corpus annotés} par des humains qui utilisent un
        inventaire donné pour réaliser la tâche qu'on essaie de faire apprendre
        aux systèmes. Cela permet à la fois d'entraîner les systèmes et de les
        évaluer. Par exemple, SemCor est annoté avec des sens WordNet, et
        FrameNet contient de nombreux exemples annotés en plus des rôles
        sémantiques définis pour chacune des situations.

\end{enumerate}

Ces ressources sont utilisées différemment suivant les méthodes, souvent
divisées en trois approches générales : supervisées, fondées sur la
connaissance et non supervisées. \citep{navigli2009word}.

\subsubsection{Supervisées}

Les méthodes supervisées utilisent un corpus annoté, et adoptent donc
l'inventaire associé. Des techniques classiques d'apprentissage automatique
sont utilisées pour déterminer le sens correct de chaque occurrence d'un mot
étant donné les informations obtenues à partir du contexte de cette occurrence.
La désambiguïsation lexicale utilise des algorithmes de classification
classiques, et même si de nombreux algorithmes ont fait leur preuves, SVM est
considéré comme l'algorithme le plus performant \cite{navigli2012quick}.
L'annotation en rôles sémantiques supervisée, quant à elle, est souvent
divisées en deux sous-tâches : l'\textbf{identification des arguments} qui
établit les syntagmes jouant un rôle dans la phrase et la
\textbf{classification des rôles} qui détermine le rôle effectif de chaque
syntagme parmi ceux retenus à la phase précédente.

Ces méthodes supervisées ont des difficultés pour couvrir un large éventail de
phrases. Pour la désambiguïsation lexicale, ces méthodes ont souvent été
confrontées à un manque de données d'entraînement du au fait qu'il faut un
nombre conséquent d'exemples annotés pour chaque mot ambigu. Étant donné d'une
part la fréquence d'utilisation de mots ambigus dans une
langue\footnote{d'après nos calculs, seulement 17\% des mots sont polysémiques
    dans WordNet 3.0, mais 73\% des occurrences de SemCor apparaissent avec
    plus d'un sens et 87\% des occurrences concernent des mots polysémiques
dans Wordnet 3.0.}, et d'autre part la distribution asymétrique de ces mêmes
mots\footnote{d'après nos calculs, 95\% des annotations d'occurences de mots
    dans SemCor concernent moins de 5\% des lemmes les plus fréquents, ce qui
limite le nombre d'occurrences pour les mots moins fréquents}, il faut disposer
d'une référence énorme, ce qui rend l'annotation difficile. On appelle ce
phénomène le \textit{language acquisition bottleneck} \citep{gale1992using}. Le
problème du manque de données se retrouve aussi en annotation en rôles
sémantiques : il s'agit alors d'obtenir une couverture suffisamment importante
des situations possibles dans un texte en cadre ouvert
\cite[p.~155]{marquez2008semantic}.

\subsubsection{Fondées sur la connaissance}

Contrairement aux approches supervisées, ces approches n'utilisent pas de
corpus annoté. Les systèmes s'affranchissent alors de la petite taille
inhérente à tout corpus annoté et peuvent utiliser un large corpus non annoté
tel que le web. Un inventaire de sens est tout de même utilisé, et il faut
toujours faire de la classification ; la difficulté principale étant ici
d'obtenir des informations utiles à partir des exemples non annotés. Étant
donné que ces méthodes continuent à utiliser un inventaire, il reste possible
de comparer les résultats entre différents systèmes et de réaliser une
évaluation sur une vérité-terrain. Il est toujours possible d'utiliser un
corpus pour régler les paramètres à l'aide d'un échantillon de validation ou
comme base pour annoter de nouveaux exemples ; mais des corpus plus conséquents
sont toujours utiilisés.

\subsubsection{Non supervisées}

Ces approches n'utilisent aucune connaissance \textit{a priori}, que ce soit un
inventaire ou un corpus annoté. Une approche non supervisée doit nécessairement
construire son propre inventaire. Cette construction peut se faire via du
\textit{clustering} de sens à partir des occurrences de contextes trouvées dans
le corpus, en considérant l'hypothèse distributionnelle (section
\ref{distrib}). Une fois que l'inventaire de sens est défini, il faut
l'utiliser pour étiqueter le texte.

Les avantages potentiels sont nombreux. Ces algorithmes ne nécessitent aucune
ressource, et offrent de fait deux propriétés intéressantes :

\begin{itemize}

    \item L'inventaire choisi colle au plus près du corpus utilisé, ce qui lui
        permet à la fois d'éviter des distinctions trop fines et de s'adapter à
        de nouveaux domaines via de nouveaux corpus, le domaine ayant un impact
        important sur les sens utilisés.

    \item Plus la quantité de texte disponible augmente, plus le système peut
        devenir efficace.

\end{itemize}

Malheureusement, les systèmes utilisant une approche non supervisée sont
difficiles à évaluer et à utiliser directement dans des systèmes plus
importants. Par exemple, dans le cadre de la traduction automatique, distinguer
les sens ne suffit pas ; il faut aussi savoir quelle traduction appliquer.

\subsection{Techniques d'apprentissage supervisé}

\subsubsection{Traits}

%\subsubsection{Désambiguïsation lexicale (WSD)}
%
%Pour la désambiguïsation lexicale supervisée, il s'agit toujours de représenter
%le contexte d'un mot donné afin de le représenter comme un simple trait. Deux
%approches sont très représentées dans la littérature : un contexte purement
%local à base de fenêtre glissante, et un contexte syntaxique qui permet
%l'identification de différents types de relations entre les mots.
%
%\begin{itemize}
%
%    \item \textbf{Fenêtre glissante} : Différentes études
%        \citep{kaplan1955experimental,choueka1985disambiguation,karlgren2001from,kohomban2005learning,dinu2007sometimes}
%        suggèrent qu'un contexte total de cinq mots (deux mots avant, deux mots
%        après) est la fenêtre qui réduit le mieux l'ambiguïté. Cette fenêtre
%        peut donner lieu à plusieurs traits ; \cite{chan2007nus} utilise ainsi
%        11 traits correspondant à différentes parties de la fenêtre.
%
%    \item \textbf{Contexte syntaxique} Les analyseurs syntaxiques étant de plus
%        en plus performants, et permettent d'apporter d'améliorer les résultats
%        \citep{martinez2002syntactic}. La richesse de la relation considérée
%        est importante.
%
%\end{itemize}
%
%\paragraph{Graphes}
%
%Néanmoins ces approches locales ne sont pas uniques, et la désambiguïsation
%lexicale n'échappe à la tendance à utiliser des représentations plus
%structurées et plus globales pour améliorer les performances
%\citep{marquez2012special}. En désambiguïsation lexicale, cette tendance est
%principalement marquée pour les approches fondées sur les connaissances et non
%supervisées \citep[p~.14]{navigli2009word}. Ainsi, les méthodes à base de
%graphes \cite{navigli2005semantic,agirre2009personalizing} obtiennent de bons
%résultats depuis quelques années
%\citep{navigli2007semeval,ponzetto2010knowledge}. %\cite{zouaq2010can}
%explorent ainsi l'utilisation de graphes syntaxiques et logiques.
%
\paragraph{Espaces distributionnels}
\label{espacesdistrib}

% TODO à dire sans WSD - et à placer avant ?
\cite{mouton2009induction} reprend la notion d'espaces sémantiques
\citep{sahlgren2006word} en se concentrant sur des informations syntaxiques
multiple. En utilisant les 38 relations syntaxiques extraites par LIMA
\citep{besancon2010lima} sur un corpus extrait du web
\citep{grefenstette2007conquering}, une matrice est extraite pour chacune des
relations. Ces matrices creuses permettent de stocker les relations syntaxiques
fines entre les 68 000 mots les plus fréquents de la langue française. Elle
parvient ainsi à représenter le contexte plus finement, ce qui permet de
réaliser plus de distinctions entre les sens.

L'article de référence sur l'annotation en rôles sémantiques
\citep{gildea2002automatic} a introduit différents traits d'apprentissage qui
ont étés réutilisés par la suite. Nous introduisons les plus intéressants ici
(se référer à \citep{palmer2010semantic} pour une liste plus complète) puis
identifions les traits efficaces apparus par la suite. Il s'agit à chaque fois
de capturer au mieux les informations syntaxiques et lexicales disponibles pour
en déduire une information sémantique. Les traits sont relatifs au syntagme
étudié pour lequel on essaie de déterminer le rôle sémantique. Nous utiliserons
la figure suivante pour illustrer notre propos.

\begin{figure}[htbl]
    \Tree [.S  SN1 [.VP V SN2 ] ]
    \caption{Exemple d'analyse syntaxique}
\end{figure}

\paragraph{Type de syntagme} Un syntagme nominal et un syntagme verbal ont
tendance à jouer des rôles différents. Par exemple, le rôle \textit{Moyen} qui
apparaît dans de nombreuses \textit{frames} FrameNet est souvent joué par un
syntagme prépositionnel, alors que les rôles d'agent sont souvent joués par des
syntagmes nominaux.

\paragraph{Catégorie principale} Ce trait indique si un syntagme donné est
sujet ou objet du verbe. Pour capturer cette information, ce trait peut être «
S » ou « SV » suivant la position du syntagme qu'on cherche à désambiguïser
dans l'arbre syntaxique. Dans notre exemple, le trait vaudra S pour SN1, et SV
pour SN2. Ainsi, SN1 a plus de chance \textit{a priori} d'être agent que SN2.

\paragraph{Voix} Le contre exemple classique est la voix passive où le sujet
syntaxique est l'objet sémantique, par exemple dans la phrase « Le nuage est
observé par l'enfant. ». Selon \cite{roland2002verb}, environ 7\% des phrases
utilisent la voix passive dans le Brown Corpus et le Wall Street Journal
Corpus. Dans notre exemple, si l'analyse syntaxique a détecté l'utilisation de
la voix passive, SN1 n'est probablement plus agent, mais bien patient.

\paragraph{Chemin syntaxique} Contient l'ensemble du chemin depuis le syntagme
considéré jusqu'au prédicat. Par exemple, en considérant SN1 et sachant que V
est le prédicat, le chemin sera : $SN1 \uparrow VP \downarrow V$. Ce trait peut
être considérée comme plus spécifique que la catégorie principale, et permet
d'identifier précisément la manière dont un syntagme donné est relié à son
prédicat. \cite{gildea2002automatic} ont choisi après expérimentation de
généraliser les étiquettes des verbes ($VBZ$ et $VBD$ apparaissent sous la
forme $VB$).

\paragraph{Position} Ce trait n'est pas directement syntaxique et est conçu
pour limiter les erreurs dues à une mauvaise analyse syntaxique initiale qui
auraient faussé le trait « Catégorie principale ». Dans la phrase « Il a mangé
des pancakes », « pancakes » est à droite du prédicat, alors que « Il » est à
gauche.

\paragraph{Tête du syntagme} Ce trait lexical permet de capturer les mots qui
sont souvent associés à un rôle donné. Par exemple, « Il » est souvent agent,
alors que « histoire » représenterait plutôt le thème. Les mots grammaticaux se
retrouvant en tête de syntagme sont souvent utiles, ce qui est le cas de
\textit{that}, \textit{of} ou \textit{along} en anglais.

\paragraph{Cadre de sous-catégorisation} Indique l'ensemble des arguments
syntaxiques d'un verbe, ce qui permet notamment de distinguer un usage
intransitif d'un usage transitif.

Depuis \cite{gildea2002automatic}, de nombreux auteurs ont proposés de nouveaux
traits améliorant quelque peu les performances. Citons ici :

\begin{itemize}

    \item l'utilisation de la partie du discours de la tête de syntagme,
        introduite par \cite{surdeanu2003using} ;

    \item l'appartenance du verbe à un cluster donné de verbes syntaxiquement
        proches et donc potentiellement sémantiquement proches (comme « manger
        » et « dévorer »), introduite par \cite{pradhan2004shallow} ;

    \item une {syntactic frame}, représentation différente du chemin syntaxique
        avec des meilleures propriétés de généralisation
        \citep{xue2004calibrating} ;

    \item les entités nommées présentes dans les syntagmes, introduite par
        \cite{pradhan2005semantic} pour identifier les rôles secondaires de
        PropBank ;

    \item des traits moins syntaxiques comme des n-grams de parties du
        discours, des sac de mots pleins, etc. \citep{surdeanu2007combination}.

\end{itemize}

Des listes plus complètes ont étés établies par ailleurs dans la littérature
\citep{pradhan2005semantic,marquez2008semantic,palmer2010semantic}.

Cette section introduit des techniques qui ne sont pas spécifique à un trait ou
à une algorithme de classification. Au contraire, elles sont applicables plus
généralement.

\subsubsection{Sélection automatique des traits}

\cite{dinu2007sometimes} suit \cite{mihalcea2002instance} en choisissant
automatique les traits à appliquer pour son algorithme. Les deux papiers
obtiennent le même résultat : utiliser moins de traits permet d'obtenir un
meilleur score. Ces résultats encourageant sont peut-être dus à la petite
taille du corpus. En effet, pour éviter le surapprentissage, réduire le nombre
de traits est un bon moyen d'améliorer la précision \citep{van2004bias}.

% Oh tiens, j'ai dit pareil en plus long là.

Réduire le nombre de traits peut améliorer la performance d'un système
\cite{mihalcea2002instance,dinu2007sometimes}. Du point de vue de
l'apprentissage automatique, une des causes possibles pour des faibles
performances est le sur-apprentissage ; où on apprend davantage à être
performant sur les exemples observés tout en généralisant mal sur les exemples
nouveaux. Une des raisons possibles de ce problèmes est le trop grand nombre de
traits utilisés. La littérature sur l'annotation en rôles sémantiques en
particulier utilise un très grand nombre de traits, qui ne sont pas forcément
utilisés à leur plein potentiel suivant la taille des corpus d'apprentissage.

Dans les deux tâches étudiées, la littérature a observé l'avantage de valider
expérimentalement l'utilisation d'un trait donné. Dans le cas de l'annotation
en rôles sémantiques, \cite{xue2004calibrating} a remarqué que de nombreux
traits potentiellement intéressants n'apportaient en réalité aucune information
nouvelle. Plus radicalement, \cite{mihalcea2002instance,dinu2007sometimes} ont
choisi pour la tâche d'annotation en rôles sémantiques d'utiliser l'ensemble
des traits donnant les meilleurs résultats ; et se rendent compte que
l'approche donnant les meilleurs résultats est celle de \textit{forward
selection}. Le principe est de commencer sans trait, puis d'itérer en ajoutant
le trait améliorant le plus les résultats à chaque étape. Dès que les résultats
ne s'améliorent plus, le processus est arrêté et les traits sélectionnés sont
utilisés pour le modèle final. Les résultats ont étés améliorés de manière
significative.

Il est important pour pouvoir utiliser cette technique d'avoir un cycle
apprentissage-évaluation relativement rapide. Il faut en effet à chaque étape
évaluer l'apport de chacune des fonctionnalités. En effet, la complexité dans
le pire des cas est en $O(n^2)$, $n$ étant le nombre de traits possibles.

\subsubsection{Combinaison de classifieurs}

Il a souvent été observé qu'un ensemble de classifieurs combinés par la suite
permettait d'obtenir des résultats intéressants. Par exemple,
\cite{kohomban2005learning,dinu2007sometimes} obtiennent des classifieurs
individuels (un par trait) souvent inférieurs à la baseline ; puis les
combinent à l'aide de différentes techniques de vote. Le résultat obtenu est
alors non seulement supérieur à la performance individuelle des classifieurs
mais aussi supérieur à un classifieur intégrant l'ensemble des traits.
\cite{mouton2009induction} utilise une version modifiée de l'algorithme Shared
Nearest Neighbours pour prendre en compte ses différents espaces
distributionnels  (cf. section \ref{espacesdistrib}).

\subsubsection{Utiliser un seul classifieur}

\cite{kohomban2005learning} choisissent de faire un apprentissage sur des
classes très générales (les « top nouns » de WordNet), ce qui permet
d'entraîner un seul classifieur sur les noms. Ce classifieur utilise, sur une
fenêtre de 2+2 mots pleins, les traits suivants : formes, étiquettes
morphosyntaxiques et relations syntaxiques (par ex. modifieur adverbial ou
sujet du verbe). Pour chacun de ces traits, un classifieur k-NN est entraîné en
rapprochant artificiellement les exemples qui ont étés étiquetés avec une
classe qui existe dans les sens du mot considéré. Par exemple, les occurences
annotées du mot « journal » en tant que \textit{GROUP} ne sont pas considérées
au moment d'annoter le mot « bande magnétiqe », étant donné que \textit{GROUP}
n'est le « top noun » d'aucun des sens de « bande magnétique ». Ceci est fait
via l'exemplar weighting implémenté dans TiMBL, le logiciel utilisé pour la
classification. Cette technique permet d'utiliser un maximum d'exemples annotés
tout en diminuant le bruit quand c'est posible. Le sens WordNet le plus
fréquent est aussi choisi comme traits. Les différents traits sont ensuite
combinés via un système de vote, ce qui permet de battre la baseline, alors que
chaque trait utilisé indépendemment est moins performant que la baseline.

% TODO papier qui utilise pas les top nouns mais va un peu plus loin dans l'arbre WordNet

% TODO VerbNet unknown verbs classifier

%%
%% Évaluation
%%

\subsection{Évaluation}
\label{subsec:evaluation}

% TODO WSD à supprimer !

L'évaluation est un problème central en apprentissage automatique et en
Traitement Automatique des Langues, et l'analyse sémantique ne fait pas
exception. L'idéal est d'évaluer l'amélioration obtenue en incorporant le
système développé dans un système plus large et directement utile, comme ça a
été fait pour les systèmes de questions-réponses \citep{shen2007using} ou
l'analyse d'opinions \citep{das2012structure}. Ce sont des évaluations
\textit{in vivo}. Quand ce n'est pas possible, on se content d'évaluations
\textit{in vitro} qui sont très utile pour attester de la pertinence des
système d'analyse sémantique. Dès lors qu'une vérité-terrain est disponible, la
littérature utilise la précision, le rappel et la F-mesure tels qu'ils sont
définis en recherche d'information pour l'évaluation. Quand ce n'est pas le
cas, le problème est plus complexe (cf. section \ref{sec:evalunsupervised}).

Des \textit{baselines} sont souvent établies ; ce sont des algorithmes souvent
extrêmement simples qui représentent la limite basse qu'un système de
désambiguïsation lexicale doit dépasser. La \textit{baseline} la plus courante
est celle du sens le plus fréquent ; et c'est une baseline dite forte. En
effet, pour un mot donné, choisir le sens le plus fréquent permet d'atteindre
un score assez honorable. Par exemple, lors de SemEval-2007, la
\textit{baseline} avait une exactitude\footnote{\textit{accuracy} en anglais,
soit le nombre de vrais positifs et de vrais négatifs sur l'ensemble des
exemples} de 78.9\% pour une désambiguïsation de tous les mots avec des sens
grossiers. Le meilleur système a atteint un score 82.5\%, et seulement 25\% des
systèmes ont battu la baseline.

Quid de la limite haute ? C'est l'accord inter-annotateurs qui est
traditionellement utilisé pour mesurer la limite haute.
\cite{navigli2007semeval} ont remarqué que l'utilisation de sens grossiers
augmentait à la fois l'accord inter-annotateurs et la performance des systèmes,
qui restaient tout de même en dessous de ce score, ce qui souligne les
améliorations possibles de performance.

Entre 1998 et 2010, des campagnes d'évaluation ont permi à différents systèmes
de désambiguïsation lexicale de se comparer, les prochaines campagnes étant
prévues en 2012 et 2013\footnote{respectivement
http://www.cs.york.ac.uk/semeval-2012/ et
http://www.cs.york.ac.uk/semeval-2013/}. Nous traiterons ici des fait les plus
marquants des campagnes récentes : SemEval-2007 et SemEval-2010. Pour une
analyse plus complète et détaillée des campagnes jusqu'à 2007, se référer à
\cite{navigli2009word}.

Les dernières campagnes évaluant la désambiguïsation lexicale sur tous les mots
d'un texte sans domaine particulier sont les tâches 7 et 17 de SemEval-2007, la
différence principale étant l'inventaire de sens utilisé. En effet, la tâche 17
a utilisé WordNet 2.1 et le WSJ, alors que la tâche 7 a utilisé un inventaire
de sens grossier basé sur WordNet et des textes provenant de différents
domaines. Le meilleur système pour les sens fins a atteint un F-score de
59.1\%, et le meilleur pour les sens grossiers à atteint 83.21\%. Ces résultats
suggèrent que WordNet ne permet pas d'atteindre de bons résultats. SemEval 2010
a présenté trois tâches de désambiguïsation lexicale. Dans La tâche 3, adaptée
à la traduction automatique, chaque système devait proposer la bonne traduction
d'un mot donné dans le corpus parallèle Europarl. Les systèmes présentés n'ont
pas battu la baseline des traductions les plus fréquentes. La tâche 14 a évalué
des systèmes non supervisés ; l'évaluation ayant posé problème
(\ref{evalunsupervised}) il serait futile de citer des résultats ici. Enfin, la
tâche 17 s'est concentré sur un domaine particulier (l'environnement). Moins de
20\% des systèmes présentés ont battu la baseline qui était à 50.5\%. De
manière intéressante, les meilleurs systèmes sont ceux qui n'étaient pas
complètement supervisés et ont appris aussi sur des larges corpus généraux, ce
qui a permis d'augmenter la performance de désambiguïsation pour les mots non
spécifiques au domaine.

\subsubsection{Évaluation des approches non supervisées}
\label{sec:evalunsupervised}

Les approches non supervisées sont difficiles à évaluer. En effet, il n'y a pas
de vérité-terrain à laquelle se comparer. Pour pallier ce problème, on peut
utiliser un mapping depuis les sens induits jusqu'aux sens d'un inventaire pour
lequel on dispose une vérité-terrain. Deux sources d'erreurs existent alors :
les deux inventaires ne sont pas nécessairement compatibles et le mapping peut
être erroné, en liant des sens qui n'ont pas de rapport. Le problème du mapping
erroné peut être évité en ne faisant pas de mapping mais en considérant
l'annotation effectuée comme un clustering et en utilisant donc des techniques
de comparaison de clustering. Différents algorithmes permettent l'évaluation
d'un algorithme de clustering par rapport à une vérité-terrain.

La campagne d'évaluation d'induction de sens de SemEval 2007
\citep{manandhar2010semeval} était aussi l'occasion d'évaluer l'efficacité de
différentes mesures d'efficacité des clustering de sens induits. Il s'avère que
les différentes mesures d'évaluation ont donné des résultats très différents
\citep{pedersen2010duluth}. La V-mesure a encouragé les résultats aléatoires,
le rappel supervisé a ramené tous les systèmes participants à 0.06\% autour de
la baseline (ce qui rend l'évaluation difficile) et le \textit{paired F-Score}
a placé la baseline au dessus de tous les systèmes \footnote{ce qui semble
indiquer qu'un seul sens par mot dans un corpus spécifique est la meilleure
solution.}. Une mesure d'évaluation fiable et consistente permettrait pourtant
d'évaluer avec précision les approches prometteuses que sont les approches
non-supervisées.

En ce qui concerne l'annotation en rôles sémantiques, le lecteur est invité à
se référer à \citep{surdeanu2008conll} et \citep{hajic2009conll} qui expliquent
en détail l'évaluation des différents systèmes ayant participé aux tâches
d'annotation en rôles sémantique de CoNLL en 2009 et 2009. Il est intéressant
de noter que l'annotation en rôles sémantiques multilingue a extrêmement bien
fonctionné, avec les meilleurs systèmes obtenant des scores proches de 80\%
pour tous les langages.

%La V-mesure a été introduite par  et utilise deux attributs (homogénéité et
%complétude), à la manière de la précision et du rappel, puis combinée pour
%obtenir la V-mesure, à la manière du F-score. Par exemple, quand le résultat
%est identique à la vérité-terrain, l'homogénéité, la complétude et la V-mesure
%valent tous 1. L'\textbf{homogénéité} évalue, pour chaque cluster produit par
%un système, la proportion d'éléments qui viennent du même cluster de la vérité
%terrain. La complétude évalue quant à elle, pour chaque cluster de la vérité
%terrain, la proportion d'éléments qui sont présents dans un même cluster
%produit par le système.

\subsection{Adaptation au domaine}

syntaxe, sémantique

%\subsection{FrameNet}

%%
%% Pistes : pas dans un état de l'art !
%%
%\section{Voies de recherche (ALL)}
%
%La désambiguïsation lexicale et l'annotation en rôles sémantiques sont des
%domaines très actifs, et un certain nombre de voies restent à explorer. Nous
%citerons ici simplement la mise en commun des deux tâches d'analyse sémantique
%que nous avons présentées, l'analyse sémantique d'une langue autre que
%l'anglais (ici le français) et l'évaluation des approches non supervisées.
%
%\subsection{Désambiguïsation lexicale et annotation en rôles sémantiques}
%
%La littérature mentionne différents essais visant à combiner les tâches de
%désambiguïsation lexicale et d'annotation en rôles sémantiques, dans le but
%d'améliorer les performances de chacune des tâches
%\citep{dang2005role,moreda2006role,che2010jointly}. En effet, le sens d'un
%verbe permet de déterminer avec une meilleure précision la \textit{frame} qu'il
%doit déclencher. De la même manière, connaître les rôles sémantiques des
%arguments d'un verbe donne des indices supplémentaires pour définir son sens.
%Il est peut-être bénéfique dans ce contexte d'opérer une modélisation jointe
%pour parvenir à un optimum plus global.
%
%Cependant, ces méthodes, même si elles proposent des améliorations
%statistiquement significatives, n'ont pas encore révolutionné ces deux tâches
%d'analyse sémantiques. Il y a notamment un manque de ressources communes qui
%commence à être comblé par différentes ressources. Ontonotes, par exemple, ne
%se contente pas de regrouper des sens WordNet mais inclut aussi un corpus
%annoté syntaxiquement et en rôles sémantiques. De manière plus originale,
%eXtended WordFrameNet \citep{laparra2010extended} est constitué du corpus
%FrameNet désambiguïsé lexicalement et ajoutent des informations de rôles
%sémantiques à WordNet. Les résulats sont prometteurs : les performances d'un
%système de désambiguïsation lexicale ont été améliorées.
%
%\subsection{Analyse sémantique du français}
%
%\subsubsection{Parallélisme sémantique}
%
%L'anglais est la langue la plus utilisée pour le Traitement Automatique des
%Langues, et il est toujours intéressant de se demander si les outils et
%ressources développés sont applicables pour d'autres langues. Par exemple,
%l'anglais n'est pas une langue morphologiquement riche et cela empêche
%l'utilisation d'outils développés pour l'anglais sur des langues plus riches
%\citep{tsarfaty2010statistical}. Au niveau sémantique, les études sont plus
%rares. \cite{pado2007annotation} se demande si les \textit{frames} FrameNet,
%\textit{a priori} indépendantes de la structure syntaxique, sont utilisables
%directement en français en évaluant l'accord inter-annotateurs sur un corpus
%donné. Ce corpus ayant la propriété d'être un corpus parallèle ; cela a permis
%de comparer l'accord avec celui obtenu par \cite{pado2006optimal} pour
%l'allemand, langue réputée plus proche de l'anglais. Il s'avère que les scores
%sont proches, et il est donc raisonnable d'utiliser FrameNet comme base pour le
%français. En ce qui concerne PropBank, les sens sont déterminés à partir
%d'informations syntaxiques spécifiques à l'anglais : une traduction n'est donc
%pas envisageable.
%
%\subsubsection{WordNet en français}
%
%WordNet, malgré sa granularité trop fine, est une ressource libre extrêmement
%intéressante qui permet de nombreuses applications. Les projets souhaitant
%établir directement un WordNet français original n'ayant pas abouti
%complètement, il est devenu naturel de se pencher à une traduction de WordNet ;
%en gardant les différents synsets, mais en traduisant les mots représentés dans
%ces synsets. Différentes approches ont étés utilisées, et nous retiendrons ici
%deux WordNets français : WOLF \cite{sagot2008construction} et JAWS
%\cite{mouton2010jaws}. WOLF était à l'origine très précis pour une couverture
%faible des mots les plus ambigus, mais des efforts sont faits pour étendre la
%ressource aux mots les plus difficiles \citep{sagot2012automatic}. De son côté,
%JAWS a été construit à partir des espaces distributionnels cités plus hauts, et
%a obtenu une précision légèrement inférieure à WOLF pour une couverture plus
%importante.
%
%« Les Verbes Français » est une ressource initialement publiée en 1997 et
%toujours maintenue (dernière modification date de 2011) sous un format XML
%facile d'accès dans le but d'encourager diverses applications du TAL. C'est un
%thésaurus de classes sémantico-syntaxiques qui repose sur l'hypothèse d'une
%adéquation entre « les schèmes syntaxiques de la langue français et
%l'interprétation sémantique qu'en font les locuteurs de cette langue ». À la
%manière de la classification faite par \cite{levin1993english} pour les verbes
%anglais, c'est une ressource extrêmement riche et facile d'accès qui gagnerait
%à être utilisée pour l'annotation en rôles sémantiques. Un PropBank du français
%pourrait être établi à partir de ces classes ; ce qui faciliterait une
%annotation supervisée du français.
%

\section{Bilan}

Désambiguïsation lexicale et annotation en rôles sémantiques sont deux tâches
qui, traitées par des approches statistiques, permettent d'obtenir des
résultats intéressants. La route vers l'intégration plus systématique à des
systèmes existants est encore longue, mais les progrès et performances sont
déjà satisfaisants. Nous avons dans cet état de l'art abordé les questions de
l'apprentissage statistique et des ressources nécessaires pour un tel
apprentissage. Les possibilités sont encore nombreuses et restent à être
explorées, en particulier dans le traitement d'autres langues comme le
français.

