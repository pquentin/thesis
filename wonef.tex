\chapter{\newjaws{} : une traduction de WordNet}
\label{ch:wonef} 

% Gaël: abstract trying to be "à la" Gregory grefenstette
Identifier les sens possibles des mots du vocabulaire est un problème difficile demandant un travail manuel très conséquent. Ce travail a été entrepris pour l'anglais : le résultat est la base de données lexicale WordNet, pour laquelle il n'existe encore que peu d'équivalents dans d'autres langues. Néanmoins, des traductions automatiques de WordNet vers de nombreuses langues cibles existent, notamment pour le français. JAWS est une telle traduction automatique utilisant des dictionnaires et un modèle de langage syntaxique. Nous améliorons cette traduction, la complétons avec les verbes et adjectifs de WordNet, et démontrons la validité de notre approche via une nouvelle évaluation manuelle. En plus de la version principale nommée \newjaws{}, nous produisons deux versions supplémentaires : une version à haute précision (93\% de précision, jusqu'à 97\% pour les noms), et une version à haute couverture contenant 109~447 paires (littéral, synset).


\section{Introduction}
\label{sec:intro}

WordNet est une base de données lexicale en développement depuis les années 80 \citep{Fellbaum1998}. Cette base est organisée autour du concept de \textbf{synset} (ensemble de synonymes), chaque synset représentant un sens très précis à l'aide d'une définition et d'un certain nombre de mots que nous nommons littéraux. Ces synsets sont liés par différentes relations sémantiques telles que la méronymie et l'hyponymie. Malgré des défauts reconnus \citep{Boyd-graber06addingdense} principalement liés à la granularité trop fine des sens, WordNet reste une ressource extrêmement utile et reproduire ce travail pour d'autres langues serait coûteux et difficile à maintenir. Et malgré quelques problèmes théoriques, \cite{fellbaum2007connecting,demelo2008utility} montrent que traduire WordNet en gardant sa structure et ses synsets mène à des ressources linguistiques utiles.

Les traductions automatiques de WordNet emploient une approche dite d'extension (\textit{extend approach}) : la structure de WordNet est préservée et seuls les littéraux sont traduits. Trois techniques principales représentent cette approche dans la littérature. La plus simple utilise des dictionnaires bilingues pour faciliter le travail des lexicographes qui filtrent ensuite manuellement les entrées proposées \citep{vossen1998eurowordnet,pianta2002developing,tufis2004balkanet}. Une deuxième méthode de traduction utilise des corpus parallèles, ce qui évite l'utilisation de dictionnaires qui peuvent entraîner un biais lexicographique. \cite{dyvik2004translations} représente cette méthode en s'appuyant sur des \textit{back-translations} entre le norvégien et l'anglais, alors que \citep{SagotFiser2008} combinent un lexique multilingue et les différents WordNets de BalkaNet comme autant de sources aidant à la désambiguïsation. Enfin, plus récemment, des ressources telles que Wikipédia ou le Wiktionnaire ont été explorées. Grâce aux nombreux liens entre les différentes langues de ces ressources, il est possible de créer de nouveaux wordnets \citep{melo2009towards,navigli2010babelnet} ou d'améliorer des wordnets existants \citep{hanoka2012wordnet}.

Concernant le français, l'EuroWordNet \citep{vossen1998eurowordnet} est la première traduction française de WordNet. C'est une ressource d'une couverture limitée qui demande des améliorations significatives avant de pouvoir être utilisée \citep{jacquin_al2007}, et qui n'est ni libre ni librement accessible. WOLF est une seconde traduction initialement construite à l'aide de corpus parallèles \citep{SagotFiser2008} et étendue depuis avec différentes techniques \citep{ApidianakiSagot2012}. WOLF est distribué sous une licence libre compatible avec la LGPL et c'est aujourd'hui le WordNet français standard. Enfin, JAWS \citep{MoutonChalendar2010} est une traduction des noms de WordNet développée à l'aide de dictionnaires bilingues et d'un modèle de langue syntaxique.

Nos travaux étendent et améliorent les techniques utilisées dans JAWS et l'évaluent à l'aide d'une adjudication de deux annotateurs. Le résultat de ce travail est \newjaws{}\footnote{Ce travail a été en partie financé par le projet ANR ASFALDA ANR-12-CORD-0023.}. Il se décline en trois versions pour répondre à différents besoins. Le \newjaws{} principal a un F-score de 70.9\%, une autre version a une précision de 93.3\%, et une dernière contient 109~447 paires (littéral, synset).

L'approche de JAWS consiste à combiner des sélecteurs variés permettant de choisir les traductions adaptées à chaque synset (section \ref{sec:jaws}). Les contributions principales de cet article sont l'amélioration de JAWS et sa complétion en ajoutant les verbes et les adjectifs (section \ref{sec:improving_jaws}) et son évaluation (sections \ref{sec:evaluating_jaws} et \ref{sec:results}). Cette évaluation se fait à travers une adjudication elle-même validée par la mesure de l'accord inter-annotateur, ce qui montre la validité de l'approche par extension pour traduire WordNet.

\section{JAWS}
\label{sec:jaws}

\subsection{Processus de traduction}
\label{subsec:translation_process}

\cite{MoutonChalendar2010} ont conçu JAWS comme un algorithme faiblement supervisé qui ne demande aucune donnée annotée manuellement. Pour traduire un wordnet source, JAWS s'appuie sur un dictionnaire bilingue et un modèle de langue syntaxique pour le langage cible.

Le dictionnaire bilingue est une concaténation du dictionnaire bilingue SCI-FRAN-EurADic\footnote{\url{http://catalog.elra.info/product_info.php?products_id=666}} et des liens entre les Wiktionnaires français et anglais\footnote{\url{http://www.wiktionary.org/}}. Le modèle de langue syntaxique a été entraîné sur un grand corpus extrait du web \citep{grefenstette2007conquering}. Le corpus a été analysé par LIMA \citep{LIMA}, une chaîne d'analyse linguistique ici utilisée comme un analyseur syntaxique à base de règles produisant des dépendances syntaxiques fines. Pour une relation donnée $r$ et un mot $x$, le modèle de langue indique quels sont les 100 premiers mots co-occurrant le plus fréquemment avec $x$ dans la relation $r$. Avec le mot \textit{avion} et la relation de complément du nom, le mot \textit{billet} modifie le plus \textit{avion} : \textit{billet d'avion} est fréquent dans le corpus. Le modèle de langue ici présenté peut-être visualisé sur \url{http://www.kalisteo.fr/demo/semanticmap/index.php}.

Grâce aux dictionnaires, JAWS n'a pas besoin de sélectionner les littéraux de chaque synset parmi l'ensemble du vocabulaire mais seulement parmi un petit nombre de candidats (9 en moyenne). Le processus de traduction se fait en trois étapes :
\begin{enumerate}
    \item Créer un wordnet vide : la structure de WordNet est préservée, mais les synsets eux-mêmes n'ont pas de littéraux associés.
    \item Choisir les traductions les plus faciles parmi les candidats des dictionnaires pour commencer à remplir JAWS.
    \item Étendre JAWS de manière incrémentale en utilisant le modèle de langue, les relations entre synsets et le JAWS déjà existant.
\end{enumerate}

\paragraph{Sélecteurs initiaux} Quatre algorithmes que nous nommons sélecteurs initiaux choisissent des traductions correctes parmi celles qui sont proposées par les dictionnaires. Premièrement, les mots qui apparaissent dans un seul synset ne sont pas ambigüs et il suffit d'ajouter toutes leurs traductions au WordNet français : c'est le sélecteur par monosémie. C'est le cas de \textit{grumpy} : toutes ses traductions sont validées dans le synset où il apparaît. Deuxièmement, le sélecteur par unicité identifie les mots n'ayant qu'une seule traduction et la valident dans tous les synsets où elle est présente. Les cinq synsets contenant \textit{pill} en anglais sont ainsi complétés avec \textit{pilule}. Un troisième sélecteur vise à traduire les mots qui ne sont pas dans le dictionnaire en utilisant directement la traduction anglaise : c'est le sélecteur des transfuges. Un quatrième sélecteur utilise la distance d'édition de Levenshtein : si la distance entre un mot anglais et sa traduction est petite, on peut considérer que c'est le même sens (c'est le cas par exemple pour \textit{portion} ou encore \textit{university}), malgré l'existence de certains faux amis. Ces quatre sélecteurs produisent une première version du WordNet français qui contient assez de traductions pour pouvoir ensuite utiliser le modèle de langue et continuer de compléter les synsets.

\paragraph{Expansion de JAWS} JAWS étant partiellement rempli, une nouvelle étape d'expansion tire parti des relations entre les synsets de WordNet pour valider de nouvelles traductions. Par exemple, si :

\begin{itemize}
    \item un synset S1 est méronyme d'un synset S2 dans WordNet,
    \item il existe un contexte où un littéral dans S1 est méronyme d'un littéral candidat C dans S2,
\end{itemize}
alors ce littéral est considéré comme correct. La tâche de traduction est ainsi réduite à une tâche de comparaison entre d'une part les relations lexicales entre les synsets de WordNet et d'autre part les relations lexicales entre les lexèmes du français.

Prenoms l'exemple de \textit{quill} qui peut se traduire par \textit{piquant} ou \textit{plume} (Figure \ref{meronymyexample}). Dans WordNet, \textit{quill} est méronyme de \textit{porcupine} qui a déjà été traduit par \textit{porc-épic} par un sélecteur initial. Dans le modèle de langue, \textit{piquant} fait partie des compléments du noms de \textit{porc-épic} mais ce n'est pas le cas de \textit{plume}. Ici, la relation de complément du nom implique la méronymie et c'est donc \textit{piquant} qu'il faut choisir comme la traduction correcte de \textit{quill}. Le modèle de langue a permis la désambiguïsation parmi les deux traductions possibles.

%\tikzstyle{block}=[draw, fill=blue!5, rectangle, minimum height=0.5cm, minimum width=3cm, text width=5cm]
%
%\begin{figure}[!ht]
%  \centering
%  \begin{tikzpicture}[auto, node distance=2cm,>=latex']
%    % Inspired from http://www.texample.net/tikz/examples/control-system-principles/
%    % first place and connect the outer blocks that represent synsets
%      \node [block, text width=5cm] (quill) {\textbf{Synset S1} \\ - Anglais : quill \\ - Français : piquant? plume? \\ (a stiff hollow protective spine on a porcupine or hedgehog)};
%      \node [block, text width=4.8cm, right of=quill, node distance=9cm] (porcupine) {\textbf{Synset S2} \\ - Anglais : porcupine, hedgehog \\ - Français : porc-épic \\ (relatively large rodents with sharp erectile bristles mingled with the fur)};
%    \draw [<-] (porcupine) -- node[above] {méronyme de} (quill);
%    \draw [<-] (porcupine) -- node[below] {(relation WordNet)} (quill);
%
%    % then the syntactic model relations
%    \node [block, below of=porcupine, text width=4.8cm, node distance=3cm] (porcupinesynt1) {porc-épic};
%    \node [block, below of=quill, node distance=3cm] (quillsynt1) {\Large{mémoire}, \Large{piquant}, \large{poil}, \large{épine}, yéti, ragoût, grotte, \small{tactique}, \small{pelage}, \small{dextre}, \small{aiguille}, ...};
%    \draw [<-] (porcupinesynt1) -- node[above] {complément du nom de} (quillsynt1);
%    \draw [<-] (porcupinesynt1) -- node[below] {(modèle de langue)} (quillsynt1);
%
%  \end{tikzpicture}
%  \caption{\protect\centering\label{meronymyexample}Traduction via la relation de méronymie de partie.}
%\end{figure}

Un problème potentiel avec cette approche est que la relation de complément du nom n'est pas limitée à la méronymie. Par exemple, le mot \textit{mémoire} qui apparaît dans le modèle de langue (Figure TODO) vient d'un livre intitulé \textit{Mémoires d'un porc-épic}. Heureusement, \textit{mémoire} n'est pas dans les candidats de \textit{quill} et ne peut pas être choisi comme une traduction. Paradoxalement, le modèle de langue ne peut pas choisir entre deux mots très différents, mais est capable de choisir la traduction correcte d'un mot polysémique. Alors que traduire WordNet automatiquement avec un dictionnaire ou un modèle de langue syntaxique est impossible, combiner les deux ressources permet de résoudre le problème.

Chaque sélecteur suit le même principe que le sélecteur par méronymie de partie et traduit de nouveaux synsets en identifiant les relations entre lexèmes via le modèle de langue syntaxique. La correspondance entre la relation de complément du nom et la relation de méronymie est directe, mais ce n'est pas le cas pour les autres relations : il n'y a par exemple pas de relation syntaxique qui exprime directement la synonymie entre deux lexèmes. Pour ces relations, il est nécessaire d'employer soit des motifs lexicaux \citep{hearst1992automatic} soit des relations syntaxiques de deuxième ordre \citep{lenci2012identifying}. Ce sont ces dernières, aussi nommées relations paradigmatiques, que JAWS utilise. Pour la synonymie, si deux mots partagent les mêmes co-occurents dans une relation syntaxique donnée, alors ils peuvent être synonymes dans ce contexte. Pour les noms, les relations syntaxiques qui donnent les meilleurs résultats sont les relations de complément du nom, d'objet du verbe et d'apposition. Concrètement, si deux noms qui modifient les mêmes noms sont les objets des mêmes verbes ou sont apposés aux mêmes noms, alors il est probable qu'ils soient synonymes et si l'un des deux est déjà dans un synset, alors on peut y ajouter le second. Par exemple, \textit{avant-propos} et \textit{préface} partagent les mêmes compléments du noms : \textit{livre, édition, ouvrage}. Le sélecteur par synonymie peut ajouter \textit{avant-propos} une fois que le littéral \textit{préface} est dans JAWS. \citep{MoutonChalendar2010,mouton2011phd} décrivent d'autres sélecteurs exploitant notamment les relations d'hyperonymie et d'hyponymie.

\subsection{Limites de JAWS}
\label{subsec:limitations}

JAWS souffre d'un certain nombre de limites. Avant tout, il ne contient que des noms, ce qui empêche de l'utiliser dans de nombreuses applications. Ensuite, la façon dont il a été évalué rend difficile tout jugement sur sa qualité. En effet, JAWS a été évalué en le comparant à l'EuroWordNet du français et à WOLF 0.1.4 (qui date de 2008). Ces deux WordNets du français ne sont pas des annotations de références : ils souffrent soit d'une précision limitée soit d'une couverture limitée.

M\&C ont décidé de compléter cette évaluation limitée par une évaluation manuelle des littéraux n'existant pas dans WOLF, mais elle n'a été faite que sur 120 paires (littéral, synset). La précision de JAWS est évaluée à 67,1\% \citep{mouton2011phd}, ce qui est plus bas que celle de WOLF 0.1.4 et considérablement plus bas que la précision de WOLF 1.0b\footnote{Nous remercions Benoît Sagot pour nous avoir fourni cette version préliminaire de WOLF 1.0.}. Ce score, même bas, est à prendre avec précaution étant donné la taille de l'échantillon de test : l'intervalle de confiance est d'environ 25\%. Une autre limite de JAWS est qu'il ne contient qu'une seule et unique ressource qui ne correspond pas à tous les besoins.

À notre connaissance, les traductions automatiques de WordNet actuelles n'existent qu'en une seule version où les auteurs décident eux-mêmes quelle métrique optimiser. Nous fournissons aussi une telle version, mais ajoutons aussi deux ressources qui peuvent servir des besoins différents. Même si notre \newjaws{} à haute précision est petit, il peut être utilisé comme une annotation de référence et servir pour entraîner un système d'apprentissage. Une ressource à haute couverture peut servir de base à une correction manuelle ou servir pour une intersection à d'autres ressources, ce qui est la raison pour laquelle nous en fournissons une aussi.

\section{\newjaws{}: un JAWS nominal amélioré}
\label{sec:improving_jaws}

Cette section présente les trois améliorations essentielles qui ont étés apportées à JAWS. Un changement non détaillé est celui qui a mené à une meilleure rapidité d'exécution : JAWS se construit en plusieurs heures contre moins d'une minute pour WoNeF, ce qui a facilité les expérimentations.

\subsection{Sélecteurs initiaux}
\label{subsec:revisiting_extraction_heuristics}

% (En fait on ne l'a pas fait) JAWS extraction heuristics were suboptimal. To overcome this problem, we first allowed multiword expressions (MWE) as translation candidates. Even if the syntactic language model does not handle multiword expressions, heuristics do not share this limitation. 97\% of nouns MWE, 75\% of verbs MWE and 95\% of adjectives MWE are monosemous: we expect heuristics to produce good results on multiword expressions.

Les sélecteurs initiaux de JAWS ne sont pas optimaux. Alors que les sélecteurs par monosémie et par unicité sont conservés, nous avons changé les autres sélecteurs. Premièrement, le sélecteur des transfuges est supprimé : sa précision était très basse, même pour les noms.

Deuxièmement, un nouveau sélecteur considère les traductions candidates provenant de plusieurs mots anglais différents dans un synset donné : c'est le sélecteur par sources multiples. Par exemple, dans le synset \textit{line, railway line, rail line (the road consisting of railroad track and roadbed)}, les littéraux français \textit{ligne de chemin de fer} et \textit{voie} sont des traductions à la fois de \textit{line} et \textit{railway line}, et sont donc choisis comme traductions.

Troisièmement, le sélecteur de la distance de Levenshtein a été amélioré. 28\% du vocabulaire anglais est d'origine française \citep{finkenstaedt1973ordered}, et l'anglicisation a produit des transformations prévisibles. Il est possible d'appliquer ces mêmes transformations aux littéraux candidats français, et seulement alors d'appliquer la distance de Levenshtein. Nous commençons par supprimer les accents, puis appliquons différentes opérations. Par exemple, l'inversion des lettres "r" et "e" prend en compte (\textit{order}/\textit{ordre}) et (\textit{tiger}/\textit{tigre})\footnote{La distance de Damerau-Levenshtein qui prend en compte les inversions n'importe-où dans un mot \citep{damerau1964technique} a donné de moins bons résultats.}. Toutes les transformations ne s'appliquent qu'à la fin des mots : \textit{-que} est transformé en \textit{-k} ou \textit{-c} (\textit{marque} devient \textit{mark}), \textit{-té} vers \textit{-ty} (\textit{extremité} devient \textit{extremity}), etc. Les faux-amis ne sont toujours pas explicitement pris en compte.

\subsection{Apprentissage de seuils}
\label{subsec:learning_thresholds}

Dans JAWS, chaque littéral anglais ne peut avoir qu'une traduction française correspondante. La traduction choisie est celle qui a le meilleur score, indépendamment des scores des traductions moins bien notées. Cela a pour effet de rejeter des candidats valides et d'accepter des candidats erronés. Par exemple, JAWS n'inclut pas \textit{particulier} au synset \textit{(a human being) ``there was too much for one person to do''} parce que \textit{personne} est déjà inclus avec un score supérieur.

Dans \newjaws{}, nous avons donc appris un seuil pour chaque partie du discours et sélecteur. Nous avons d'abord généré les scores pour toutes les paires (littéral, synset) candidates, puis trié ces paires par score. Les 12 399 paires présentes dans l'évaluation manuelle associée à WOLF 1.0b (notre ensemble d'apprentissage) ont été jugées correctes tandis que les paires n'y étant pas ont été jugées erronées. Nous avons ensuite calculé les seuils maximisant la précision et le F-score. Le seuil qui maximise le F-score est utilisé dans les ressources à haut F-score et à haute couverture, tandis que le seuil maximisant la précision est utilisé dans la ressource à haute précision.

Une fois que ces seuils sont définis, les sélecteurs choisissent tous les candidats au-dessus du nouveau seuil, ce qui a deux effets positifs :

\begin{itemize}
    \item des candidats valides ne sont plus rejetés simplement parce qu'un meilleur candidat est aussi sélectionné, ce qui améliore à la fois le rappel et la couverture.
    \item les candidats invalides qui étaient jusque-là acceptés sont maintenant rejetés grâce au seuil plus strict : la précision s'en retrouve augmentée.
\end{itemize}

\subsection{Vote}
\label{subsec:voting}

Après l'application des différents sélecteurs, notre WordNet est large mais contient des synsets bruités. Comme toutes les traductions automatiques de WordNet, \newjaws{} doit alors être nettoyé \citep{sagot2012cleaning}. Dans \newjaws{}, le bruit provient de différents facteurs :

\begin{itemize}
    \item les sélecteurs essaient d'inférer des informations sémantiques à partir d'une analyse syntaxique sans prendre en compte toute la complexité de l'interface syntaxe-sémantique,
    \item l'analyseur syntaxique produit lui-même des résultats bruités,
    \item le modèle de langue syntaxique est produit à partir d'un corpus extrait du web lui-même bruité (texte mal écrit, contenu non textuel, phrases non françaises) et n'est pas une «~distribution idéale~» \citep{copestakelexicalised},
    \item les traductions déjà choisies sont considérées comme valides dans les étapes suivantes alors que ce n'est pas toujours le cas.
\end{itemize}

Pour la ressource haute-précision, il fallait donc un moyen de ne garder que les littéraux pour lesquels les sélecteurs étaient les plus confiants. Étant donné que, contrairement à JAWS, plusieurs sélecteurs peuvent choisir une même traduction (sous-section \ref{subsec:learning_thresholds}), notre solution est simple et efficace : les traductions validées par un bon sélecteur ou par plusieurs sélecteurs moyens sont conservées tandis que les autres sont supprimées. Ce principe de vote est aussi appelé méthode d'ensemble en apprentissage automatique. Les sélecteurs performants varient d'une partie du discours à une autre : le choix est fait sur un ensemble de développement contenant 10\% de notre référence.

Cette opération de nettoyage ne conserve que 18\% des traductions (de 87~757 paires (littéral, synset) à 15~625) mais la précision grimpe de 68,4\% à 93,3\%. Cette ressource à haute précision peut être utilisée comme donnée d'entraînement. Un défaut classique des méthodes de vote est de ne choisir que des exemples faciles et peu intéressants, mais la ressource obtenue ici est équilibrée entre les synsets ne contenant que des mots monosémiques et d'autres synsets contenant des mots polysémiques et plus difficiles à désambiguïser (section \ref{subsec:allvsbcs}).

\subsection{Extension aux verbes, adjectifs et adverbes}
\label{sec:extending_jaws}

Les travaux sur JAWS ont commencé par les noms parce qu'ils représentent 70\% des synsets dans WordNet. Nous avons continué ce travail sur les autres parties du discours qui sont aussi importantes pour examiner le sens d'un texte donné : verbes, adjectifs et adverbes. Les sélecteurs génériques ont ici été modifiés, mais il s'agira dans le futur d'implémenter des sélecteurs prenant en compte les spécificités des différentes parties du discours dans WordNet.

\paragraph{Verbes}
Les sélecteurs choisis pour les verbes sont le sélecteur par unicité et par monosémie. En effet, la distance de Levenshtein a donné des résultats médiocres pour les verbes : seuls 25\% des verbes choisis par ce sélecteur étaient des traductions correctes. Concernant les sélecteurs syntaxiques, seul le sélecteur par synonymie a donné de bons résultats, alors que le sélecteur par hyponymie avait les performances d'un classifieur aléatoire.

\paragraph{Adjectifs}
Les adjectifs sont traduits de la même manière que les noms : tout d'abord un nombre limité de sélecteurs initiaux remplit un WordNet vide, puis les sélecteurs syntaxiques complètent cette traduction avec le modèle de langue syntaxique. Tous les sélecteurs initiaux sont ici choisis, et le sélecteur syntaxique choisi est le sélecteur par synonymie. Ils ont donné de bons résultats qui sont présentés dans la section \ref{subsec:selectors}.

\paragraph{Adverbes}

Nous n'avons pas d'annotation de référence pour les adverbes, ce qui explique qu'ils ne sont pas inclus dans \newjaws{} : nous ne pouvons évaluer leur précision. Cependant, la comparaison avec WOLF (section~\ref{subsec:vswolf}) montre que les adverbes ont de meilleurs résultats que les autres parties du discours, ce qui laisse penser que c'est une ressource de qualité. C'est une ressource aussi très complémentaire : 87\% des adverbes proposés ne sont pas dans WOLF. Une fusion entre \newjaws{} et WOLF aurait trois fois plus d'adverbes que WOLF seul.

\section{\newjaws{}: un JAWS évalué}
\label{sec:evaluating_jaws}

\subsection{Développement d'une annotation de référence}
\label{subsec:gold_standard}

L'évaluation de JAWS souffre d'un certain nombre de limites (section \ref{subsec:limitations}). Pour évaluer rigoureusement notre propre traduction de WordNet, nous avons produit une annotation de référence. Pour chaque partie du discours, 300 synsets ont été annotés par deux annotateurs locuteurs natifs du français. Pour chaque traduction candidate fournie par nos dictionnaires, il fallait décider si oui ou non elle appartenait au synset. Puisque les dictionnaires ne proposent pas de candidats pour tous les synsets et que certains synsets n'ont pas de candidat valable, le nombre réel de synsets non vides est inférieur à 300 (section \ref{subsec:interannotator_agreement}).

Durant l'annotation manuelle, nous avons rencontré une difficulté importante découlant de la tentative de traduire WordNet dans une autre langue. Dans le cas de l'anglais vers le français, la plupart des difficultés proviennent des verbes et adjectifs figurant dans une collocation. Dans WordNet, ils peuvent être regroupés d'une manière qui fait sens en anglais, mais qui ne se retrouve pas directement dans une autre langue. Par exemple, l'adjectif \textit{pointed} est le seul élément d'un synset défini comme \textit{direct and obvious in meaning or reference; often unpleasant; ``a pointed critique''; ``a pointed allusion to what was going on''; ``another pointed look in their direction''}. Ces exemples se traduiraient par trois adjectifs différents en français : \textit{une critique dure}, \textit{une allusion claire} et \textit{un regard appuyé}. Il n'existe pas de solution satisfaisante lors de la traduction d'un tel synset : le synset résultant contiendra soit trop soit trop peu de traductions. Nous avons décidé de ne pas traduire ces synsets dans notre annotation manuelle. Ces problèmes de granularité concernent 3\% des synsets nominaux, 8\% des synsets verbaux et 6\% des synsets adjectivaux. Actuellement, \newjaws{} ne détecte pas de tels synsets. 

L'autre difficulté principale découle de traductions manquantes, ce qui peut être considéré comme un défaut de nos ressources. Les sens rares d'un mot sont parfois absents. Par exemple, le sens \textit{to catch} du jeu du chat (ou du loup) et le sens \textit{coat with beaten egg} du verbe \textit{to egg} ne sont pas présents. Aucun de ces sens ne sont dans les synsets les polysémiques (définis à la section~\ref{subsec:allvsbcs}), ce qui confirme que cela ne se produit que pour les sens rares. Pourtant, \newjaws{} pourrait être amélioré en utilisant des dictionnaires spécifiques pour, par exemple, les espèces (comme dans \cite{SagotFiser2008}), les termes médicaux, les entités nommées (en utilisant Wikipedia) et ainsi de suite. Un autre exemple est celui des adjectifs de jugement : il n'y a pas de bonne traduction de \textit{weird} en français. Même si la plupart des dictionnaires fournissent \textit{bizarre} comme traduction, on ne retrouve pas dans \textit{bizarre} l'aspect \textit{stupide} du mot \textit{weird}: les deux adjectifs ne sont pas substituables dans tous les contextes, ce qui est un problème si l'on considère que le sens d'un synset doit être conservé par la traduction.

\subsection{Accord inter-annotateurs}
\label{subsec:interannotator_agreement}

Malgré les difficultés mentionnées ci-dessus, l'annotation résultante a été validée par la mesure de l'accord inter-annotateurs, qui montre que l'approche par extension pour la création de nouveaux wordnets est valide et peut produire des ressources utiles. Deux annotateurs humains, auteurs de cet article, respectivement linguiste informaticien et informaticien linguiste, ont annoté de façon indépendante les mêmes synsets choisis au hasard pour chaque partie du discours. Ils ont utilisé WordNet pour examiner les synsets voisins, le dictionnaire Merriam-Webster, le TLFi \citep{TLFi} et des moteurs de recherche pour attester l'utilisation des divers sens des mots considérés. Après adjudication faite par ces deux annotateurs en confrontant leurs opinions en cas de désaccord, l'annotation de référence a été formée.

\begin{table}[ht]
\centering
\begin{tabular}{rccc}
  \toprule
                        & Noms    & Verbes   & Adjectifs \\
  \midrule
  Kappa de Fleiss        & 0.715   & 0.711   & 0.663    \\
 %Agreement \%          & 87.82\% & 91.75\% & 85.10\%    \\
  Synsets non-vides     & 270     & 222     & 267       \\
  Candidats par synset  & 6.22    & 14.50   & 7.27 \\
  \bottomrule
\end{tabular}
\caption{\protect\centering\label{table:kappa}Accord inter-annotateurs sur l'annotation de référence}
\end{table}

La table~\ref{table:kappa} montre l'accord inter-annotateur évalué par le kappa de Fleiss pour les trois parties du discours annotées. Même s'il s'agit d'une métrique discutée \citep{powers2012problem}, toutes les tables d'évaluation existantes considèrent ces scores comme étant suffisamment élevés pour décrire cet accord inter-annotateurs comme «~bon~» \citep{gwet2001handbook}, ce qui nous permet de dire que notre annotation de référence est de bonne qualité. L'approche par extension pour la traduction de WordNet est elle aussi validée.

\section{Résultats}
\label{sec:results}

% DONE arrondir les résultats
% DONE mettre en gras les bons trucs.
% DONE F1 partout ?

Nous présentons dans cette section les résultats de \newjaws{}. Nous commençons par décrire les résultats après l'application de l'étape des sélecteurs initiaux seulement puis ceux de la ressource complète. Notre annotation de référence est découpée en deux parties : 10\% des littéraux forment l'ensemble de développement utilisé pour choisir les sélecteurs s'appliquant aux différentes versions de \newjaws{}, tandis que les 90\% restant forment l'ensemble de test servant à l'évaluation. Précision et rappel sont calculés sur l'intersection des synsets présents dans \newjaws{} et l'annotation de référence considérée, que ce soit l'ensemble de test de notre propre adjudication (sections \ref{subsec:heuristics} à \ref{subsec:selectors}) ou WOLF (section \ref{subsec:vswolf}). Par exemple, la précision est la fraction des paires (littéral, synset) correctes au sein de l'intersection en question.

\subsection{Sélecteurs initiaux}
\label{subsec:heuristics}

Pour les noms, les verbes et les adjectifs, nous avons calculé l'efficacité de chaque sélecteur initial sur notre ensemble de développement, et utilisé ces données pour déterminer ceux qui doivent être inclus dans la version ayant une haute précision, celle ayant un F-score élevé et celle présentant une grande couverture. Les scores ci-dessous sont calculés sur l'ensemble de test, plus grand et plus représentatif.

\let\b\textbf

\begin{table}[ht]
\centering
\begin{tabular}{rcccc}
  \toprule
                    & P & R & F1 & C \\
  monosémie         & 71.5 & 76.6 & 74.0 & 54~499 \\
  unicité           & 91.7 & 63.0 & 75.3 & ~9~533 \\
  sources multiples & 64.5 & 45.0 & 53.0 & 27~316 \\
  Levenshtein       & 61.9 & 29.0 & 39.3 & 20~034 \\
  \midrule
  haute précision   & \b{93.8} & 50.1     & 65.3     & 13~867 \\
  haut F-score      & 71.1     & \b{72.7} & \b{71.9} & 82~730 \\
  haute couverture  & 69.0     & 69.8     & 69.4     & \b{90~248} \\
  \bottomrule
\end{tabular}
\caption{\protect\centering\label{table:heuristics}Sélecteurs initiaux sur l'ensemble des traductions (noms, verbes et adjectifs). La couverture C est le nombre total de paires (littéral, synset).}
\end{table}

%\begin{table}[ht]
%\centering
%\begin{tabular}{rccc|ccc|ccc}
%  \toprule
%                 & \multicolumn{3}{c}{Noms} & \multicolumn{3}{c}{Verbes} & \multicolumn{3}{c}{Adjectifs} \\
%                 & P     & R     & C         & P     & R     & C         & P     & R     & C             \\
%  monosemous     & 77.73 & 79.53 & 32068     & 50.00 & 79.80 & 6521      & 69.78 & 69.53 & 12910         \\
%  uniq           & 92.86 & 70.91 & 6681      & 73.68 & 43.75 & 1077      & 92.50 & 36.27 & 1775          \\
%  multiplesource & 68.37 & 44.67 & 12164     & 50.25 & 44.10 & 9531      & 74.64 & 45.98 & 5605          \\
%  levenshtein    & 61.93 & 31.67 & 12808     & 25.00 & 7.69  & 1033      & 77.14 & 30.51 & 3303          \\
%  \midrule
%  high-precision & \b{95.08} & 54.72     & 9775      & \b{73.68} & 43.75     & 1077      & \b{93.75} & 37.27     & 3015      \\
%  high-fscore    & 72.47     & \b{73.03} & 55092     & 52.00     & \b{75.21} & 7564      & 70.09     & \b{68.07} & 20074     \\
%  high-coverage  & 72.47     & 73.03     & \b{55092} & 48.80     & 56.01     & \b{15065} & 70.09     & 68.07     & \b{20074} \\
%  \bottomrule
%\end{tabular}
%\caption{\protect\centering\label{table:heuristics}Résultats des sélecteurs initiaux pour chaque partie du discours. C est la couverture : nombre de paires (littéral, synset).} % explain why greater than 8616
%\end{table}

La table~\ref{table:heuristics} montre les résultats de cette opération. La couverture donne une idée de la taille des ressources. En fonction des objectifs de chaque ressource, les sélecteurs initiaux choisis seront différents. Différents sélecteurs peuvent choisir plusieurs fois une même traduction, ce qui explique que la somme des couvertures soit supérieure à la couverture de la ressource à haute couverture. Fait intéressant non visible dans la table, le sélecteur le moins efficace pour les verbes est la distance de Levenshtein avec une précision de l'ordre de 25\%~: les faux amis semblent être plus nombreux pour les verbes.

%\begin{table}[ht]
%\centering
%\begin{tabular}{rccc}
%  \toprule
%                 & Nouns & Verbs & Adjectives \\
%  \midrule
%  high-precision & monosemous                          & uniq            & monosemous uniq levenshtein      \\
%  high-fscore    & +uniq +multiplesource +levenshtein  & +monosemous     & +multiplesource \\
%  high-coverage  &                                     & +multiplesource &  \\
%  \bottomrule
%\end{tabular}
%\caption{\protect\centering\label{table:heuristicsused}Heuristics used results for all parts-of-speech and resources types.}
%\end{table}

\subsection{Résultats globaux}
\label{subsec:allvsbcs}

Nous nous intéressons maintenant aux résultats globaux (Table~\ref{table:allvsbcs}). Ils comprennent l'application des sélecteurs initiaux et des sélecteurs syntaxiques. Le mode de haute précision applique également un vote (section \ref{subsec:voting}). Comme pour la table précédente, la couverture C indique le nombre de paires (littéral, synset).

\begin{table}[ht]
\centering
\begin{tabular}{rcccc|cccc}
  \toprule
                   & \multicolumn{4}{c}{Tous synsets} & \multicolumn{4}{c}{Synsets BCS}     \\
                   &   P      &    R     &   F1     &   C         &   P      &   R      &   F1     &   C    \\
  haute précision  & \b{93.3} & 51.5     & 66.4     & ~15~625     & \b{90.4} & 36.5     & 52.0     & ~1~877 \\
  haut F-score     & 68.9     & 73.0     & \b{70.9} & ~88~736     & 56.5     & 62.8     & \b{59.1} & 14~405 \\
  haute couverture & 60.5     & \b{74.3} & 66.7     & \b{109~447} & 44.5     & \b{66.9} & 53.5     & \b{23~166} \\
  \bottomrule
\end{tabular}
\caption{\protect\centering\label{table:allvsbcs}Résultats globaux : tous les synsets et synsets BCS.}
\end{table}


Dans WordNet, les mots sont majoritairement monosémiques, mais c'est une petite minorité de mots polysémiques qui est la plus représentée dans les textes. C'est justement sur cette minorité que nous souhaitons produire une ressource de qualité. Pour l'évaluer, nous utilisons la liste des synsets \textbf{BCS} (Basic Concept Set) fournie par le projet BalkaNet \citep{tufis2004balkanet}. Cette liste contient les 8~516 synsets lexicalisés dans six traductions différentes de WordNet, et représente les synsets les plus fréquents et ceux qui comportent le plus de mots polysémiques. Les résultats montrent le nombre de synsets BCS pour les ressources à haut F-score et haute couverture. Alors que les ressources à haut F-score et à haute couverture perdent en précision pour les synsets BCS, ce n'est pas le cas pour la ressource à haute précision. En effet, le mécanisme de vote rend la ressource haute-précision très robuste, et ce même pour les synsets BCS.



\subsection{Résultats par partie du discours}
\label{subsec:selectors}

%\begin{table}[ht]
%\centering
%\begin{tabular}{rccc|ccc|ccc}
%  \toprule
%                 & \multicolumn{3}{c}{Noms}  & \multicolumn{3}{c}{Verbes} & \multicolumn{3}{c}{Adjectifs} \\
%                 &     P     &     R     &    C      &     P     &     R     &    C     &     P     &     R     &    C     \\
%  $\uparrow$ Précision  & \b{95.59} & 57.02     & 11294     & 70.00     & 43.75     & 1082 & \b{91.04} & 35.88     & 3216     \\
%  $\uparrow$ F-score    & 70.63     & 72.93     & 59213     & 48.40     & 75.21     & 8169 & 69.96     & \b{69.18} & 20375    \\
%  $\uparrow$ Couverture  & 61.01     & \b{77.67} & \b{69814} & 44.76     & 58.76     & 17099& 69.96     & 69.18     & \b{20375}\\
%  \bottomrule
%\end{tabular}
%\caption{\protect\centering\label{table:nounfull}Résultats globaux}
%\end{table}

%\begin{table}[ht]
%\centering
%\begin{tabular}{rcccc}%|ccc|ccc}
%  \toprule
%                 %& \multicolumn{3}{c}{Tous synsets} \\ %& \multicolumn{3}{c}{Polysémiques} & \multicolumn{3}{c}{BCS} \\
%                   &     P     &     R     & F1        &    C      \\ %& P         &     R     &    C      &     P     &     R     & C         \\
%  haute précision  & \b{95.6} & 57.0     & 71.4     & 11~294     \\ %& \b{95.00} & 36.54     & 7133      &    -      &      -    & 1706      \\ % 3 4
%  haut F-score     & 70.6     & 72.9     & \b{71.8} & 59~213     \\ %& 62.26     & 44.15     & 30018     & \b{59.74} & 57.50     & 11382     \\ % 3 4 8
%  haute couverture & 61.0     & \b{77.7} & 68.3     & \b{69~814} \\ %& 49.36     & \b{50.83} & \b{40599} & 45.16     & \b{66.67} & \b{15265} \\ % 3 4 8 1 2 6 7
%  \bottomrule
%\end{tabular}
%\caption{\protect\centering\label{table:nounfull}Noms}
%\end{table}

%\begin{table}[ht]
%\centering
%\begin{tabular}{rcccccccccccc}%|ccc|ccc}
%  \toprule
%                   &     \multicolumn{3}{|c|}{P}    &     \multicolumn{3}{|c|}{R}    & \multicolumn{3}{|c|}{F1}       &    \multicolumn{3}{|c|}{C}           \\
%                   & N        & V        & A        & N        & V        & A        & N        & V        & A        & N          & V          & A          \\           
%  haute précision  & \b{95.6} & \b{70.0} & \b{91.0} & 57.0     & 43.8     & 35.88    & 71.4     & 53.9     & 51.47    & 11~294     & ~1~082     & ~3~216     \\ 
%  haut F-score     & 70.6     & 48.4     & 70.0     & 72.9     & \b{75.2} & \b{69.1} & \b{71.8} & \b{58.9} & \b{69.6} & 59~213     & ~8~169     & 20~375     \\ 
%  haute couverture & 61.0     & 44.8     & 70.0     & \b{77.7} & 58.8     & 69.1     & 68.3     & 50.8     & 69.6     & \b{69~814} & \b{17~099} & \b{20~375} \\ 
%  \bottomrule
%\end{tabular}
%\caption{\protect\centering\label{table:posfull}Résultats par partie du discours}
%\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{rccccc}
  \toprule
                                    &   &     P    &     R    & F1       &    C       \\ \hline
  \multirow{3}{*}{haute précision}  & noms      & \b{96.8} & 56.6     & 71.4     & 11~294     \\ 
                                    & verbes    & \b{68.4} & 41.9     & 52.0     & ~1~110     \\ 
                                    & adjectifs & \b{90.0} & 36.7    & 52.2    & ~3~221     \\ \hline
  \multirow{4}{*}{haut F-score}     & noms & 71.7     & 73.2     & \b{72.4} & 59~213     \\ 
                                    & \b{JAWS}     & 70.7     & 68.5  & 69.6 & ~55~416  \\
                                    & verbes & 48.9     & \b{76.6} & \b{59.6} & ~9~138     \\ 
                                    & adjectifs & 69.8     & 71.0 & 70.4 & 20~385     \\ \hline
  \multirow{3}{*}{haute couverture} & noms & 61.8     & \b{78.4} & 69.1     & \b{70~218} \\ 
                                    & verbes & 45.4     & 61.5     & 52.2     & \b{18~844} \\ 
                                    & adjectifs & 69.8     & \b{71.9}     & \b{70.8}     & \b{20~385} \\ 
  \bottomrule
\end{tabular}
\caption{\protect\centering\label{table:posfull}Résultats par partie du discours. JAWS ne contient que des noms : il est comparé à la ressource nominale à haut F-score.}
\end{table}

La table~\ref{table:posfull} montre les résultats détaillés pour chaque partie du discours. Concernant les noms, le mode de haute précision utilise deux sélecteurs, tous deux fondés sur la relation syntaxique de complément du nom : le sélecteur par méronymie décrit à la section \ref{subsec:translation_process}, et le sélecteur par hyponymie. La ressource de haute précision pour les noms est notre meilleure ressource. La version avec le F-score optimisé a un F-score de 72,4\%, ce qui garantit que peu de paires (littéral, synset) sont absentes tout en ayant une précision légèrement supérieure à celle de JAWS.

%\begin{table}[ht]
%\centering
%\begin{tabular}{rcccc}%|ccc|ccc}
%  \toprule
%                 %& \multicolumn{3}{c}{Tous synsets} \\ %& \multicolumn{3}{c}{Polysémiques} & \multicolumn{3}{c}{BCS} \\
%                   &     P     &     R     &    F1     & C          \\ % & P         &     R     &    C      &     P     &     R     & C     \\
%  haute précision  & \b{70.0} & 43.8     & 53.9     & ~1~082     \\ % & 70.00     & 43.75     & 1072      &   -       &   -       & 132   \\ % 2
%  haut F-score     & 48.4     & \b{75.2} & \b{58.9} & ~8~169     \\ % & 48.40     & 75.21     & 8156      & 42.59     & 74.19     & 2326  \\ % 2
%  haute couverture & 44.8     & 58.8     & 50.8     & \b{17~099} \\ % & 44.76     & 58.76     & 17086     & 41.74     & 58.54     & 6899  \\ % 2
%  \bottomrule
%\end{tabular}
%\caption{\protect\centering\label{table:verbfull}Verbes}
%\end{table}

Les résultats des verbes 
%(Table~\ref{table:verbfull}) 
sont moins élevés. L'explication principale est que les verbes sont en moyenne plus polysémiques dans WordNet et nos dictionnaires que les autres parties du discours~: les synsets verbaux ont deux fois plus de candidats que les noms et les adjectifs (Table~\ref{table:kappa}). Cela montre l'importance du dictionnaire pour limiter le nombre initial de littéraux parmi lesquels les algorithmes doivent choisir.

Le sélecteur par synonymie est le seul sélecteur syntaxique appliqué aux verbes. Il utilise les relations syntaxiques de second ordre pour trois types de dépendances syntaxiques verbales~: si deux verbes partagent les mêmes objets, ils sont susceptibles d'être synonymes ou quasi-synonymes. C'est le cas des verbes \textit{dévorer} et \textit {manger} qui acceptent tous deux l'objet \textit{pain}. Les autres sélecteurs syntaxiques n'ont pas été retenus pour les verbes en raison de leurs faibles résultats. En effet, alors que la détection de l'hyponymie en utilisant seulement l'inclusion de contextes a été efficace sur les noms, elle a les performances d'un classifieur aléatoire pour les verbes. Cela met en évidence la complexité de la polysémie des verbes.

%\begin{table}[ht]
%\centering
%\begin{tabular}{rcccc}%|ccc|ccc}
%  \toprule
%                 %& \multicolumn{3}{c}{Tous synsets} \\ %& \multicolumn{3}{c}{Polysémiques} & \multicolumn{3}{c}{BCS} \\
%                   &     P     &     R     &    F1 &   C     \\ % & P         &     R     &    C      &     P   & R & C     \\
%  haute précision  & \b{91.0} & 35.88     & 51.47     & ~3~216     \\ % & \b{90.77} & 34.71     & 3125       &   -     & - & 27    \\ % 2
%  haut F-score     & 70.0     & \b{69.1} & \b{69.6} & 20~375    \\ % &     69.00 & \b{65.63} & 19671 &   -     & - & 327   \\ % 2
%  haute couverture & 70.0     & 69.1     & 69.6     & \b{20~375}\\ % & 69.00     & 65.63     & \b{19671} &   -     & - & 327   \\ % 2
%  \bottomrule
%\end{tabular}
%\caption{\protect\centering\label{table:adjfull}Adjectifs}
%\end{table}

% Les résultats pour les adjectifs sont présentés dans la table~\ref{table:adjfull}. 
Pour les adjectifs, comme pour les verbes, seul le sélecteur de synonymie a été appliqué. Pour les ressources à haut F-score et haute couverture, ce sont les mêmes sélecteurs (initiaux et syntaxiques) qui sont appliqués, ce qui explique que les résultats sont les mêmes. Alors que l'accord inter-annotateurs était plus bas sur les adjectifs que sur les verbes, les résultats eux sont bien meilleurs pour les adjectifs. Cela s'explique principalement par le nombre de candidats parmi lesquels sélectionner : il y en a deux fois moins pour les adjectifs. Cela met en avant l'importance des dictionnaires.

\subsection{Évaluation par rapport à WOLF}
\label{subsec:vswolf}

\begin{table}[ht]
\centering
\begin{tabular}{rccc|ccc}
  \toprule
             & \multicolumn{3}{c}{WOLF 0.1.4}    & \multicolumn{3}{c}{WOLF 1.0b} \\
             &   pP      &    pR     & Ajouts    &     pP    &    pR    & Ajouts \\
  Noms       & 50.7     & 40.0     & 9~646     & 73.6     & 46.4    & 6~842  \\
  Verbes     & 33.0     & 23.9     & 1~064     & 41.7     & 17.5    & 1~084  \\
  Adjectifs  & 41.7     & 46.1     & 3~009     & 64.4     & 53.8    & 3~172  \\
  Adverbes   & 56.2     & 44.4     & 3~061     & 76.5     & 41.9    & 2~835  \\ 
  \bottomrule
\end{tabular}
\caption{\protect\centering\label{table:wolfcomparison}Évaluation de la ressource à haute précision en considérant WOLF 0.1.4 et 1.0b comme des références.}
\end{table}


Il n'est pas possible de comparer WOLF et \newjaws{} en utilisant notre annotation de référence : tout mot correct de WOLF non présent dans les dictionnaires pénalisera WOLF injustement. Nous avons décidé d'évaluer \newjaws{} en considérant WOLF 0.1.4 et WOLF 1.0b comme des références (Table~\ref{table:wolfcomparison}). Les mesures ne sont pas de véritables précision et rappel puisque WOLF lui-même n'est pas entièrement validé. Le dernier article pF donnant des chiffres globaux \citep{sagot-fiser-igwc2012} : iindique un nombre de paires autour de 77~000 pour une précision de 86\% \footnote{Les résultats détaillés pour WOLF 1.0b ne sont pas actuellement disponibles.}. Nous appelons donc pseudo-précision (pP) le pourcentage des éléments présents dans \newjaws{} qui sont également présents dans WOLF, et pseudo-rappel le pourcentage d'éléments de WOLF qui sont présents dans \newjaws{}. Ces chiffres montrent que même si \newjaws{} est encore plus petit que WOLF, il s'agit d'une ressource complémentaire, surtout quand on se souvient que le \newjaws{} utilisé pour cette comparaison est celui présentant une précision élevée, avec une précision globale de 93,3\%. Il convient également de noter que la comparaison de la différence entre WOLF 0.1.4 et \ WOLF 1.0b est instructive puisque elle montre l'étendue des améliorations apportées à WOLF.

La colonne «~Ajouts~» donne le nombre de traductions qui sont présentes dans \newjaws{} mais pas dans WOLF. Pour les noms, les verbes et les adjectifs, cela signifie que nous pouvons contribuer 11~098 nouvelles paires (littéral, synset) de haute précision en cas de fusion de WOLF et \newjaws{}, soit 94\% des paires du \newjaws{} haute précision ce qui montre la complémentarité des approches : ce sont des littéraux différents qui sont ici choisis. Cela produira un wordnet français 13\% plus grand que WOLF avec une précision améliorée. Une fusion avec la ressource de F-score élevée aurait une précision légèrement inférieure, mais fournirait 57~032 nouvelles paires (littéral, synset) par rapport à WOLF 1.0b, résultant en une fusion contenant 73~712 synsets non vides et 159~705 paires (littéral, synset), augmentant la couverture de WOLF de 56\% et celle de \newjaws{} de 83\%.

%\subsection{Hyponymy detection}
%There is no clear inclusion relationship between the distributions of hyponyms/hypernyms. As a result, research used other ways to detect hypernymy \citep{hearst1992automatic, entailment baroni}. Our works show that it is in fact possible to use ... syntactic language model ... dictionaries ... but only works on nouns so far Result of 60%...

\section*{Conclusion}

Dans ce travail, nous avons montré que l'utilisation d'un modèle de langue syntaxique pour identifier des relations lexicales entre des lexèmes est possible dans un environnement contraint et conduit à des résultats ayant une précision au niveau de l'état de l'art pour la tâche de traduction de WordNet. Nous offrons trois ressources différentes, chacune d'elles ayant un objectif différent. Enfin, nous fournissons une annotation de référence validée de haute qualité qui nous a permis de montrer à la fois la validité de l'approche de traduction de WordNet par extension et la validité de notre approche spécifique. Cette annotation de référence peut également être utilisée pour évaluer et développer d'autres traductions françaises de WordNet. \newjaws{} est disponible librement au format XML DEBVisDic\footnote{\url{http://nlp.fi.muni.cz/trac/deb2/wiki/WordNetFormat}} sur \url{http://wonef.fr/} sous la licence CC-BY-SA.

Les travaux futurs sur \newjaws{} mettront l'accent sur les verbes, les adjectifs et les adverbes, pour lesquels de nouveaux sélecteurs efficaces peuvent être envisagés pour améliorer la couverture. Par exemple, le sélecteur de similarité peut être étendu à la relation de quasi-synonymité que partagent certains adjectifs dans WordNet. En effet, la synonymie entre les adjectifs est limitée par rapport à la quasi-synonymie: alors que \textit{fast} est le seul mot dans son synset, c'est le quasi-synonyme de 20 synsets. Puisque les techniques de sémantique distributionnelle ont plutôt tendance à identifier des quasi-synonymes plutôt que des synonymes, utiliser cette relation de WordNet pour identifier de nouveaux adjectifs fait partie de nos objectifs.

Une autre source importante d'amélioration sera l'enrichissement de notre modèle de langue syntaxique qui pourra prendre en compte les verbes pronominaux et les expressions multi-mots. Nous aimerions aussi nous orienter vers un modèle de langue continu \citep{haison2012continuous} plus performant. Cela sera couplé avec la collecte d'un corpus issu du Web plus récent et plus grand analysé avec une version récente de notre analyseur linguistique LIMA. Cela nous permettra de mesurer l'impact de la qualité du modèle de langue sur la traduction de WordNet.

Le wordnet français WOLF a été construit en utilisant plusieurs techniques. La fusion de WOLF et de \newjaws{} permettra de bientôt améliorer à nouveau le statut de la traduction française de WordNet: nous travaillons avec les auteurs de WOLF afin de fusionner WOLF et \newjaws{}.

