% vim: set spelllang=fr:

\setchapterpreamble[ur][.7\textwidth]{%
  \dictum[Robin Hobb, \textit{La Voie magique}]{%
  « S'il vous plaît ! fis-je. Taisez-vous, s'il vous plaît ! » Je désirais seulement qu'ils fassent silence, qu'ils cessent les bruits qu'ils faisaient avec leur bouche, mais le son de mes propres paroles capta mon attention. « S'il-vous-plaît, répétai-je, en m'étonnant de tous les mouvements que devait effectuer ma propre bouche pour produire ces sons imprécis. « Taisez-vous ! » Je m'aperçus que ces deux mots avaient trop de sens pour en avoir un véritable.}}

\chapter{WoNeF : une traduction de WordNet}
\label{ch:wonef} 

Identifier les sens possibles des mots du vocabulaire est un problème difficile
demandant un travail manuel très conséquent (section~\ref{subsec:sens_mots}).
Ce travail est cependant nécessaire pour pour pouvoir identifier le sens
correct d'un mot utilisé en contexte. WordNet \citep{fellbaum1998wordnet}
regroupe les mots en ensembles de synonymes (\emph{synonym sets} ou
\emph{synsets}) et lie ces synsets entre eux par différentes relations
sémantiques comme par exemple l'antonymie, l'hyperonymie, l'hyponymie ou encore
la méronymie (section~\ref{subsec:ressources_lexicales}).

Nos travaux \citep{pradet2013wonef,pradet2014wonef} sont la suite de JAWS, dont
le fonctionnement est détaillé à la
section~\ref{subsec:jaws_translation_process}. Cette nouvelle version de JAWS
se nomme WoNeF : le nouveau nom évite la confusion avec une API Java pour
WordNet (\emph{Java API for WordNet Searching}). Dans la suite de ce travail,
le terme \emph{JAWS} ne fera pas référence à l'API mais à la version précédente
de WoNeF \citep{mouton2010jaws,mouton2010phd}.

Nous étendons et améliorons les techniques utilisées dans JAWS et l'évaluons à
l'aide d'une vérité-terrain obtenue par adjudication de deux annotateurs. WoNeF
se décline en trois versions pour répondre à différents besoins. Le WoNeF
principal a un F-score de 70.9~\%, une autre version a une précision de
93.3~\%, et une dernière contient 109~447 paires (littéral, synset).

\section{WoNeF: un JAWS étendu aux verbes, adjectifs et adverbes}
\label{sec:improving_jaws}

\subsection{Limites de JAWS}
\label{subsec:limitations}

JAWS souffre d'un certain nombre de limites. Avant tout, il ne contient que des
noms alors que WordNet contient des noms, verbes, adjectifs et adverbes.

Ensuite, la façon dont il a été évalué rend difficile tout jugement sur sa
qualité. En effet, JAWS a été évalué en le comparant à l'EuroWordNet du
français et à WOLF 0.1.4 (qui date de 2008). Ces deux WordNets du français ne
sont pas des annotations de références : ils souffrent soit d'une précision
limitée soit d'une couverture limitée. Cette évaluation limitée a été complétée
par une évaluation manuelle des littéraux n'existant pas dans WOLF, mais elle
n'a été faite que sur 120 paires (littéral, synset). La précision de JAWS est
évaluée à 67,1~\% \citep{mouton2010phd}, ce qui est plus bas que celle de WOLF
0.1.4 et considérablement plus bas que la précision de WOLF 1.0b\footnote{Nous
remercions Benoît Sagot pour nous avoir fourni cette version préliminaire de
WOLF 1.0.}. Ce score est à prendre avec précaution étant donné la taille de
l'échantillon de test : l'intervalle de confiance est d'environ 25~\%. Une
autre limite de JAWS est qu'il ne contient qu'une seule et unique ressource qui
ne correspond pas à tous les besoins.

% Il n'y a pas besoin de donner tous ces détails, on peut juste dire que pour
% nous c'est 70.7 / 69.6 basé sur la section susbec:pos_results

À notre connaissance, les traductions automatiques de WordNet actuelles
n'existent qu'en une seule version, soit où les auteurs décident eux-mêmes
quelle métrique optimiser, soit où il faut choisir un seuil de confiance pour
ne garder que les traductions les plus sûres. Nous fournissons aussi une telle
version, mais ajoutons aussi deux ressources qui peuvent servir des besoins
différents. Même si notre WoNeF à haute précision est petit, il peut être
utilisé comme une annotation de référence et servir pour entraîner un système
d'apprentissage. Une ressource à haute couverture peut servir de base à une
correction manuelle ou servir pour une intersection avec d'autres ressources,
ce qui est la raison pour laquelle nous en fournissons une aussi.


Cette section présente les trois améliorations essentielles qui ont étés
apportées à JAWS. Un changement non détaillé ici est celui qui a mené à une
meilleure rapidité d'exécution : JAWS se construit en plusieurs heures contre
moins d'une minute pour WoNeF, ce qui a facilité les expérimentations.

\subsection{Sélecteurs initiaux}
\label{subsec:revisiting_extraction_heuristics}

% (En fait on ne l'a pas fait) JAWS extraction heuristics were suboptimal. To
% overcome this problem, we first allowed multiword expressions (MWE) as
% translation candidates. Even if the syntactic language model does not handle
% multiword expressions, heuristics do not share this limitation. 97~\% of
% nouns MWE, 75~\% of verbs MWE and 95~\% of adjectives MWE are monosemous: we
% expect heuristics to produce good results on multiword expressions.

Les sélecteurs initiaux de JAWS ne sont pas optimaux. Alors que les sélecteurs
par monosémie et par unicité sont conservés, nous avons changé les autres
sélecteurs. Premièrement, le sélecteur des transfuges est supprimé : sa
précision était très basse, même pour les noms.

Deuxièmement, un nouveau sélecteur considère les traductions candidates
provenant de plusieurs mots anglais différents dans un synset donné : c'est le
sélecteur par sources multiples. Par exemple, dans le synset \textit{line,
railway line, rail line (the road consisting of railroad track and roadbed)},
les littéraux français \textit{ligne de chemin de fer} et \textit{voie} sont
des traductions à la fois de \textit{line} et \textit{railway line}, et sont
donc choisis comme traductions.

Troisièmement, le sélecteur de la distance de Levenshtein a été amélioré. 28~\%
du vocabulaire anglais est d'origine française \citep{finkenstaedt1973ordered},
et l'anglicisation a produit des transformations prévisibles. Il est possible
d'appliquer ces mêmes transformations aux littéraux candidats français, et
seulement alors d'appliquer la distance de Levenshtein. Nous commençons par
supprimer les accents, puis appliquons différentes opérations. Par exemple,
l'inversion des lettres "r" et "e" prend en compte
(\textit{order}/\textit{ordre}) et (\textit{tiger}/\textit{tigre})\footnote{La
distance de Damerau-Levenshtein qui prend en compte les inversions n'importe-où
dans un mot \citep{damerau1964technique} a donné de moins bons résultats.}.
Toutes les transformations ne s'appliquent qu'à la fin des mots : \textit{-que}
est transformé en \textit{-k} ou \textit{-c} (\textit{marque} devient
\textit{mark}), \textit{-té} vers \textit{-ty} (\textit{extremité} devient
\textit{extremity}), etc. Les faux-amis ne sont toujours pas explicitement pris
en compte.

\begin{table}[ht]
\centering
\begin{tabular}{rllcl}
  \toprule
-que & -k & banque &$\to$& bank \\
-aire & -ary & tertiaire &$\to$& tertiary \\
-eur & -or & vapeur &$\to$& vapor \\
-ie & -y & cajolerie &$\to$& cajolery \\
-té & -ty & extremité &$\to$& extremity \\
-re & -er & tigre &$\to$& tiger \\
-ais & -ese & libanais &$\to$& libanese \\
-ant & -ing & changeant &$\to$& changing \\
  \bottomrule
\end{tabular}

\caption{\label{table:levenshteinrules}Règles appliquées à la fin des mots
français avant d'utiliser le sélecteur de la distance de Levenshtein.}

\end{table}


\subsection{Apprentissage de seuils}
\label{subsec:learning_thresholds}

Dans JAWS, chaque littéral anglais ne peut avoir qu'une traduction française
correspondante. La traduction choisie est celle qui a le meilleur score,
indépendamment des scores des traductions moins bien notées. Cela a pour effet
de rejeter des candidats valides et d'accepter des candidats erronés. Par
exemple, JAWS n'inclut pas \textit{particulier} au synset \textit{(a human
being) ``there was too much for one person to do''} parce que \textit{personne}
est déjà inclus avec un score supérieur.

Dans WoNeF, nous avons donc appris un seuil pour chaque partie du discours et
sélecteur. Nous avons d'abord généré les scores pour toutes les paires
(littéral, synset) candidates, puis trié ces paires par score. Les 12 399
paires présentes dans l'évaluation manuelle associée à WOLF 1.0b (notre
ensemble d'apprentissage) ont été jugées correctes tandis que les paires n'y
étant pas ont été jugées erronées. Nous avons ensuite calculé les seuils
maximisant la précision et le F-score. Le seuil qui maximise le F-score est
utilisé dans les ressources à haut F-score et à haute couverture, tandis que le
seuil maximisant la précision est utilisé dans la ressource à haute précision.

Une fois que ces seuils sont définis, les sélecteurs choisissent tous les
candidats au-dessus du nouveau seuil, ce qui a deux effets positifs :

\begin{itemize}

    \item des candidats valides ne sont plus rejetés simplement parce qu'un
        meilleur candidat est aussi sélectionné, ce qui améliore à la fois le
        rappel et la couverture.

    \item les candidats invalides qui étaient jusque-là acceptés sont
        maintenant rejetés grâce au seuil plus strict : la précision s'en
        retrouve augmentée.

\end{itemize}

\subsection{Vote}
\label{subsec:voting}

Après l'application des différents sélecteurs, notre WordNet est large mais
contient des synsets bruités. Comme toutes les traductions automatiques de
WordNet, WoNeF doit alors être nettoyé \citep{sagot2012cleaning}. Dans WoNeF,
le bruit provient de différents facteurs :

\begin{itemize}

    \item les sélecteurs essaient d'inférer des informations sémantiques à
        partir d'une analyse syntaxique sans prendre en compte toute la
        complexité de l'interface syntaxe-sémantique,

    \item l'analyseur syntaxique produit lui-même des résultats bruités,

    \item le modèle de langue syntaxique est produit à partir d'un corpus
        extrait du web lui-même bruité (texte mal écrit, contenu non textuel,
        phrases non françaises) et n'est pas une «~distribution idéale~»
        \citep{copestake2013lexicalised},

    \item les traductions déjà choisies sont considérées comme valides dans les
        étapes suivantes alors que ce n'est pas toujours le cas.

\end{itemize}

Pour la ressource haute-précision, il fallait donc un moyen de ne garder que
les littéraux pour lesquels les sélecteurs étaient les plus confiants. Étant
donné que, contrairement à JAWS, plusieurs sélecteurs peuvent choisir une même
traduction (sous-section \ref{subsec:learning_thresholds}), notre solution est
simple et efficace : les traductions validées par un bon sélecteur ou par
plusieurs sélecteurs moyens sont conservées tandis que les autres sont
supprimées. Ce principe de vote est aussi appelé méthode d'ensemble en
apprentissage automatique. Les sélecteurs performants varient d'une partie du
discours à une autre : le choix est fait sur un ensemble de développement
contenant 10~\% de notre référence.

Cette opération de nettoyage ne conserve que 18~\% des traductions (de 87~757
paires (littéral, synset) à 15~625) mais la précision grimpe de 68,4~\% à
93,3~\%. Cette ressource à haute précision peut être utilisée comme donnée
d'entraînement. Un défaut classique des méthodes de vote est de ne choisir que
des exemples faciles et peu intéressants, mais la ressource obtenue ici est
équilibrée entre les synsets ne contenant que des mots monosémiques et d'autres
synsets contenant des mots polysémiques et plus difficiles à désambiguïser
(section \ref{subsec:allvsbcs}).

\subsection{Extension aux verbes, adjectifs et adverbes}
\label{sec:extending_jaws}

Les travaux sur JAWS ont commencé par les noms parce qu'ils représentent 70~\%
des synsets dans WordNet. Nous avons continué ce travail sur les autres parties
du discours qui sont aussi importantes pour examiner le sens d'un texte donné :
verbes, adjectifs et adverbes. Les sélecteurs génériques ont ici été modifiés,
mais il s'agira dans le futur d'implémenter des sélecteurs prenant en compte
les spécificités des différentes parties du discours dans WordNet.

\paragraph{Verbes} Les sélecteurs choisis pour les verbes sont le sélecteur par
unicité et par monosémie. En effet, la distance de Levenshtein a donné des
résultats médiocres pour les verbes : seuls 25~\% des verbes choisis par ce
sélecteur étaient des traductions correctes. Concernant les sélecteurs
syntaxiques, seul le sélecteur par synonymie a donné de bons résultats, alors
que le sélecteur par hyponymie avait les performances d'un classifieur
aléatoire.

\paragraph{Adjectifs} Les adjectifs sont traduits de la même manière que les
noms : tout d'abord un nombre limité de sélecteurs initiaux remplit un WordNet
vide, puis les sélecteurs syntaxiques complètent cette traduction avec le
modèle de langue syntaxique. Tous les sélecteurs initiaux sont ici choisis, et
le sélecteur syntaxique choisi est le sélecteur par synonymie. Ils ont donné de
bons résultats qui sont présentés dans la section \ref{subsec:pos_results}.

\paragraph{Adverbes} Les adverbes sont traduits avec les quatre sélecteurs :
par monosémie, par unicité, par sources multiples et enfin par la distance de
Levenshtein. Dans le cas des adverbes, nous n'avons pas trouvé de cas où la
granularité des synsets de WordNet n'était pas applicable en français, comme
c'était le cas pour les autres parties du discours. L'accord inter-annotateur
était de 0.57, ce qui peut s'expliquer par le fait qu'un des deux annotateurs
n'avait jamais effectué cette tâche, alors que l'autre avait déjà annoté les
autres parties du discours. Les adverbes obtenus avec notre approche sont de
bonne qualité tout en étant complémentaires avec WOLF : 87~\% des adverbes
proposés ne sont pas dans WOLF. Une fusion entre WoNeF et WOLF aurait trois
fois plus d'adverbes que WOLF seul.

\section{WoNeF: un JAWS évalué}
\label{sec:evaluating_jaws}

\subsection{Développement d'une annotation de référence}
\label{subsec:gold_standard}

L'évaluation de JAWS souffre d'un certain nombre de limites (section
\ref{subsec:limitations}). Pour évaluer rigoureusement notre propre traduction
de WordNet, nous avons produit une annotation de référence. Pour chaque partie
du discours, 300 synsets ont été annotés par deux annotateurs locuteurs natifs
du français. Pour chaque traduction candidate fournie par nos dictionnaires, il
fallait décider si elle appartenait au synset. Puisque les dictionnaires ne
proposent pas de candidats pour tous les synsets et que certains synsets n'ont
pas de candidat valable, le nombre réel de synsets non vides est inférieur à
300 (section \ref{subsec:interannotator_agreement}).

Durant l'annotation manuelle, nous avons rencontré une difficulté importante
découlant de la tentative de traduire WordNet dans une autre langue. Dans le
cas de l'anglais vers le français, la plupart des difficultés proviennent des
verbes et adjectifs figurant dans une collocation. Dans WordNet, ils peuvent
être regroupés d'une manière qui fait sens en anglais, mais qui ne se retrouve
pas directement dans une autre langue. Par exemple, l'adjectif \textit{pointed}
est le seul élément d'un synset défini comme \textit{direct and obvious in
meaning or reference; often unpleasant; ``a pointed critique''; ``a pointed
allusion to what was going on''; ``another pointed look in their direction''}.
Ces exemples se traduiraient par trois adjectifs différents en français :
\textit{une critique dure}, \textit{une allusion claire} et \textit{un regard
appuyé}. Il n'existe pas de solution satisfaisante lors de la traduction d'un
tel synset : le synset résultant contiendra soit trop soit trop peu de
traductions. Nous avons décidé de ne pas traduire ces synsets dans notre
annotation manuelle. Ces problèmes de granularité concernent 3~\% des synsets
nominaux, 8~\% des synsets verbaux et 6~\% des synsets adjectivaux.
Actuellement, WoNeF ne détecte pas de tels synsets.

L'autre difficulté principale découle de traductions manquantes, ce qui peut
être considéré comme un défaut de nos dictionnaires. Les sens rares d'un mot
sont parfois absents. Par exemple, le sens \textit{to catch} du jeu du chat (ou
du loup) et le sens \textit{coat with beaten egg} du verbe \textit{to egg} ne
sont pas présents. Aucun de ces sens n'est dans les synsets les plus
polysémiques (définis à la section~\ref{subsec:allvsbcs}), ce qui confirme que
cela ne se produit que pour les sens rares. Pourtant, WoNeF pourrait être
amélioré en utilisant des dictionnaires spécifiques pour, par exemple, les
espèces (comme dans \cite{sagot2008construction}), les termes médicaux, les
entités nommées (en utilisant Wikipedia) et ainsi de suite. Un autre exemple
est celui des adjectifs de jugement : il n'y a pas de bonne traduction de
\textit{weird} en français. Même si la plupart des dictionnaires fournissent
\textit{bizarre} comme traduction, on ne retrouve pas dans \textit{bizarre}
l'aspect \textit{stupide} du mot \textit{weird}: les deux adjectifs ne sont pas
substituables dans tous les contextes, ce qui est un problème si l'on considère
que le sens d'un synset doit être conservé par la traduction.

\subsection{Accord inter-annotateurs}
\label{subsec:interannotator_agreement}

Malgré les difficultés mentionnées ci-dessus, l'annotation résultante a été
validée par la mesure de l'accord inter-annotateurs, qui montre que l'approche
par extension pour la création de nouveaux wordnets est valide et peut produire
des ressources utiles. Trois annotateurs humains, soit linguiste informaticien
soit informaticien linguiste, ont annoté de façon indépendante les mêmes
synsets choisis au hasard pour chaque partie du discours. Ils ont utilisé
WordNet pour examiner les synsets voisins, le dictionnaire Merriam-Webster, le
TLFi \citep{TLFi} et des moteurs de recherche pour attester l'utilisation des
divers sens des mots considérés. Après adjudication faite par ces deux
annotateurs en confrontant leurs opinions en cas de désaccord, l'annotation de
référence a été formée.

\begin{table}[ht]
\centering
\begin{tabular}{rcccc}
  \toprule
                        & Noms    & Verbes  & Adjectifs & Adverbes\\
  \midrule
  Kappa de Fleiss       & 0.72    & 0.71    & 0.66      & 0.57 \\
  Synsets non-vides     & 270     & 222     & 267       & TODO \\
  Candidats par synset  & 6.22    & 14.50   & 7.27      & TODO \\
  \bottomrule
\end{tabular}
\caption{\label{table:kappa}Accord inter-annotateurs sur l'annotation de référence}
\end{table}

La table~\ref{table:kappa} montre l'accord inter-annotateur évalué par le kappa
de Fleiss pour les trois parties du discours annotées. Même s'il s'agit d'une
métrique discutée \citep{powers2012problem}, toutes les tables d'évaluation
existantes considèrent ces scores comme étant suffisamment élevés pour décrire
cet accord inter-annotateurs comme «~bon~» \citep{gwet2001handbook}, ce qui
nous permet de dire que notre annotation de référence est de bonne qualité.
L'approche par extension pour la traduction de WordNet est elle aussi validée.

\section{Résultats}
\label{sec:results}

Nous présentons dans cette section les résultats de WoNeF. Nous commençons par
décrire les résultats après l'application de l'étape des sélecteurs initiaux
seulement puis ceux de la ressource complète. Notre annotation de référence est
découpée en deux parties : 10~\% des littéraux forment l'ensemble de
développement utilisé pour choisir les sélecteurs s'appliquant aux différentes
versions de WoNeF, tandis que les 90~\% restant forment l'ensemble de test
servant à l'évaluation. Précision et rappel sont calculés sur l'intersection
des synsets présents dans WoNeF et l'annotation de référence considérée, que ce
soit l'ensemble de test de notre propre adjudication (sections
\ref{subsec:heuristics} à \ref{subsec:pos_results}) ou WOLF (section
\ref{subsec:vswolf}). Par exemple, la précision est la fraction des paires
(littéral, synset) correctes au sein de l'intersection en question.

\subsection{Sélecteurs initiaux}
\label{subsec:heuristics}

Pour les noms, les verbes et les adjectifs, nous avons calculé l'efficacité de
chaque sélecteur initial sur notre ensemble de développement, et utilisé ces
données pour déterminer ceux qui doivent être inclus dans la version ayant une
haute précision, celle ayant un F-score élevé et celle présentant une grande
couverture. Les scores ci-dessous sont calculés sur l'ensemble de test, plus
grand et plus représentatif.

\let\b\textbf

\begin{table}[ht]
\centering
\begin{tabular}{rcccc}
  \toprule
                    & P & R & F1 & C \\
  monosémie         & 71.5 & 76.6 & 74.0 & 54~499 \\
  unicité           & 91.7 & 63.0 & 75.3 & ~9~533 \\
  sources multiples & 64.5 & 45.0 & 53.0 & 27~316 \\
  Levenshtein       & 61.9 & 29.0 & 39.3 & 20~034 \\
  \midrule
  haute précision   & \b{93.8} & 50.1     & 65.3     & 13~867 \\
  haut F-score      & 71.1     & \b{72.7} & \b{71.9} & 82~730 \\
  haute couverture  & 69.0     & 69.8     & 69.4     & \b{90~248} \\
  \bottomrule
\end{tabular}
\caption{\label{table:heuristics}Sélecteurs initiaux sur l'ensemble des traductions (noms, verbes et adjectifs). La couverture C est le nombre total de paires (littéral, synset).}
\end{table}

La table~\ref{table:heuristics} montre les résultats de cette opération. La
couverture donne une idée de la taille des ressources. En fonction des
objectifs de chaque ressource, les sélecteurs initiaux choisis seront
différents. Différents sélecteurs peuvent choisir plusieurs fois une même
traduction, ce qui explique que la somme des couvertures soit supérieure à la
couverture de la ressource à haute couverture. Fait intéressant non visible
dans la table, le sélecteur le moins efficace pour les verbes est la distance
de Levenshtein avec une précision de l'ordre de 25~\%~: les faux amis semblent
être plus nombreux pour les verbes.

%\begin{table}[ht]
%\centering
%\begin{tabular}{rccc}
%  \toprule
%                 & Nouns & Verbs & Adjectives \\
%  \midrule
%  high-precision & monosemous                          & uniq            & monosemous uniq levenshtein      \\
%  high-fscore    & +uniq +multiplesource +levenshtein  & +monosemous     & +multiplesource \\
%  high-coverage  &                                     & +multiplesource &  \\
%  \bottomrule
%\end{tabular}
%\caption{\protect\centering\label{table:heuristicsused}Heuristics used results for all parts-of-speech and resources types.}
%\end{table}

\subsection{Résultats globaux}
\label{subsec:allvsbcs}

Nous nous intéressons maintenant aux résultats globaux
(Table~\ref{table:allvsbcs}). Ils comprennent l'application des sélecteurs
initiaux et des sélecteurs syntaxiques. Le mode de haute précision applique
également un vote (section \ref{subsec:voting}). Comme pour la table
précédente, la couverture C indique le nombre de paires (littéral, synset).

\begin{table}[ht]
\centering
\begin{tabular}{rcccc|cccc}
  \toprule
                   & \multicolumn{4}{c}{Tous synsets} & \multicolumn{4}{c}{Synsets BCS}     \\
                   &   P      &    R     &   F1     &   C         &   P      &   R      &   F1     &   C    \\
  haute précision  & \b{93.3} & 51.5     & 66.4     & ~15~625     & \b{90.4} & 36.5     & 52.0     & ~1~877 \\
  haut F-score     & 68.9     & 73.0     & \b{70.9} & ~88~736     & 56.5     & 62.8     & \b{59.1} & 14~405 \\
  haute couverture & 60.5     & \b{74.3} & 66.7     & \b{109~447} & 44.5     & \b{66.9} & 53.5     & \b{23~166} \\
  \bottomrule
\end{tabular}
\caption{\label{table:allvsbcs}Résultats globaux : tous les synsets et synsets BCS.}
\end{table}


Dans WordNet, les mots sont majoritairement monosémiques, mais c'est une petite
minorité de mots polysémiques qui est la plus représentée dans les textes.
C'est justement sur cette minorité que nous souhaitons produire une ressource
de qualité. Pour l'évaluer, nous utilisons la liste des synsets \textbf{BCS}
(Basic Concept Set) fournie par le projet BalkaNet \citep{tufis2004balkanet}.
Cette liste contient les 8~516 synsets lexicalisés dans six traductions
différentes de WordNet, et représente les synsets les plus fréquents et ceux
qui comportent le plus de mots polysémiques. Les résultats montrent le nombre
de synsets BCS pour les ressources à haut F-score et haute couverture. Alors
que les ressources à haut F-score et à haute couverture perdent en précision
pour les synsets BCS, ce n'est pas le cas pour la ressource à haute précision.
En effet, le mécanisme de vote rend la ressource haute-précision très robuste,
et ce même pour les synsets BCS.


\subsection{Résultats par partie du discours}
\label{subsec:pos_results}

\begin{table}[ht]
\centering
\begin{tabular}{rccccr}
  \toprule
                                    &           &     P    &     R    & F1       &    C       \\ \hline
  \multirow{3}{*}{haute précision}  & noms      & \b{96.8} & 56.6     & 71.4     & 11~294     \\ 
                                    & verbes    & \b{68.4} & 41.9     & 52.0     &  1~110     \\ 
                                    & adjectifs & \b{90.0} & 36.7     & 52.2     &  3~221     \\
                                    & adverbes  & \b{92.2} & 32.4     & 48.8     &    968     \\ \hline
  \multirow{4}{*}{haut F-score}     & noms      & 71.7     & 73.2     & \b{72.4} & 59~213     \\ 
                                    & \b{JAWS}  & 70.7     & 68.5     & 69.6     &  55~416    \\
                                    & verbes    & 48.9     & \b{76.6} & \b{59.6} &  9~138     \\ 
                                    & adjectifs & 69.8     & 71.9     & \b{70.8} & 20~385     \\
                                    & adverbes  & 82.1     & 74.6     & \b{78.2} &  5~950     \\ \hline
  \multirow{3}{*}{haute couverture} & noms      & 61.8     & \b{78.4} & 69.1     & \b{70~218} \\ 
                                    & verbes    & 45.4     & 61.5     & 52.2     & \b{18~844} \\ 
                                    & adjectifs & 69.8     & \b{71.9} & 70.8     & \b{20~385} \\ 
                                    & adverbes  & 82.1     & \b{74.6} & 78.2     & \b{5~950}  \\
  \bottomrule
\end{tabular}

\caption{\label{table:posfull}Résultats par partie du discours. JAWS ne
    contient que des noms : il est comparé à la ressource nominale à haut
    F-score. Les scores sont les mêmes pour les adjectifs et les adverbes pour
    les ressources à haut F-score et à haute couverture : les mêmes sélecteurs
ont étés appliqués.}

\end{table}

La table~\ref{table:posfull} montre les résultats détaillés pour chaque partie
du discours. Concernant les noms, le mode de haute précision utilise deux
sélecteurs, tous deux fondés sur la relation syntaxique de complément du nom :
le sélecteur par méronymie décrit à la section
\ref{subsec:jaws_translation_process}, et le sélecteur par hyponymie. La
ressource de haute précision pour les noms est notre meilleure ressource. La
version avec le F-score optimisé a un F-score de 72,4~\%, ce qui garantit que
peu de paires (littéral, synset) sont absentes tout en ayant une précision
légèrement supérieure à celle de JAWS.

Les résultats des verbes sont moins élevés. L'explication principale est que
les verbes sont en moyenne plus polysémiques dans WordNet et nos dictionnaires
que les autres parties du discours~: les synsets verbaux ont deux fois plus de
candidats que les noms et les adjectifs (Table~\ref{table:kappa}). Cela montre
l'importance du dictionnaire pour limiter le nombre initial de littéraux parmi
lesquels les algorithmes doivent choisir.

Le sélecteur par synonymie est le seul sélecteur syntaxique appliqué aux
verbes. Il utilise les relations syntaxiques de second ordre pour trois types
de dépendances syntaxiques verbales~: si deux verbes partagent les mêmes
objets, ils sont susceptibles d'être synonymes ou quasi-synonymes. C'est le cas
des verbes \textit{dévorer} et \textit {manger} qui acceptent tous deux l'objet
\textit{pain}. Les autres sélecteurs syntaxiques n'ont pas été retenus pour les
verbes en raison de leurs faibles résultats. En effet, alors que la détection
de l'hyponymie en utilisant seulement l'inclusion de contextes a été efficace
sur les noms, elle a les performances d'un classifieur aléatoire pour les
verbes. Cela met en évidence la complexité de la polysémie des verbes.

% Les résultats pour les adjectifs sont présentés dans la
% table~\ref{table:adjfull}. 
Pour les adjectifs, comme pour les verbes, seul le sélecteur de synonymie a été
appliqué. Pour les ressources à haut F-score et haute couverture, ce sont les
mêmes sélecteurs (initiaux et syntaxiques) qui sont appliqués, ce qui explique
que les résultats sont les mêmes. Alors que l'accord inter-annotateurs était
plus bas sur les adjectifs que sur les verbes, les résultats eux sont bien
meilleurs pour les adjectifs. Cela s'explique principalement par le nombre de
candidats parmi lesquels sélectionner : il y en a deux fois moins pour les
adjectifs. Cela met en avant l'importance des dictionnaires.

\subsection{Évaluation par rapport à WOLF}
\label{subsec:vswolf}

\begin{table}[ht]
\centering
\begin{tabular}{rccc|ccc}
  \toprule
             & \multicolumn{3}{c}{WOLF 0.1.4}    & \multicolumn{3}{c}{WOLF 1.0b} \\
             &   pP      &    pR     & Ajouts    &     pP    &    pR    & Ajouts \\
  Noms       & 50.7     & 40.0     & 9~646     & 73.6     & 46.4    & 6~842  \\
  Verbes     & 33.0     & 23.9     & 1~064     & 41.7     & 17.5    & 1~084  \\
  Adjectifs  & 41.7     & 46.1     & 3~009     & 64.4     & 53.8    & 3~172  \\
  Adverbes   & 56.2     & 44.4     & 3~061     & 76.5     & 41.9    & 2~835  \\ 
  \bottomrule
\end{tabular}
\caption{\label{table:wolfcomparison}Évaluation de la ressource à haute précision en considérant WOLF 0.1.4 et 1.0b comme des références.}
\end{table}


Il n'est pas possible de comparer WOLF et WoNeF en utilisant notre annotation
de référence : tout mot correct de WOLF non présent dans les dictionnaires
pénalisera WOLF injustement. Nous avons décidé d'évaluer WoNeF en considérant
WOLF 0.1.4 et WOLF 1.0b comme des références
(Table~\ref{table:wolfcomparison}). Les mesures ne sont pas de véritables
précision et rappel puisque WOLF lui-même n'est pas entièrement validé. Le
dernier article donnant des chiffres globaux \citep{sagot2012automatic} indique
un nombre de paires autour de 77~000 pour une précision de 86~\% \footnote{Les
résultats détaillés pour WOLF 1.0b ne sont pas actuellement disponibles.}. Nous
appelons donc pseudo-précision (pP) le pourcentage des éléments présents dans
WoNeF qui sont également présents dans WOLF, et pseudo-rappel le pourcentage
d'éléments de WOLF qui sont présents dans WoNeF.  Ces chiffres montrent que
même si WoNeF est encore plus petit que WOLF, il s'agit d'une ressource
complémentaire, surtout quand on se souvient que le WoNeF utilisé pour cette
comparaison est celui présentant une précision élevée, avec une précision
globale de 93,3~\%. Il convient également de noter que la comparaison de la
différence entre WOLF 0.1.4 et \ WOLF 1.0b est instructive puisque elle montre
l'étendue des améliorations apportées à WOLF.

La colonne «~Ajouts~» donne le nombre de traductions qui sont présentes dans
WoNeF mais pas dans WOLF. Pour les noms, les verbes et les adjectifs, cela
signifie que nous pouvons contribuer 11~098 nouvelles paires (littéral, synset)
de haute précision en cas de fusion de WOLF et WoNeF, soit 94~\% des paires du
WoNeF haute précision ce qui montre la complémentarité des approches : ce sont
des littéraux différents qui sont ici choisis. Cela produira un wordnet
français 13~\% plus grand que WOLF avec une précision améliorée. Une fusion
avec la ressource de F-score élevée aurait une précision légèrement inférieure,
mais fournirait 57~032 nouvelles paires (littéral, synset) par rapport à WOLF
1.0b, résultant en une fusion contenant 73~712 synsets non vides et 159~705
paires (littéral, synset), augmentant la couverture de WOLF de 56~\% et celle
de WoNeF de 83~\%.

%\subsection{Hyponymy detection}
%There is no clear inclusion relationship between the distributions of hyponyms/hypernyms. As a result, research used other ways to detect hypernymy \citep{hearst1992automatic, entailment baroni}. Our works show that it is in fact possible to use ... syntactic language model ... dictionaries ... but only works on nouns so far Result of 60%...

\section*{Conclusion}

Dans ce chapitre, nous avons montré que l'utilisation d'un modèle de langue
syntaxique pour identifier des relations lexicales entre des lexèmes est
possible dans un environnement contraint et conduit à des résultats ayant une
précision au niveau de l'état de l'art pour la tâche de traduction de WordNet.
Nous offrons trois ressources différentes, chacune d'elles ayant un objectif
différent. Enfin, nous fournissons une annotation de référence validée de haute
qualité qui nous a permis de montrer à la fois la validité de l'approche de
traduction de WordNet par extension et la validité de notre approche
spécifique. Cette annotation de référence peut également être utilisée pour
évaluer et développer d'autres traductions françaises de WordNet. WoNeF est
disponible librement au format XML
DEBVisDic\footnote{\url{http://nlp.fi.muni.cz/trac/deb2/wiki/WordNetFormat}}
sur \url{http://wonef.fr/} sous la licence CC-BY-SA.

Les travaux futurs sur WoNeF mettront l'accent sur les verbes, les adjectifs et
les adverbes, pour lesquels de nouveaux sélecteurs efficaces peuvent être
envisagés pour améliorer la couverture. Par exemple, le sélecteur de similarité
peut être étendu à la relation de quasi-synonymité que partagent certains
adjectifs dans WordNet. En effet, la synonymie entre les adjectifs est limitée
par rapport à la quasi-synonymie: alors que \textit{fast} est le seul mot dans
son synset, c'est le quasi-synonyme de 20 synsets. Puisque les techniques de
sémantique distributionnelle ont plutôt tendance à identifier des
quasi-synonymes plutôt que des synonymes, utiliser cette relation de WordNet
pour identifier de nouveaux adjectifs fait partie de nos objectifs.

Une autre source importante d'amélioration sera l'enrichissement de notre
modèle de langue syntaxique qui pourra prendre en compte les verbes pronominaux
et les expressions multi-mots. Nous aimerions aussi nous orienter vers un
modèle de langue continu \citep{haison2012continuous} plus performant. Cela
sera couplé avec la collecte d'un corpus issu du Web plus récent et plus grand
analysé avec une version récente de notre analyseur linguistique LIMA. Cela
nous permettra de mesurer l'impact de la qualité du modèle de langue sur la
traduction de WordNet.

% TODO explications donnée à Francis Bond sur la non-fusion
